{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multivariate_multistep_mlp_lstm_cnn_hyper_parameter_optimization_model_checkpoint_performance_on_test-8.16.19.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "V5hyrNVwe2BF",
        "9YntpzaYyd_r",
        "ICj_cUwXOVUe",
        "vbaN9X-RIh90",
        "djX1IWkeClFS",
        "RpxJUYy612rm"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i4pVUgN_Sk6",
        "colab_type": "text"
      },
      "source": [
        "## Stock Price Velocity, Acceleration, Jerk and Snap? \n",
        "#### _Identifying Hidden Signals of Market Change with Optimized Multivariate-Multi Step MLPs, LSTMs and CNNs for Time Series Modeling of Higher Order Motion Derivatives in Physics and Their Relationship to Stock Price Dynamics_\n",
        "Brian Griner and Thomas Ball"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xb3Y427olmNH",
        "colab_type": "text"
      },
      "source": [
        "### Introduction\n",
        "This is a collaborative work in progress to explore how concepts in the physical world like velocity, jerk and other higher order derivatives of motion used in physics might help us identify early signals of  marketplace activity that are eventually revealed in patterns of stock price change at different periods.\n",
        "### Data\n",
        "Daily prices for 2,600 stocks over a period of 11 months (225 days)\n",
        "### Methodology\n",
        "*   **MODELS:** Multivariate multi step sequence models (specifically Multi Layer Perceptrons, Long Short Term Memory networks and Convolutional Neural Networks) are developed on training and validation samples. \n",
        "*   **TUNING:** Model hyper parameters are optimized using a full factorial experimental design with 2 to 3 levels for each hyper parameter.  \n",
        "Callbacks  are used for early stopping of training when the mean squared error of the validation sample does not improve over a specified number of training epochs. The _patience_ of the early stopping callback (number of completed epoches before training is terminated) is also tested as a hyper parameter in the experimental design.\n",
        "*   **TESTING:** Model performance is tested using the Test sample after hyper parameter tuning.  The best model from the tuning phase is used to predict the stock price for a specific stock. Several performance metrics are calculated on the Mean Squared Error of the difference between the predicted and actual prices in the Test sample. (TOM TO ADD DETAILS ON METRICS HERE) \n",
        "### Results\n",
        "TBD\n",
        "### Insights & Discussion\n",
        "TBD\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZn0iPeocfK9",
        "colab_type": "text"
      },
      "source": [
        "**1. Load data from local device using colab import files upload method**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_7zeDogtyl7",
        "colab_type": "code",
        "outputId": "47637ac1-c4de-4145-99f2-a0269f47adaf",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "source": [
        "# import colab files method and upload data\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2a902815-2c3f-4bc7-88e2-37e3b0d9f34f\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-2a902815-2c3f-4bc7-88e2-37e3b0d9f34f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving stocks.txt to stocks.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5hyrNVwe2BF",
        "colab_type": "text"
      },
      "source": [
        "### **Read Full Dataset - 2,600 tickers -- 225 days**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOLS8fQXe0Qm",
        "colab_type": "code",
        "outputId": "01af15ab-e0ea-45bf-83e2-9e30a1e96c17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# LARGE DATASET - stocks.txt \n",
        "\n",
        "# import libs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# set numeric format\n",
        "pd.options.display.float_format = '{:,.2f}'.format\n",
        "\n",
        "# create data objects \n",
        "stocks = pd.read_csv('stocks.txt',sep='\\t',usecols=range(0,10))\n",
        "\n",
        "# check columns & dtypes\n",
        "print(stocks.columns)\n",
        "print(stocks.info())\n",
        "\n",
        "# format date to datetime.date\n",
        "stocks['DATE'] =  pd.to_datetime(stocks.DATE.str.strip(' \\t\\r\\n'),format='%d%b%y')\n",
        "\n",
        "# format ticker id - strip leading & training spaces\n",
        "stocks['TICKER'] = stocks['TICKER'].str.strip()\n",
        "\n",
        "'''strip leading and trailing space example\n",
        "df1['State'] = df1['State'].str.strip()\n",
        "print (df1)'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['TICKER', 'DATE', 'TREND', 'RAWPRICE', 'LOGPRICE', 'RETURNS',\n",
            "       'FIRST DERIVATIVE', 'SECOND DERIVATIVE', 'THIRD DERIVATIVE',\n",
            "       'FOURTH DERIVATIVE'],\n",
            "      dtype='object')\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 598725 entries, 0 to 598724\n",
            "Data columns (total 10 columns):\n",
            "TICKER               598725 non-null object\n",
            "DATE                 598725 non-null object\n",
            "TREND                598725 non-null int64\n",
            "RAWPRICE             598725 non-null float64\n",
            "LOGPRICE             598725 non-null float64\n",
            "RETURNS              598725 non-null float64\n",
            "FIRST DERIVATIVE     598725 non-null float64\n",
            "SECOND DERIVATIVE    598725 non-null float64\n",
            "THIRD DERIVATIVE     598725 non-null float64\n",
            "FOURTH DERIVATIVE    598725 non-null float64\n",
            "dtypes: float64(7), int64(1), object(2)\n",
            "memory usage: 45.7+ MB\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"strip leading and trailing space example\\ndf1['State'] = df1['State'].str.strip()\\nprint (df1)\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YntpzaYyd_r",
        "colab_type": "text"
      },
      "source": [
        "### Data Dimensions: Overall"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4646E1ayWRi",
        "colab_type": "code",
        "outputId": "64feb0a6-4143-47a0-d510-74393b895d7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "print(stocks.columns)\n",
        "print(stocks.head())\n",
        "print(stocks.tail())\n",
        "print(stocks.describe().T)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['TICKER', 'DATE', 'TREND', 'RAWPRICE', 'LOGPRICE', 'RETURNS',\n",
            "       'FIRST DERIVATIVE', 'SECOND DERIVATIVE', 'THIRD DERIVATIVE',\n",
            "       'FOURTH DERIVATIVE'],\n",
            "      dtype='object')\n",
            "  TICKER       DATE  ...  THIRD DERIVATIVE  FOURTH DERIVATIVE\n",
            "0      A 2016-09-06  ...             -0.00               0.00\n",
            "1      A 2016-09-07  ...              0.01               0.01\n",
            "2      A 2016-09-08  ...             -0.01              -0.02\n",
            "3      A 2016-09-09  ...             -0.04              -0.03\n",
            "4      A 2016-09-12  ...              0.11               0.15\n",
            "\n",
            "[5 rows x 10 columns]\n",
            "       TICKER       DATE  ...  THIRD DERIVATIVE  FOURTH DERIVATIVE\n",
            "598720   ZYNE 2017-07-21  ...              0.04               0.06\n",
            "598721   ZYNE 2017-07-24  ...             -0.01              -0.06\n",
            "598722   ZYNE 2017-07-25  ...             -0.07              -0.05\n",
            "598723   ZYNE 2017-07-26  ...              0.13               0.20\n",
            "598724   ZYNE 2017-07-27  ...             -0.10              -0.23\n",
            "\n",
            "[5 rows x 10 columns]\n",
            "                       count   mean   std    min   25%    50%    75%      max\n",
            "TREND             598,725.00 113.00 64.95   1.00 57.00 113.00 169.00   225.00\n",
            "RAWPRICE          598,725.00  41.54 64.10   0.00 10.96  25.49  53.09 2,033.21\n",
            "LOGPRICE          598,725.00   3.18  1.13   0.00  2.48   3.28   3.99     7.62\n",
            "RETURNS           598,725.00   0.00  0.03  -1.84 -0.01   0.00   0.01     3.28\n",
            "FIRST DERIVATIVE  598,725.00   0.00  0.04  -3.27 -0.01   0.00   0.01     4.38\n",
            "SECOND DERIVATIVE 598,725.00  -0.00  0.05  -4.44 -0.01  -0.00   0.01     6.47\n",
            "THIRD DERIVATIVE  598,725.00  -0.00  0.09  -9.85 -0.03  -0.00   0.03     9.66\n",
            "FOURTH DERIVATIVE 598,725.00   0.00  0.17 -19.51 -0.05  -0.00   0.05    13.77\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pq-c1-pH0Z9P",
        "colab_type": "text"
      },
      "source": [
        "### Data Dimensions: Dates, Tickers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg08Nz6s0aOY",
        "colab_type": "code",
        "outputId": "33b4f063-1a62-4d5c-a460-9921627917ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# dates\n",
        "print('number of unique dates: ',len(stocks['DATE'].unique()))\n",
        "print(stocks['DATE'].unique())\n",
        "\n",
        "# tickers\n",
        "print('number of unique tickers: ', len(stocks['TICKER'].unique()))\n",
        "print(stocks['TICKER'].value_counts())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of unique dates:  225\n",
            "['2016-09-06T00:00:00.000000000' '2016-09-07T00:00:00.000000000'\n",
            " '2016-09-08T00:00:00.000000000' '2016-09-09T00:00:00.000000000'\n",
            " '2016-09-12T00:00:00.000000000' '2016-09-13T00:00:00.000000000'\n",
            " '2016-09-14T00:00:00.000000000' '2016-09-15T00:00:00.000000000'\n",
            " '2016-09-16T00:00:00.000000000' '2016-09-19T00:00:00.000000000'\n",
            " '2016-09-20T00:00:00.000000000' '2016-09-21T00:00:00.000000000'\n",
            " '2016-09-22T00:00:00.000000000' '2016-09-23T00:00:00.000000000'\n",
            " '2016-09-26T00:00:00.000000000' '2016-09-27T00:00:00.000000000'\n",
            " '2016-09-28T00:00:00.000000000' '2016-09-29T00:00:00.000000000'\n",
            " '2016-09-30T00:00:00.000000000' '2016-10-03T00:00:00.000000000'\n",
            " '2016-10-04T00:00:00.000000000' '2016-10-05T00:00:00.000000000'\n",
            " '2016-10-06T00:00:00.000000000' '2016-10-07T00:00:00.000000000'\n",
            " '2016-10-10T00:00:00.000000000' '2016-10-11T00:00:00.000000000'\n",
            " '2016-10-12T00:00:00.000000000' '2016-10-13T00:00:00.000000000'\n",
            " '2016-10-14T00:00:00.000000000' '2016-10-17T00:00:00.000000000'\n",
            " '2016-10-18T00:00:00.000000000' '2016-10-19T00:00:00.000000000'\n",
            " '2016-10-20T00:00:00.000000000' '2016-10-21T00:00:00.000000000'\n",
            " '2016-10-24T00:00:00.000000000' '2016-10-25T00:00:00.000000000'\n",
            " '2016-10-26T00:00:00.000000000' '2016-10-27T00:00:00.000000000'\n",
            " '2016-10-28T00:00:00.000000000' '2016-10-31T00:00:00.000000000'\n",
            " '2016-11-01T00:00:00.000000000' '2016-11-02T00:00:00.000000000'\n",
            " '2016-11-03T00:00:00.000000000' '2016-11-04T00:00:00.000000000'\n",
            " '2016-11-07T00:00:00.000000000' '2016-11-08T00:00:00.000000000'\n",
            " '2016-11-09T00:00:00.000000000' '2016-11-10T00:00:00.000000000'\n",
            " '2016-11-11T00:00:00.000000000' '2016-11-14T00:00:00.000000000'\n",
            " '2016-11-15T00:00:00.000000000' '2016-11-16T00:00:00.000000000'\n",
            " '2016-11-17T00:00:00.000000000' '2016-11-18T00:00:00.000000000'\n",
            " '2016-11-21T00:00:00.000000000' '2016-11-22T00:00:00.000000000'\n",
            " '2016-11-23T00:00:00.000000000' '2016-11-25T00:00:00.000000000'\n",
            " '2016-11-28T00:00:00.000000000' '2016-11-29T00:00:00.000000000'\n",
            " '2016-11-30T00:00:00.000000000' '2016-12-01T00:00:00.000000000'\n",
            " '2016-12-02T00:00:00.000000000' '2016-12-05T00:00:00.000000000'\n",
            " '2016-12-06T00:00:00.000000000' '2016-12-07T00:00:00.000000000'\n",
            " '2016-12-08T00:00:00.000000000' '2016-12-09T00:00:00.000000000'\n",
            " '2016-12-12T00:00:00.000000000' '2016-12-13T00:00:00.000000000'\n",
            " '2016-12-14T00:00:00.000000000' '2016-12-15T00:00:00.000000000'\n",
            " '2016-12-16T00:00:00.000000000' '2016-12-19T00:00:00.000000000'\n",
            " '2016-12-20T00:00:00.000000000' '2016-12-21T00:00:00.000000000'\n",
            " '2016-12-22T00:00:00.000000000' '2016-12-23T00:00:00.000000000'\n",
            " '2016-12-27T00:00:00.000000000' '2016-12-28T00:00:00.000000000'\n",
            " '2016-12-29T00:00:00.000000000' '2016-12-30T00:00:00.000000000'\n",
            " '2017-01-03T00:00:00.000000000' '2017-01-04T00:00:00.000000000'\n",
            " '2017-01-05T00:00:00.000000000' '2017-01-06T00:00:00.000000000'\n",
            " '2017-01-09T00:00:00.000000000' '2017-01-10T00:00:00.000000000'\n",
            " '2017-01-11T00:00:00.000000000' '2017-01-12T00:00:00.000000000'\n",
            " '2017-01-13T00:00:00.000000000' '2017-01-17T00:00:00.000000000'\n",
            " '2017-01-18T00:00:00.000000000' '2017-01-19T00:00:00.000000000'\n",
            " '2017-01-20T00:00:00.000000000' '2017-01-23T00:00:00.000000000'\n",
            " '2017-01-24T00:00:00.000000000' '2017-01-25T00:00:00.000000000'\n",
            " '2017-01-26T00:00:00.000000000' '2017-01-27T00:00:00.000000000'\n",
            " '2017-01-30T00:00:00.000000000' '2017-01-31T00:00:00.000000000'\n",
            " '2017-02-01T00:00:00.000000000' '2017-02-02T00:00:00.000000000'\n",
            " '2017-02-03T00:00:00.000000000' '2017-02-06T00:00:00.000000000'\n",
            " '2017-02-07T00:00:00.000000000' '2017-02-08T00:00:00.000000000'\n",
            " '2017-02-09T00:00:00.000000000' '2017-02-10T00:00:00.000000000'\n",
            " '2017-02-13T00:00:00.000000000' '2017-02-14T00:00:00.000000000'\n",
            " '2017-02-15T00:00:00.000000000' '2017-02-16T00:00:00.000000000'\n",
            " '2017-02-17T00:00:00.000000000' '2017-02-21T00:00:00.000000000'\n",
            " '2017-02-22T00:00:00.000000000' '2017-02-23T00:00:00.000000000'\n",
            " '2017-02-24T00:00:00.000000000' '2017-02-27T00:00:00.000000000'\n",
            " '2017-02-28T00:00:00.000000000' '2017-03-01T00:00:00.000000000'\n",
            " '2017-03-02T00:00:00.000000000' '2017-03-03T00:00:00.000000000'\n",
            " '2017-03-06T00:00:00.000000000' '2017-03-07T00:00:00.000000000'\n",
            " '2017-03-08T00:00:00.000000000' '2017-03-09T00:00:00.000000000'\n",
            " '2017-03-10T00:00:00.000000000' '2017-03-13T00:00:00.000000000'\n",
            " '2017-03-14T00:00:00.000000000' '2017-03-15T00:00:00.000000000'\n",
            " '2017-03-16T00:00:00.000000000' '2017-03-17T00:00:00.000000000'\n",
            " '2017-03-20T00:00:00.000000000' '2017-03-21T00:00:00.000000000'\n",
            " '2017-03-22T00:00:00.000000000' '2017-03-23T00:00:00.000000000'\n",
            " '2017-03-24T00:00:00.000000000' '2017-03-27T00:00:00.000000000'\n",
            " '2017-03-28T00:00:00.000000000' '2017-03-29T00:00:00.000000000'\n",
            " '2017-03-30T00:00:00.000000000' '2017-03-31T00:00:00.000000000'\n",
            " '2017-04-03T00:00:00.000000000' '2017-04-04T00:00:00.000000000'\n",
            " '2017-04-05T00:00:00.000000000' '2017-04-06T00:00:00.000000000'\n",
            " '2017-04-07T00:00:00.000000000' '2017-04-10T00:00:00.000000000'\n",
            " '2017-04-11T00:00:00.000000000' '2017-04-12T00:00:00.000000000'\n",
            " '2017-04-13T00:00:00.000000000' '2017-04-17T00:00:00.000000000'\n",
            " '2017-04-18T00:00:00.000000000' '2017-04-19T00:00:00.000000000'\n",
            " '2017-04-20T00:00:00.000000000' '2017-04-21T00:00:00.000000000'\n",
            " '2017-04-24T00:00:00.000000000' '2017-04-25T00:00:00.000000000'\n",
            " '2017-04-26T00:00:00.000000000' '2017-04-27T00:00:00.000000000'\n",
            " '2017-04-28T00:00:00.000000000' '2017-05-01T00:00:00.000000000'\n",
            " '2017-05-02T00:00:00.000000000' '2017-05-03T00:00:00.000000000'\n",
            " '2017-05-04T00:00:00.000000000' '2017-05-05T00:00:00.000000000'\n",
            " '2017-05-08T00:00:00.000000000' '2017-05-09T00:00:00.000000000'\n",
            " '2017-05-10T00:00:00.000000000' '2017-05-11T00:00:00.000000000'\n",
            " '2017-05-12T00:00:00.000000000' '2017-05-15T00:00:00.000000000'\n",
            " '2017-05-16T00:00:00.000000000' '2017-05-17T00:00:00.000000000'\n",
            " '2017-05-18T00:00:00.000000000' '2017-05-19T00:00:00.000000000'\n",
            " '2017-05-22T00:00:00.000000000' '2017-05-23T00:00:00.000000000'\n",
            " '2017-05-24T00:00:00.000000000' '2017-05-25T00:00:00.000000000'\n",
            " '2017-05-26T00:00:00.000000000' '2017-05-30T00:00:00.000000000'\n",
            " '2017-05-31T00:00:00.000000000' '2017-06-01T00:00:00.000000000'\n",
            " '2017-06-02T00:00:00.000000000' '2017-06-05T00:00:00.000000000'\n",
            " '2017-06-06T00:00:00.000000000' '2017-06-07T00:00:00.000000000'\n",
            " '2017-06-08T00:00:00.000000000' '2017-06-09T00:00:00.000000000'\n",
            " '2017-06-12T00:00:00.000000000' '2017-06-13T00:00:00.000000000'\n",
            " '2017-06-14T00:00:00.000000000' '2017-06-15T00:00:00.000000000'\n",
            " '2017-06-16T00:00:00.000000000' '2017-06-19T00:00:00.000000000'\n",
            " '2017-06-20T00:00:00.000000000' '2017-06-21T00:00:00.000000000'\n",
            " '2017-06-22T00:00:00.000000000' '2017-06-23T00:00:00.000000000'\n",
            " '2017-06-26T00:00:00.000000000' '2017-06-27T00:00:00.000000000'\n",
            " '2017-06-28T00:00:00.000000000' '2017-06-29T00:00:00.000000000'\n",
            " '2017-06-30T00:00:00.000000000' '2017-07-03T00:00:00.000000000'\n",
            " '2017-07-05T00:00:00.000000000' '2017-07-06T00:00:00.000000000'\n",
            " '2017-07-07T00:00:00.000000000' '2017-07-10T00:00:00.000000000'\n",
            " '2017-07-11T00:00:00.000000000' '2017-07-12T00:00:00.000000000'\n",
            " '2017-07-13T00:00:00.000000000' '2017-07-14T00:00:00.000000000'\n",
            " '2017-07-17T00:00:00.000000000' '2017-07-18T00:00:00.000000000'\n",
            " '2017-07-19T00:00:00.000000000' '2017-07-20T00:00:00.000000000'\n",
            " '2017-07-21T00:00:00.000000000' '2017-07-24T00:00:00.000000000'\n",
            " '2017-07-25T00:00:00.000000000' '2017-07-26T00:00:00.000000000'\n",
            " '2017-07-27T00:00:00.000000000']\n",
            "number of unique tickers:  2661\n",
            "SHLM    225\n",
            "PWE     225\n",
            "AON     225\n",
            "FCN     225\n",
            "CERS    225\n",
            "HCSG    225\n",
            "VIV     225\n",
            "PEG     225\n",
            "SAFM    225\n",
            "NWS     225\n",
            "LPI     225\n",
            "FSIC    225\n",
            "GWR     225\n",
            "LC      225\n",
            "MLHR    225\n",
            "MOBL    225\n",
            "JQC     225\n",
            "NADL    225\n",
            "QRVO    225\n",
            "ARCB    225\n",
            "VOD     225\n",
            "AT      225\n",
            "RGLS    225\n",
            "OXY     225\n",
            "JASO    225\n",
            "NVS     225\n",
            "ERI     225\n",
            "MARA    225\n",
            "BKCC    225\n",
            "UNFI    225\n",
            "       ... \n",
            "VAC     225\n",
            "RLOG    225\n",
            "UNH     225\n",
            "VIVO    225\n",
            "OFG     225\n",
            "BG      225\n",
            "AEO     225\n",
            "NCMI    225\n",
            "AHL     225\n",
            "FEYE    225\n",
            "NUS     225\n",
            "COLM    225\n",
            "EMES    225\n",
            "QIWI    225\n",
            "EVRI    225\n",
            "BBRY    225\n",
            "AI      225\n",
            "GPRO    225\n",
            "LGIH    225\n",
            "CBZ     225\n",
            "L       225\n",
            "NANO    225\n",
            "SAND    225\n",
            "DSLV    225\n",
            "MUR     225\n",
            "SHAK    225\n",
            "HAIN    225\n",
            "AFG     225\n",
            "OIS     225\n",
            "WU      225\n",
            "Name: TICKER, Length: 2661, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICj_cUwXOVUe",
        "colab_type": "text"
      },
      "source": [
        "### Create Train, Validation and Test Data for 1 Stock - EBAY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IIz5W8QYCID",
        "colab_type": "code",
        "outputId": "097d6f27-2c09-4501-ebd0-708e6dd4cffb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# mlp train, val, test split func\n",
        "\n",
        "def timeseries_train_val_test_split(df,fcol,fval,dcol,pcol,cols,train_end,val_end):\n",
        "  \"Use to create static train, val and test samples for timeseries.\"\n",
        "  # select one stock ticker\n",
        "  df1 = df.loc[df[fcol]==fval].set_index(dcol)\n",
        "  print(df1.describe().T)\n",
        "  print(df1.columns)\n",
        "  # plot series\n",
        "  plt.plot(df1[pcol])\n",
        "  plt.ylabel(pcol)\n",
        "  plt.show()\n",
        "  # select inputs & output\n",
        "  inoutcols = cols  # ['LOGPRICE','FIRST DERIVATIVE','SECOND DERIVATIVE','THIRD DERIVATIVE','FOURTH DERIVATIVE']\n",
        "  # select train, validation and test data\n",
        "  df_train, df_val, df_test = df1[:train_end] [inoutcols], df1[train_end:val_end] [inoutcols], df1[val_end:] [inoutcols]\n",
        "  print('N size: df_train, df_val, df_test = ',len(df_train),len(df_val), len(df_test))\n",
        "  # plot train, val and test data\n",
        "  plt.plot(df_train[pcol])\n",
        "  plt.plot(df_val[pcol]) \n",
        "  plt.plot(df_test[pcol])\n",
        "  plt.ylabel(pcol)\n",
        "  plt.show()\n",
        "  \n",
        "  return df_train, df_val, df_test\n",
        "\n",
        "\n",
        "# input parameters: \n",
        "df = stocks\n",
        "fcol = 'TICKER'\n",
        "fval = 'EBAY'\n",
        "dcol  = 'DATE'\n",
        "pcol = 'LOGPRICE'\n",
        "cols = ['FIRST DERIVATIVE','SECOND DERIVATIVE','THIRD DERIVATIVE','FOURTH DERIVATIVE','LOGPRICE']\n",
        "train_end = 175\n",
        "val_end = 200\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  \"\"\"Create train, validation and test dfs.\"\"\"\n",
        "  train_df,val_df,test_df = timeseries_train_val_test_split(df,fcol,fval,dcol,pcol,cols,train_end,val_end)\n",
        "  print(f'test_df: LOGPRICE for {fval}')\n",
        "  print(test_df['LOGPRICE'])\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                   count   mean   std   min   25%    50%    75%    max\n",
            "TREND             225.00 113.00 65.10  1.00 57.00 113.00 169.00 225.00\n",
            "RAWPRICE          225.00  32.40  2.39 27.39 30.27  32.89  34.06  37.18\n",
            "LOGPRICE          225.00   3.51  0.07  3.35  3.44   3.52   3.56   3.64\n",
            "RETURNS           225.00   0.00  0.01 -0.11 -0.01   0.00   0.01   0.05\n",
            "FIRST DERIVATIVE  225.00   0.00  0.01 -0.11 -0.01   0.00   0.01   0.05\n",
            "SECOND DERIVATIVE 225.00  -0.00  0.02 -0.14 -0.01   0.00   0.01   0.12\n",
            "THIRD DERIVATIVE  225.00  -0.00  0.04 -0.17 -0.02   0.00   0.02   0.26\n",
            "FOURTH DERIVATIVE 225.00  -0.00  0.07 -0.37 -0.04   0.00   0.03   0.43\n",
            "Index(['TICKER', 'TREND', 'RAWPRICE', 'LOGPRICE', 'RETURNS',\n",
            "       'FIRST DERIVATIVE', 'SECOND DERIVATIVE', 'THIRD DERIVATIVE',\n",
            "       'FOURTH DERIVATIVE'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/plotting/_converter.py:129: FutureWarning: Using an implicitly registered datetime converter for a matplotlib plotting method. The converter was registered by pandas on import. Future versions of pandas will require you to explicitly register matplotlib converters.\n",
            "\n",
            "To register the converters:\n",
            "\t>>> from pandas.plotting import register_matplotlib_converters\n",
            "\t>>> register_matplotlib_converters()\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAD8CAYAAABdCyJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "N size: df_train, df_val, df_test =  175 25 25\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAD8CAYAAABdCyJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbOM7FjKUc+wAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "test_df: LOGPRICE for EBAY\n",
            "DATE\n",
            "2017-06-22   3.59\n",
            "2017-06-23   3.60\n",
            "2017-06-26   3.59\n",
            "2017-06-27   3.58\n",
            "2017-06-28   3.59\n",
            "2017-06-29   3.57\n",
            "2017-06-30   3.58\n",
            "2017-07-03   3.57\n",
            "2017-07-05   3.58\n",
            "2017-07-06   3.56\n",
            "2017-07-07   3.56\n",
            "2017-07-10   3.58\n",
            "2017-07-11   3.59\n",
            "2017-07-12   3.61\n",
            "2017-07-13   3.62\n",
            "2017-07-14   3.64\n",
            "2017-07-17   3.64\n",
            "2017-07-18   3.63\n",
            "2017-07-19   3.64\n",
            "2017-07-20   3.64\n",
            "2017-07-21   3.63\n",
            "2017-07-24   3.62\n",
            "2017-07-25   3.62\n",
            "2017-07-26   3.64\n",
            "2017-07-27   3.62\n",
            "Name: LOGPRICE, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbaN9X-RIh90",
        "colab_type": "text"
      },
      "source": [
        "### Description: Data Format for Multivariate Multistep MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHqdqKGkKdqp",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Description**\n",
        "\n",
        "*1. Data prep step:*\n",
        "* *Panel data must be split into samples*\n",
        "* *Sample contains a sequence of time periods for inputs and the output*\n",
        "* *Each sample represents the inputs/output sequence in the next time period* \n",
        "\n",
        ">  **time series example (3 time period sequences):** \n",
        "\n",
        "> **X1:** 0, 1, 2, 3, 4\n",
        "\n",
        "> **X2:** 5, 6, 7, 8\n",
        "\n",
        "> **y:** 9, 10, 11, 12\n",
        "\n",
        "> * **1st sample:** 0, 1, 2\n",
        "\n",
        ">  * **2nd sample:** 1, 2, 3\n",
        "\n",
        "> * **3rd sample:** 2, 3, 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgHdhTHvR9kU",
        "colab_type": "text"
      },
      "source": [
        "## **EBAY from full dataset as example of Multivariate Input Multistep Inputs/Outputs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djX1IWkeClFS",
        "colab_type": "text"
      },
      "source": [
        "### Multivariate - Multistep Samples Prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGf5w8yhXZWb",
        "colab_type": "code",
        "outputId": "b534ee03-db04-420f-90f6-a8c03c45891b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# multivariate multi-step data preparation func\n",
        "\n",
        "from numpy import array\n",
        "# from numpy import hstack\n",
        "\n",
        "def df2samples(train_df,val_df,test_df,n_steps_in=3,n_steps_out=2):\n",
        "  \"Transform stacked timeseries to sequence samples.\"\n",
        "  # convert to np array\n",
        "  train_ar = train_df.to_numpy()\n",
        "  val_ar = val_df.to_numpy() \n",
        "  test_ar = test_df.to_numpy()\n",
        "  print('train array dimensions')\n",
        "  print(train_ar.shape)\n",
        "  print('validation array dimensions')\n",
        "  print(val_ar.shape)\n",
        "  print('test array dimensions')\n",
        "  print(test_ar.shape)\n",
        "  # split a multivariate arrays into sequence samples\n",
        "  def split_sequences(sequences, n_steps_in, n_steps_out):\n",
        "    X, y = list(), list()\n",
        "    for i in range(len(sequences)):\n",
        "      # find the end of this pattern\n",
        "      end_ix = i + n_steps_in\n",
        "      out_end_ix = end_ix + n_steps_out-1\n",
        "      # check if we are beyond the dataset\n",
        "      if out_end_ix > len(sequences):\n",
        "        break\n",
        "      # gather input and output parts of the pattern\n",
        "      seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n",
        "      X.append(seq_x)\n",
        "      y.append(seq_y)\n",
        "    return array(X), array(y)\n",
        "  # create train, val, test samples\n",
        "  X_train, y_train = split_sequences(train_ar, n_steps_in, n_steps_out)\n",
        "  X_val, y_val = split_sequences(val_ar, n_steps_in, n_steps_out)\n",
        "  X_test, y_test = split_sequences(test_ar, n_steps_in, n_steps_out)\n",
        "  return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "\n",
        "# set input parameters:\n",
        "\n",
        "# train_df, val_df, test_df created from timeseries_train_val_test_split()\n",
        "\n",
        "# number INPUT STEPS\n",
        "nsteps_input = 3\n",
        "print(f'Number of input time steps: {nsteps_input}')\n",
        "# number OUTPUT STEPS\n",
        "nsteps_output = 2\n",
        "print(f'Number of output time steps: {nsteps_output}')\n",
        "# number FEATURES\n",
        "nfeatures = 4\n",
        "print(f'Number Model of Features: {nfeatures}')\n",
        "\n",
        "# create samples\n",
        "if __name__ == '__main__':\n",
        "  \"\"\"Create and summarize samples.\"\"\"\n",
        "  # create TRAIN SAMPLES\n",
        "  X_train, y_train, X_val, y_val, X_test, y_test = df2samples(train_df,val_df,test_df,nsteps_input,nsteps_output)\n",
        "  # summarize TRAIN SAMPLES\n",
        "  print(f'Shape of input/output samples: X_train {X_train.shape}, y_train {y_train.shape}')\n",
        "  print('Input/output samples X_train, y_train:')\n",
        "  for i in range(len(X_train)):\n",
        "    print(i, X_train[i], y_train[i])\n",
        "    \n",
        "    \n",
        "  # summarize VAL SAMPLES\n",
        "  print(f'Shape of input/output samples: X_val {X_val.shape}, y_val {y_val.shape}')\n",
        "  print('Input, output samples X_val, y_val:')\n",
        "  for i in range(len(X_val)):\n",
        "    print(i, X_val[i], y_val[i])\n",
        "\n",
        "  \n",
        "  # summarize TEST SAMPLES\n",
        "  print(f'Shape of input/output samples: X_test {X_test.shape}, y_test {y_test.shape}')\n",
        "  print('Input, output samples X_test, y_test:')\n",
        "  for i in range(len(X_test)):\n",
        "    print(i, X_test[i], y_test[i])\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of input time steps: 3\n",
            "Number of output time steps: 2\n",
            "Number Model of Features: 4\n",
            "train array dimensions\n",
            "(175, 5)\n",
            "validation array dimensions\n",
            "(25, 5)\n",
            "test array dimensions\n",
            "(25, 5)\n",
            "Shape of input/output samples: X_train (172, 3, 4), y_train (172, 2)\n",
            "Input/output samples X_train, y_train:\n",
            "0 [[ 0.0040192  -0.0006384  -0.00622925 -0.02495434]\n",
            " [ 0.00308066 -0.00093855 -0.00030015  0.00592911]\n",
            " [ 0.00582744  0.00274678  0.00368533  0.00398547]] [3.51749787 3.49316863]\n",
            "1 [[ 0.00308066 -0.00093855 -0.00030015  0.00592911]\n",
            " [ 0.00582744  0.00274678  0.00368533  0.00398547]\n",
            " [-0.02508266 -0.03091009 -0.03365688 -0.0373422 ]] [3.49316863 3.51124695]\n",
            "2 [[ 0.00582744  0.00274678  0.00368533  0.00398547]\n",
            " [-0.02508266 -0.03091009 -0.03365688 -0.0373422 ]\n",
            " [ 0.01863996  0.04372262  0.07463271  0.10828959]] [3.51124695 3.49377656]\n",
            "3 [[-0.02508266 -0.03091009 -0.03365688 -0.0373422 ]\n",
            " [ 0.01863996  0.04372262  0.07463271  0.10828959]\n",
            " [-0.01801297 -0.03665293 -0.08037555 -0.15500827]] [3.49377656 3.49042848]\n",
            "4 [[ 0.01863996  0.04372262  0.07463271  0.10828959]\n",
            " [-0.01801297 -0.03665293 -0.08037555 -0.15500827]\n",
            " [-0.00345318  0.01455979  0.05121272  0.13158827]] [3.49042848 3.49590132]\n",
            "5 [[-0.01801297 -0.03665293 -0.08037555 -0.15500827]\n",
            " [-0.00345318  0.01455979  0.05121272  0.13158827]\n",
            " [ 0.00564445  0.00909763 -0.00546216 -0.05667488]] [3.49590132 3.48951346]\n",
            "6 [[-0.00345318  0.01455979  0.05121272  0.13158827]\n",
            " [ 0.00564445  0.00909763 -0.00546216 -0.05667488]\n",
            " [-0.00658826 -0.01223271 -0.02133034 -0.01586818]] [3.48951346 3.48339162]\n",
            "7 [[ 0.00564445  0.00909763 -0.00546216 -0.05667488]\n",
            " [-0.00658826 -0.01223271 -0.02133034 -0.01586818]\n",
            " [-0.00631515  0.00027311  0.01250582  0.03383616]] [3.48339162 3.47692312]\n",
            "8 [[-0.00658826 -0.01223271 -0.02133034 -0.01586818]\n",
            " [-0.00631515  0.00027311  0.01250582  0.03383616]\n",
            " [-0.00667407 -0.00035893 -0.00063204 -0.01313787]] [3.47692312 3.49559803]\n",
            "9 [[-0.00631515  0.00027311  0.01250582  0.03383616]\n",
            " [-0.00667407 -0.00035893 -0.00063204 -0.01313787]\n",
            " [ 0.0192647   0.02593878  0.02629771  0.02692975]] [3.49559803 3.50495659]\n",
            "10 [[-0.00667407 -0.00035893 -0.00063204 -0.01313787]\n",
            " [ 0.0192647   0.02593878  0.02629771  0.02692975]\n",
            " [ 0.00964988 -0.00961482 -0.0355536  -0.06185131]] [3.50495659 3.49862653]\n",
            "11 [[ 0.0192647   0.02593878  0.02629771  0.02692975]\n",
            " [ 0.00964988 -0.00961482 -0.0355536  -0.06185131]\n",
            " [-0.0065268  -0.01617668 -0.00656186  0.02899175]] [3.49862653 3.48645723]\n",
            "12 [[ 0.00964988 -0.00961482 -0.0355536  -0.06185131]\n",
            " [-0.0065268  -0.01617668 -0.00656186  0.02899175]\n",
            " [-0.01255115 -0.00602435  0.01015232  0.01671418]] [3.48645723 3.49468776]\n",
            "13 [[-0.0065268  -0.01617668 -0.00656186  0.02899175]\n",
            " [-0.01255115 -0.00602435  0.01015232  0.01671418]\n",
            " [ 0.00848931  0.02104047  0.02706482  0.0169125 ]] [3.49468776 3.49195174]\n",
            "14 [[-0.01255115 -0.00602435  0.01015232  0.01671418]\n",
            " [ 0.00848931  0.02104047  0.02706482  0.0169125 ]\n",
            " [-0.00282179 -0.01131111 -0.03235157 -0.05941639]] [3.49195174 3.50555737]\n",
            "15 [[ 0.00848931  0.02104047  0.02706482  0.0169125 ]\n",
            " [-0.00282179 -0.01131111 -0.03235157 -0.05941639]\n",
            " [ 0.01402982  0.01685161  0.02816272  0.06051429]] [3.50555737 3.52341507]\n",
            "16 [[-0.00282179 -0.01131111 -0.03235157 -0.05941639]\n",
            " [ 0.01402982  0.01685161  0.02816272  0.06051429]\n",
            " [ 0.01840552  0.0043757  -0.01247591 -0.04063862]] [3.52341507 3.51303683]\n",
            "17 [[ 0.01402982  0.01685161  0.02816272  0.06051429]\n",
            " [ 0.01840552  0.0043757  -0.01247591 -0.04063862]\n",
            " [-0.01069538 -0.0291009  -0.0334766  -0.02100069]] [3.51303683 3.51333491]\n",
            "18 [[ 0.01840552  0.0043757  -0.01247591 -0.04063862]\n",
            " [-0.01069538 -0.0291009  -0.0334766  -0.02100069]\n",
            " [ 0.00030723  0.01100261  0.04010351  0.07358012]] [3.51333491 3.50104278]\n",
            "19 [[-0.01069538 -0.0291009  -0.0334766  -0.02100069]\n",
            " [ 0.00030723  0.01100261  0.04010351  0.07358012]\n",
            " [-0.01267206 -0.01297929 -0.02398191 -0.06408542]] [3.50104278 3.50043925]\n",
            "20 [[ 0.00030723  0.01100261  0.04010351  0.07358012]\n",
            " [-0.01267206 -0.01297929 -0.02398191 -0.06408542]\n",
            " [-0.00062231  0.01204975  0.02502904  0.04901095]] [3.50043925 3.48492563]\n",
            "21 [[-0.01267206 -0.01297929 -0.02398191 -0.06408542]\n",
            " [-0.00062231  0.01204975  0.02502904  0.04901095]\n",
            " [-0.01600034 -0.01537803 -0.02742778 -0.05245683]] [3.48492563 3.49499129]\n",
            "22 [[-0.00062231  0.01204975  0.02502904  0.04901095]\n",
            " [-0.01600034 -0.01537803 -0.02742778 -0.05245683]\n",
            " [ 0.01038235  0.02638269  0.04176072  0.0691885 ]] [3.49499129 3.48000853]\n",
            "23 [[-0.01600034 -0.01537803 -0.02742778 -0.05245683]\n",
            " [ 0.01038235  0.02638269  0.04176072  0.0691885 ]\n",
            " [-0.01545535 -0.0258377  -0.05222039 -0.09398111]] [3.48000853 3.48124009]\n",
            "24 [[ 0.01038235  0.02638269  0.04176072  0.0691885 ]\n",
            " [-0.01545535 -0.0258377  -0.05222039 -0.09398111]\n",
            " [ 0.00127068  0.01672603  0.04256372  0.09478411]] [3.48124009 3.48154773]\n",
            "25 [[-0.01545535 -0.0258377  -0.05222039 -0.09398111]\n",
            " [ 0.00127068  0.01672603  0.04256372  0.09478411]\n",
            " [ 0.00031741 -0.00095327 -0.0176793  -0.06024302]] [3.48154773 3.49316863]\n",
            "26 [[ 0.00127068  0.01672603  0.04256372  0.09478411]\n",
            " [ 0.00031741 -0.00095327 -0.0176793  -0.06024302]\n",
            " [ 0.01198749  0.01167008  0.01262335  0.03030265]] [3.49316863 3.49073332]\n",
            "27 [[ 0.00031741 -0.00095327 -0.0176793  -0.06024302]\n",
            " [ 0.01198749  0.01167008  0.01262335  0.03030265]\n",
            " [-0.00251177 -0.01449927 -0.02616935 -0.03879271]] [3.49073332 3.4855385 ]\n",
            "28 [[ 0.01198749  0.01167008  0.01262335  0.03030265]\n",
            " [-0.00251177 -0.01449927 -0.02616935 -0.03879271]\n",
            " [-0.00535856 -0.00284679  0.01165248  0.03782183]] [3.4855385  3.51214228]\n",
            "29 [[-0.00251177 -0.01449927 -0.02616935 -0.03879271]\n",
            " [-0.00535856 -0.00284679  0.01165248  0.03782183]\n",
            " [ 0.02743317  0.03279174  0.03563852  0.02398604]] [3.51214228 3.40186383]\n",
            "30 [[-0.00535856 -0.00284679  0.01165248  0.03782183]\n",
            " [ 0.02743317  0.03279174  0.03563852  0.02398604]\n",
            " [-0.11387004 -0.14130321 -0.17409495 -0.20973347]] [3.40186383 3.40319535]\n",
            "31 [[ 0.02743317  0.03279174  0.03563852  0.02398604]\n",
            " [-0.11387004 -0.14130321 -0.17409495 -0.20973347]\n",
            " [ 0.00137738  0.11524741  0.25655062  0.43064557]] [3.40319535 3.40850392]\n",
            "32 [[-0.11387004 -0.14130321 -0.17409495 -0.20973347]\n",
            " [ 0.00137738  0.11524741  0.25655062  0.43064557]\n",
            " [ 0.00549075  0.00411337 -0.11113404 -0.36768467]] [3.40850392 3.40286263]\n",
            "33 [[ 0.00137738  0.11524741  0.25655062  0.43064557]\n",
            " [ 0.00549075  0.00411337 -0.11113404 -0.36768467]\n",
            " [-0.00583492 -0.01132567 -0.01543904  0.095695  ]] [3.40286263 3.39517931]\n",
            "34 [[ 0.00549075  0.00411337 -0.11113404 -0.36768467]\n",
            " [-0.00583492 -0.01132567 -0.01543904  0.095695  ]\n",
            " [-0.00794886 -0.00211393  0.00921174  0.02465078]] [3.39517931 3.39484387]\n",
            "35 [[-0.00583492 -0.01132567 -0.01543904  0.095695  ]\n",
            " [-0.00794886 -0.00211393  0.00921174  0.02465078]\n",
            " [-0.00034708  0.00760178  0.00971572  0.00050398]] [3.39484387 3.38777436]\n",
            "36 [[-0.00794886 -0.00211393  0.00921174  0.02465078]\n",
            " [-0.00034708  0.00760178  0.00971572  0.00050398]\n",
            " [-0.0073158  -0.00696872 -0.0145705  -0.02428622]] [3.38777436 3.38472919]\n",
            "37 [[-0.00034708  0.00760178  0.00971572  0.00050398]\n",
            " [-0.0073158  -0.00696872 -0.0145705  -0.02428622]\n",
            " [-0.00315182  0.00416398  0.0111327   0.0257032 ]] [3.38472919 3.38031414]\n",
            "38 [[-0.0073158  -0.00696872 -0.0145705  -0.02428622]\n",
            " [-0.00315182  0.00416398  0.0111327   0.0257032 ]\n",
            " [-0.00457027 -0.00141845 -0.00558243 -0.01671513]] [3.38031414 3.3762214 ]\n",
            "39 [[-0.00315182  0.00416398  0.0111327   0.0257032 ]\n",
            " [-0.00457027 -0.00141845 -0.00558243 -0.01671513]\n",
            " [-0.00423726  0.00033301  0.00175146  0.00733389]] [3.3762214  3.36936262]\n",
            "40 [[-0.00457027 -0.00141845 -0.00558243 -0.01671513]\n",
            " [-0.00423726  0.00033301  0.00175146  0.00733389]\n",
            " [-0.00710234 -0.00286508 -0.00319809 -0.00494954]] [3.36936262 3.36557024]\n",
            "41 [[-0.00423726  0.00033301  0.00175146  0.00733389]\n",
            " [-0.00710234 -0.00286508 -0.00319809 -0.00494954]\n",
            " [-0.0039278   0.00317453  0.00603961  0.0092377 ]] [3.36557024 3.37553763]\n",
            "42 [[-0.00710234 -0.00286508 -0.00319809 -0.00494954]\n",
            " [-0.0039278   0.00317453  0.00603961  0.0092377 ]\n",
            " [ 0.01032218  0.01424998  0.01107545  0.00503583]] [3.37553763 3.36832981]\n",
            "43 [[-0.0039278   0.00317453  0.00603961  0.0092377 ]\n",
            " [ 0.01032218  0.01424998  0.01107545  0.00503583]\n",
            " [-0.00746401 -0.01778619 -0.03203617 -0.04311162]] [3.36832981 3.3772462 ]\n",
            "44 [[ 0.01032218  0.01424998  0.01107545  0.00503583]\n",
            " [-0.00746401 -0.01778619 -0.03203617 -0.04311162]\n",
            " [ 0.00923302  0.01669703  0.03448322  0.06651939]] [3.3772462  3.37279791]\n",
            "45 [[-0.00746401 -0.01778619 -0.03203617 -0.04311162]\n",
            " [ 0.00923302  0.01669703  0.03448322  0.06651939]\n",
            " [-0.00460589 -0.01383891 -0.03053594 -0.06501916]] [3.37279791 3.38912477]\n",
            "46 [[ 0.00923302  0.01669703  0.03448322  0.06651939]\n",
            " [-0.00460589 -0.01383891 -0.03053594 -0.06501916]\n",
            " [ 0.01690178  0.02150767  0.03534657  0.06588252]] [3.38912477 3.36349548]\n",
            "47 [[-0.00460589 -0.01383891 -0.03053594 -0.06501916]\n",
            " [ 0.01690178  0.02150767  0.03534657  0.06588252]\n",
            " [-0.02653614 -0.04343792 -0.06494558 -0.10029216]] [3.36349548 3.38405122]\n",
            "48 [[ 0.01690178  0.02150767  0.03534657  0.06588252]\n",
            " [-0.02653614 -0.04343792 -0.06494558 -0.10029216]\n",
            " [ 0.02128498  0.04782112  0.09125904  0.15620462]] [3.38405122 3.37861088]\n",
            "49 [[-0.02653614 -0.04343792 -0.06494558 -0.10029216]\n",
            " [ 0.02128498  0.04782112  0.09125904  0.15620462]\n",
            " [-0.00563183 -0.02691682 -0.07473794 -0.16599698]] [3.37861088 3.39685467]\n",
            "50 [[ 0.02128498  0.04782112  0.09125904  0.15620462]\n",
            " [-0.00563183 -0.02691682 -0.07473794 -0.16599698]\n",
            " [ 0.01888171  0.02451355  0.05143037  0.12616831]] [3.39685467 3.39081032]\n",
            "51 [[-0.00563183 -0.02691682 -0.07473794 -0.16599698]\n",
            " [ 0.01888171  0.02451355  0.05143037  0.12616831]\n",
            " [-0.00625436 -0.02513608 -0.04964963 -0.10107999]] [3.39081032 3.40119738]\n",
            "52 [[ 0.01888171  0.02451355  0.05143037  0.12616831]\n",
            " [-0.00625436 -0.02513608 -0.04964963 -0.10107999]\n",
            " [ 0.01074717  0.01700153  0.04213761  0.09178723]] [3.40119738 3.40319535]\n",
            "53 [[-0.00625436 -0.02513608 -0.04964963 -0.10107999]\n",
            " [ 0.01074717  0.01700153  0.04213761  0.09178723]\n",
            " [ 0.00206679 -0.00868037 -0.0256819  -0.06781951]] [3.40319535 3.39785848]\n",
            "54 [[ 0.01074717  0.01700153  0.04213761  0.09178723]\n",
            " [ 0.00206679 -0.00868037 -0.0256819  -0.06781951]\n",
            " [-0.00552103 -0.00758782  0.00109255  0.02677445]] [3.39785848 3.39952936]\n",
            "55 [[ 0.00206679 -0.00868037 -0.0256819  -0.06781951]\n",
            " [-0.00552103 -0.00758782  0.00109255  0.02677445]\n",
            " [ 0.00172864  0.00724967  0.01483749  0.01374495]] [3.39952936 3.38676033]\n",
            "56 [[-0.00552103 -0.00758782  0.00109255  0.02677445]\n",
            " [ 0.00172864  0.00724967  0.01483749  0.01374495]\n",
            " [-0.01321302 -0.01494167 -0.02219134 -0.03702883]] [3.38676033 3.38472919]\n",
            "57 [[ 0.00172864  0.00724967  0.01483749  0.01374495]\n",
            " [-0.01321302 -0.01494167 -0.02219134 -0.03702883]\n",
            " [-0.00210231  0.01111071  0.02605237  0.04824371]] [3.38472919 3.36072251]\n",
            "58 [[-0.01321302 -0.01494167 -0.02219134 -0.03702883]\n",
            " [-0.00210231  0.01111071  0.02605237  0.04824371]\n",
            " [-0.02485927 -0.02275696 -0.03386767 -0.05992004]] [3.36072251 3.34603694]\n",
            "59 [[-0.00210231  0.01111071  0.02605237  0.04824371]\n",
            " [-0.02485927 -0.02275696 -0.03386767 -0.05992004]\n",
            " [-0.01521769  0.00964159  0.03239854  0.06626621]] [3.34603694 3.38167472]\n",
            "60 [[-0.02485927 -0.02275696 -0.03386767 -0.05992004]\n",
            " [-0.01521769  0.00964159  0.03239854  0.06626621]\n",
            " [ 0.03691518  0.05213286  0.04249128  0.01009273]] [3.38167472 3.37929255]\n",
            "61 [[-0.01521769  0.00964159  0.03239854  0.06626621]\n",
            " [ 0.03691518  0.05213286  0.04249128  0.01009273]\n",
            " [-0.00246609 -0.03938127 -0.09151413 -0.13400541]] [3.37929255 3.37451108]\n",
            "62 [[ 0.03691518  0.05213286  0.04249128  0.01009273]\n",
            " [-0.00246609 -0.03938127 -0.09151413 -0.13400541]\n",
            " [-0.00495054 -0.00248445  0.03689682  0.12841095]] [3.37451108 3.39819287]\n",
            "63 [[-0.00246609 -0.03938127 -0.09151413 -0.13400541]\n",
            " [-0.00495054 -0.00248445  0.03689682  0.12841095]\n",
            " [ 0.02451107  0.02946161  0.03194606 -0.00495077]] [3.39819287 3.41510029]\n",
            "64 [[-0.00495054 -0.00248445  0.03689682  0.12841095]\n",
            " [ 0.02451107  0.02946161  0.03194606 -0.00495077]\n",
            " [ 0.01748716 -0.00702391 -0.03648551 -0.06843157]] [3.41510029 3.43301896]\n",
            "65 [[ 0.02451107  0.02946161  0.03194606 -0.00495077]\n",
            " [ 0.01748716 -0.00702391 -0.03648551 -0.06843157]\n",
            " [ 0.01852213  0.00103497  0.00805888  0.0445444 ]] [3.43301896 3.44073852]\n",
            "66 [[ 0.01748716 -0.00702391 -0.03648551 -0.06843157]\n",
            " [ 0.01852213  0.00103497  0.00805888  0.0445444 ]\n",
            " [ 0.00797611 -0.01054602 -0.01158099 -0.01963987]] [3.44073852 3.42816383]\n",
            "67 [[ 0.01852213  0.00103497  0.00805888  0.0445444 ]\n",
            " [ 0.00797611 -0.01054602 -0.01158099 -0.01963987]\n",
            " [-0.01299365 -0.02096977 -0.01042375  0.00115724]] [3.42816383 3.42816383]\n",
            "68 [[ 0.00797611 -0.01054602 -0.01158099 -0.01963987]\n",
            " [-0.01299365 -0.02096977 -0.01042375  0.00115724]\n",
            " [ 0.          0.01299365  0.03396342  0.04438717]] [3.42816383 3.42523938]\n",
            "69 [[-0.01299365 -0.02096977 -0.01042375  0.00115724]\n",
            " [ 0.          0.01299365  0.03396342  0.04438717]\n",
            " [-0.00302267 -0.00302267 -0.01601633 -0.04997975]] [3.42523938 3.42132679]\n",
            "70 [[ 0.          0.01299365  0.03396342  0.04438717]\n",
            " [-0.00302267 -0.00302267 -0.01601633 -0.04997975]\n",
            " [-0.00404446 -0.00102179  0.00200088  0.01801721]] [3.42132679 3.41378446]\n",
            "71 [[-0.00302267 -0.00302267 -0.01601633 -0.04997975]\n",
            " [-0.00404446 -0.00102179  0.00200088  0.01801721]\n",
            " [-0.00779804 -0.00375358 -0.00273179 -0.00473267]] [3.41378446 3.40982671]\n",
            "72 [[-0.00404446 -0.00102179  0.00200088  0.01801721]\n",
            " [-0.00779804 -0.00375358 -0.00273179 -0.00473267]\n",
            " [-0.00409274  0.0037053   0.00745888  0.01019067]] [3.40982671 3.41312599]\n",
            "73 [[-0.00779804 -0.00375358 -0.00273179 -0.00473267]\n",
            " [-0.00409274  0.0037053   0.00745888  0.01019067]\n",
            " [ 0.00341184  0.00750458  0.00379929 -0.00365959]] [3.41312599 3.41903733]\n",
            "74 [[-0.00409274  0.0037053   0.00745888  0.01019067]\n",
            " [ 0.00341184  0.00750458  0.00379929 -0.00365959]\n",
            " [ 0.00611207  0.00270023 -0.00480435 -0.00860364]] [3.41903733 3.42718999]\n",
            "75 [[ 0.00341184  0.00750458  0.00379929 -0.00365959]\n",
            " [ 0.00611207  0.00270023 -0.00480435 -0.00860364]\n",
            " [ 0.00842749  0.00231542 -0.00038481  0.00441954]] [3.42718999 3.44169933]\n",
            "76 [[ 0.00611207  0.00270023 -0.00480435 -0.00860364]\n",
            " [ 0.00842749  0.00231542 -0.00038481  0.00441954]\n",
            " [ 0.01499275  0.00656526  0.00424985  0.00463466]] [3.44169933 3.43430973]\n",
            "77 [[ 0.00842749  0.00231542 -0.00038481  0.00441954]\n",
            " [ 0.01499275  0.00656526  0.00424985  0.00463466]\n",
            " [-0.00763489 -0.02262764 -0.02919291 -0.03344275]] [3.43430973 3.43334183]\n",
            "78 [[ 0.01499275  0.00656526  0.00424985  0.00463466]\n",
            " [-0.00763489 -0.02262764 -0.02919291 -0.03344275]\n",
            " [-0.00100017  0.00663473  0.02926237  0.05845527]] [3.43334183 3.4239369 ]\n",
            "79 [[-0.00763489 -0.02262764 -0.02919291 -0.03344275]\n",
            " [-0.00100017  0.00663473  0.02926237  0.05845527]\n",
            " [-0.00972017 -0.00872    -0.01535473 -0.0446171 ]] [3.4239369  3.42881255]\n",
            "80 [[-0.00100017  0.00663473  0.02926237  0.05845527]\n",
            " [-0.00972017 -0.00872    -0.01535473 -0.0446171 ]\n",
            " [ 0.00503945  0.01475962  0.02347963  0.03883436]] [3.42881255 3.42621514]\n",
            "81 [[-0.00972017 -0.00872    -0.01535473 -0.0446171 ]\n",
            " [ 0.00503945  0.01475962  0.02347963  0.03883436]\n",
            " [-0.00268456 -0.00772402 -0.02248364 -0.04596327]] [3.42621514 3.43430973]\n",
            "82 [[ 0.00503945  0.01475962  0.02347963  0.03883436]\n",
            " [-0.00268456 -0.00772402 -0.02248364 -0.04596327]\n",
            " [ 0.00836545  0.01105001  0.01877403  0.04125767]] [3.43430973 3.46729715]\n",
            "83 [[-0.00268456 -0.00772402 -0.02248364 -0.04596327]\n",
            " [ 0.00836545  0.01105001  0.01877403  0.04125767]\n",
            " [ 0.03406812  0.02570267  0.01465265 -0.00412138]] [3.46729715 3.45789273]\n",
            "84 [[ 0.00836545  0.01105001  0.01877403  0.04125767]\n",
            " [ 0.03406812  0.02570267  0.01465265 -0.00412138]\n",
            " [-0.00970878 -0.0437769  -0.06947957 -0.08413222]] [3.45789273 3.44201938]\n",
            "85 [[ 0.03406812  0.02570267  0.01465265 -0.00412138]\n",
            " [-0.00970878 -0.0437769  -0.06947957 -0.08413222]\n",
            " [-0.01639381 -0.00668503  0.03709187  0.10657144]] [3.44201938 3.44712631]\n",
            "86 [[-0.00970878 -0.0437769  -0.06947957 -0.08413222]\n",
            " [-0.01639381 -0.00668503  0.03709187  0.10657144]\n",
            " [ 0.00527532  0.02166913  0.02835415 -0.00873772]] [3.44712631 3.44521427]\n",
            "87 [[-0.01639381 -0.00668503  0.03709187  0.10657144]\n",
            " [ 0.00527532  0.02166913  0.02835415 -0.00873772]\n",
            " [-0.00197498 -0.0072503  -0.02891943 -0.05727358]] [3.44521427 3.44329859]\n",
            "88 [[ 5.27531720e-03  2.16691270e-02  2.83541548e-02 -8.73771600e-03]\n",
            " [-1.97498400e-03 -7.25030100e-03 -2.89194280e-02 -5.72735830e-02]\n",
            " [-1.97885900e-03 -3.87526800e-06  7.24642610e-03  3.61658545e-02]] [3.44329859 3.44329859]\n",
            "89 [[-1.97498400e-03 -7.25030100e-03 -2.89194280e-02 -5.72735830e-02]\n",
            " [-1.97885900e-03 -3.87526800e-06  7.24642610e-03  3.61658545e-02]\n",
            " [ 0.00000000e+00  1.97885950e-03  1.98273470e-03 -5.26369100e-03]] [3.44329859 3.44648934]\n",
            "90 [[-1.97885900e-03 -3.87526800e-06  7.24642610e-03  3.61658545e-02]\n",
            " [ 0.00000000e+00  1.97885950e-03  1.98273470e-03 -5.26369100e-03]\n",
            " [ 3.29591600e-03  3.29591600e-03  1.31705650e-03 -6.65678000e-04]] [3.44648934 3.45694737]\n",
            "91 [[ 0.          0.00197886  0.00198273 -0.00526369]\n",
            " [ 0.00329592  0.00329592  0.00131706 -0.00066568]\n",
            " [ 0.0108003   0.00750439  0.00420847  0.00289141]] [3.45694737 3.45442211]\n",
            "92 [[ 0.00329592  0.00329592  0.00131706 -0.00066568]\n",
            " [ 0.0108003   0.00750439  0.00420847  0.00289141]\n",
            " [-0.00260756 -0.01340786 -0.02091225 -0.02512072]] [3.45442211 3.44265917]\n",
            "93 [[ 0.0108003   0.00750439  0.00420847  0.00289141]\n",
            " [-0.00260756 -0.01340786 -0.02091225 -0.02512072]\n",
            " [-0.01214919 -0.00954163  0.00386624  0.02477849]] [3.44265917 3.43366457]\n",
            "94 [[-0.00260756 -0.01340786 -0.02091225 -0.02512072]\n",
            " [-0.01214919 -0.00954163  0.00386624  0.02477849]\n",
            " [-0.00929313  0.00285606  0.01239768  0.00853145]] [3.43366457 3.44137917]\n",
            "95 [[-0.01214919 -0.00954163  0.00386624  0.02477849]\n",
            " [-0.00929313  0.00285606  0.01239768  0.00853145]\n",
            " [ 0.00797082  0.01726395  0.01440789  0.0020102 ]] [3.44137917 3.48859757]\n",
            "96 [[-0.00929313  0.00285606  0.01239768  0.00853145]\n",
            " [ 0.00797082  0.01726395  0.01440789  0.0020102 ]\n",
            " [ 0.04874291  0.04077209  0.02350814  0.00910026]] [3.48859757 3.51184384]\n",
            "97 [[ 0.00797082  0.01726395  0.01440789  0.0020102 ]\n",
            " [ 0.04874291  0.04077209  0.02350814  0.00910026]\n",
            " [ 0.02396996 -0.02477295 -0.06554504 -0.08905318]] [3.51184384 3.50164579]\n",
            "98 [[ 0.04874291  0.04077209  0.02350814  0.00910026]\n",
            " [ 0.02396996 -0.02477295 -0.06554504 -0.08905318]\n",
            " [-0.01051339 -0.03448335 -0.0097104   0.05583464]] [3.50164579 3.49134273]\n",
            "99 [[ 0.02396996 -0.02477295 -0.06554504 -0.08905318]\n",
            " [-0.01051339 -0.03448335 -0.0097104   0.05583464]\n",
            " [-0.01062504 -0.00011164  0.03437171  0.04408211]] [3.49134273 3.50194728]\n",
            "100 [[-0.01051339 -0.03448335 -0.0097104   0.05583464]\n",
            " [-0.01062504 -0.00011164  0.03437171  0.04408211]\n",
            " [ 0.0109359   0.02156094  0.02167258 -0.01269913]] [3.50194728 3.49559803]\n",
            "101 [[-0.01062504 -0.00011164  0.03437171  0.04408211]\n",
            " [ 0.0109359   0.02156094  0.02167258 -0.01269913]\n",
            " [-0.00654721 -0.01748311 -0.03904405 -0.06071663]] [3.49559803 3.49862653]\n",
            "102 [[ 0.0109359   0.02156094  0.02167258 -0.01269913]\n",
            " [-0.00654721 -0.01748311 -0.03904405 -0.06071663]\n",
            " [ 0.00312308  0.00967029  0.0271534   0.06619745]] [3.49862653 3.49741621]\n",
            "103 [[-0.00654721 -0.01748311 -0.03904405 -0.06071663]\n",
            " [ 0.00312308  0.00967029  0.0271534   0.06619745]\n",
            " [-0.00124808 -0.00437116 -0.01404145 -0.04119486]] [3.49741621 3.5094537 ]\n",
            "104 [[ 0.00312308  0.00967029  0.0271534   0.06619745]\n",
            " [-0.00124808 -0.00437116 -0.01404145 -0.04119486]\n",
            " [ 0.01241099  0.01365907  0.01803023  0.03207169]] [3.5094537  3.53368656]\n",
            "105 [[-0.00124808 -0.00437116 -0.01404145 -0.04119486]\n",
            " [ 0.01241099  0.01365907  0.01803023  0.03207169]\n",
            " [ 0.02497085  0.01255986 -0.00109921 -0.01912944]] [3.53368656 3.52988369]\n",
            "106 [[ 0.01241099  0.01365907  0.01803023  0.03207169]\n",
            " [ 0.02497085  0.01255986 -0.00109921 -0.01912944]\n",
            " [-0.00391747 -0.02888832 -0.04144817 -0.04034896]] [3.52988369 3.54472039]\n",
            "107 [[ 0.02497085  0.01255986 -0.00109921 -0.01912944]\n",
            " [-0.00391747 -0.02888832 -0.04144817 -0.04034896]\n",
            " [ 0.01528126  0.01919872  0.04808704  0.08953521]] [3.54472039 3.54933002]\n",
            "108 [[-0.00391747 -0.02888832 -0.04144817 -0.04034896]\n",
            " [ 0.01528126  0.01919872  0.04808704  0.08953521]\n",
            " [ 0.00474637 -0.01053488 -0.0297336  -0.07782064]] [3.54933002 3.54472039]\n",
            "109 [[ 0.01528126  0.01919872  0.04808704  0.08953521]\n",
            " [ 0.00474637 -0.01053488 -0.0297336  -0.07782064]\n",
            " [-0.00474637 -0.00949275  0.00104213  0.03077574]] [3.54472039 3.55563368]\n",
            "110 [[ 0.00474637 -0.01053488 -0.0297336  -0.07782064]\n",
            " [-0.00474637 -0.00949275  0.00104213  0.03077574]\n",
            " [ 0.01123598  0.01598236  0.02547511  0.02443298]] [3.55563368 3.5421181 ]\n",
            "111 [[-0.00474637 -0.00949275  0.00104213  0.03077574]\n",
            " [ 0.01123598  0.01598236  0.02547511  0.02443298]\n",
            " [-0.01391575 -0.02515174 -0.0411341  -0.06660921]] [3.5421181  3.54846724]\n",
            "112 [[ 0.01123598  0.01598236  0.02547511  0.02443298]\n",
            " [-0.01391575 -0.02515174 -0.0411341  -0.06660921]\n",
            " [ 0.00653782  0.02045358  0.04560532  0.08673942]] [3.54846724 3.55305967]\n",
            "113 [[-0.01391575 -0.02515174 -0.0411341  -0.06660921]\n",
            " [ 0.00653782  0.02045358  0.04560532  0.08673942]\n",
            " [ 0.00472814 -0.00180968 -0.02226326 -0.06786858]] [3.55305967 3.55019193]\n",
            "114 [[ 0.00653782  0.02045358  0.04560532  0.08673942]\n",
            " [ 0.00472814 -0.00180968 -0.02226326 -0.06786858]\n",
            " [-0.00295241 -0.00768055 -0.00587087  0.01639239]] [3.55019193 3.54385362]\n",
            "115 [[ 0.00472814 -0.00180968 -0.02226326 -0.06786858]\n",
            " [-0.00295241 -0.00768055 -0.00587087  0.01639239]\n",
            " [-0.00652634 -0.00357393  0.00410662  0.00997749]] [3.54385362 3.55706091]\n",
            "116 [[-0.00295241 -0.00768055 -0.00587087  0.01639239]\n",
            " [-0.00652634 -0.00357393  0.00410662  0.00997749]\n",
            " [ 0.0135977   0.02012403  0.02369796  0.01959134]] [3.55706091 3.5633162 ]\n",
            "117 [[-0.00652634 -0.00357393  0.00410662  0.00997749]\n",
            " [ 0.0135977   0.02012403  0.02369796  0.01959134]\n",
            " [ 0.00643836 -0.00715934 -0.02728337 -0.05098133]] [3.5633162  3.55248689]\n",
            "118 [[ 0.0135977   0.02012403  0.02369796  0.01959134]\n",
            " [ 0.00643836 -0.00715934 -0.02728337 -0.05098133]\n",
            " [-0.01114699 -0.01758535 -0.01042601  0.01685736]] [3.55248689 3.56303274]\n",
            "119 [[ 0.00643836 -0.00715934 -0.02728337 -0.05098133]\n",
            " [-0.01114699 -0.01758535 -0.01042601  0.01685736]\n",
            " [ 0.01085526  0.02200225  0.0395876   0.05001362]] [3.56303274 3.55277332]\n",
            "120 [[-0.01114699 -0.01758535 -0.01042601  0.01685736]\n",
            " [ 0.01085526  0.02200225  0.0395876   0.05001362]\n",
            " [-0.01056038 -0.02141564 -0.0434179  -0.0830055 ]] [3.55277332 3.54789182]\n",
            "121 [[ 0.01085526  0.02200225  0.0395876   0.05001362]\n",
            " [-0.01056038 -0.02141564 -0.0434179  -0.0830055 ]\n",
            " [-0.00502582  0.00553456  0.0269502   0.0703681 ]] [3.54789182 3.54500905]\n",
            "122 [[-0.01056038 -0.02141564 -0.0434179  -0.0830055 ]\n",
            " [-0.00502582  0.00553456  0.0269502   0.0703681 ]\n",
            " [-0.00296833  0.00205749 -0.00347707 -0.03042727]] [3.54500905 3.54008941]\n",
            "123 [[-0.00502582  0.00553456  0.0269502   0.0703681 ]\n",
            " [-0.00296833  0.00205749 -0.00347707 -0.03042727]\n",
            " [-0.00506626 -0.00209793 -0.00415542 -0.00067835]] [3.54008941 3.53572827]\n",
            "124 [[-0.00296833  0.00205749 -0.00347707 -0.03042727]\n",
            " [-0.00506626 -0.00209793 -0.00415542 -0.00067835]\n",
            " [-0.00449173  0.00057453  0.00267246  0.00682788]] [3.53572827 3.53164068]\n",
            "125 [[-0.00506626 -0.00209793 -0.00415542 -0.00067835]\n",
            " [-0.00449173  0.00057453  0.00267246  0.00682788]\n",
            " [-0.00421053  0.0002812  -0.00029334 -0.0029658 ]] [3.53164068 3.53017679]\n",
            "126 [[-0.00449173  0.00057453  0.00267246  0.00682788]\n",
            " [-0.00421053  0.0002812  -0.00029334 -0.0029658 ]\n",
            " [-0.00150804  0.00270249  0.0024213   0.00271464]] [3.53017679 3.52075665]\n",
            "127 [[-0.00421053  0.0002812  -0.00029334 -0.0029658 ]\n",
            " [-0.00150804  0.00270249  0.0024213   0.00271464]\n",
            " [-0.00970587 -0.00819783 -0.01090033 -0.01332162]] [3.52075665 3.52223441]\n",
            "128 [[-0.00150804  0.00270249  0.0024213   0.00271464]\n",
            " [-0.00970587 -0.00819783 -0.01090033 -0.01332162]\n",
            " [ 0.00152277  0.01122863  0.01942647  0.03032679]] [3.52223441 3.52311996]\n",
            "129 [[-0.00970587 -0.00819783 -0.01090033 -0.01332162]\n",
            " [ 0.00152277  0.01122863  0.01942647  0.03032679]\n",
            " [ 0.00091249 -0.00061028 -0.01183891 -0.03126538]] [3.52311996 3.52929733]\n",
            "130 [[ 0.00152277  0.01122863  0.01942647  0.03032679]\n",
            " [ 0.00091249 -0.00061028 -0.01183891 -0.03126538]\n",
            " [ 0.00636459  0.00545211  0.00606238  0.0179013 ]] [3.52929733 3.54673972]\n",
            "131 [[ 9.12486800e-04 -6.10279000e-04 -1.18389130e-02 -3.12653790e-02]\n",
            " [ 6.36459220e-03  5.45210540e-03  6.06238420e-03  1.79012977e-02]\n",
            " [ 1.79646451e-02  1.16000529e-02  6.14794750e-03  8.55633000e-05]] [3.54673972 3.54990473]\n",
            "132 [[ 6.36459220e-03  5.45210540e-03  6.06238420e-03  1.79012977e-02]\n",
            " [ 1.79646451e-02  1.16000529e-02  6.14794750e-03  8.55633000e-05]\n",
            " [ 3.25877930e-03 -1.47058660e-02 -2.63059190e-02 -3.24538660e-02]] [3.54990473 3.53193317]\n",
            "133 [[ 1.79646451e-02  1.16000529e-02  6.14794750e-03  8.55633000e-05]\n",
            " [ 3.25877930e-03 -1.47058660e-02 -2.63059190e-02 -3.24538660e-02]\n",
            " [-1.85080510e-02 -2.17668300e-02 -7.06096400e-03  1.92449546e-02]] [3.53193317 3.54182848]\n",
            "134 [[ 0.00325878 -0.01470587 -0.02630592 -0.03245387]\n",
            " [-0.01850805 -0.02176683 -0.00706096  0.01924495]\n",
            " [ 0.01019194  0.02869999  0.05046682  0.05752778]] [3.54182848 3.54153887]\n",
            "135 [[-0.01850805 -0.02176683 -0.00706096  0.01924495]\n",
            " [ 0.01019194  0.02869999  0.05046682  0.05752778]\n",
            " [-0.00029825 -0.01049019 -0.03919018 -0.08965699]] [3.54153887 3.53805662]\n",
            "136 [[ 0.01019194  0.02869999  0.05046682  0.05752778]\n",
            " [-0.00029825 -0.01049019 -0.03919018 -0.08965699]\n",
            " [-0.00358632 -0.00328806  0.00720213  0.04639231]] [3.53805662 3.53892828]\n",
            "137 [[-0.00029825 -0.01049019 -0.03919018 -0.08965699]\n",
            " [-0.00358632 -0.00328806  0.00720213  0.04639231]\n",
            " [ 0.00089774  0.00448406  0.00777212  0.00056999]] [3.53892828 3.54182848]\n",
            "138 [[-0.00358632 -0.00328806  0.00720213  0.04639231]\n",
            " [ 0.00089774  0.00448406  0.00777212  0.00056999]\n",
            " [ 0.00298683  0.00208909 -0.00239497 -0.01016708]] [3.54182848 3.54529778]\n",
            "139 [[ 0.00089774  0.00448406  0.00777212  0.00056999]\n",
            " [ 0.00298683  0.00208909 -0.00239497 -0.01016708]\n",
            " [ 0.00357258  0.00058575 -0.00150333  0.00089163]] [3.54529778 3.54673972]\n",
            "140 [[ 0.00298683  0.00208909 -0.00239497 -0.01016708]\n",
            " [ 0.00357258  0.00058575 -0.00150333  0.00089163]\n",
            " [ 0.00148475 -0.00208783 -0.00267359 -0.00117025]] [3.54673972 3.54298625]\n",
            "141 [[ 0.00357258  0.00058575 -0.00150333  0.00089163]\n",
            " [ 0.00148475 -0.00208783 -0.00267359 -0.00117025]\n",
            " [-0.00386506 -0.00534981 -0.00326197 -0.00058839]] [3.54298625 3.54846724]\n",
            "142 [[ 0.00148475 -0.00208783 -0.00267359 -0.00117025]\n",
            " [-0.00386506 -0.00534981 -0.00326197 -0.00058839]\n",
            " [ 0.0056438   0.00950886  0.01485866  0.01812064]] [3.54846724 3.55934005]\n",
            "143 [[-0.00386506 -0.00534981 -0.00326197 -0.00058839]\n",
            " [ 0.0056438   0.00950886  0.01485866  0.01812064]\n",
            " [ 0.01119308  0.00554928 -0.00395958 -0.01881824]] [3.55934005 3.54990473]\n",
            "144 [[ 0.0056438   0.00950886  0.01485866  0.01812064]\n",
            " [ 0.01119308  0.00554928 -0.00395958 -0.01881824]\n",
            " [-0.00971304 -0.02090612 -0.0264554  -0.02249582]] [3.54990473 3.55990907]\n",
            "145 [[ 0.01119308  0.00554928 -0.00395958 -0.01881824]\n",
            " [-0.00971304 -0.02090612 -0.0264554  -0.02249582]\n",
            " [ 0.01029872  0.02001176  0.04091788  0.06737328]] [3.55990907 3.55277332]\n",
            "146 [[-0.00971304 -0.02090612 -0.0264554  -0.02249582]\n",
            " [ 0.01029872  0.02001176  0.04091788  0.06737328]\n",
            " [-0.00734541 -0.01764413 -0.0376559  -0.07857378]] [3.55277332 3.55706091]\n",
            "147 [[ 0.01029872  0.02001176  0.04091788  0.06737328]\n",
            " [-0.00734541 -0.01764413 -0.0376559  -0.07857378]\n",
            " [ 0.00441375  0.01175916  0.0294033   0.0670592 ]] [3.55706091 3.55506236]\n",
            "148 [[-0.00734541 -0.01764413 -0.0376559  -0.07857378]\n",
            " [ 0.00441375  0.01175916  0.0294033   0.0670592 ]\n",
            " [-0.00205728 -0.00647103 -0.01823019 -0.04763349]] [3.55506236 3.56303274]\n",
            "149 [[ 0.00441375  0.01175916  0.0294033   0.0670592 ]\n",
            " [-0.00205728 -0.00647103 -0.01823019 -0.04763349]\n",
            " [ 0.00820391  0.0102612   0.01673223  0.03496242]] [3.56303274 3.56473252]\n",
            "150 [[-0.00205728 -0.00647103 -0.01823019 -0.04763349]\n",
            " [ 0.00820391  0.0102612   0.01673223  0.03496242]\n",
            " [ 0.00174933 -0.00645458 -0.01671578 -0.03344801]] [3.56473252 3.5678411 ]\n",
            "151 [[ 0.00820391  0.0102612   0.01673223  0.03496242]\n",
            " [ 0.00174933 -0.00645458 -0.01671578 -0.03344801]\n",
            " [ 0.00319898  0.00144965  0.00790424  0.02462002]] [3.5678411  3.55591933]\n",
            "152 [[ 0.00174933 -0.00645458 -0.01671578 -0.03344801]\n",
            " [ 0.00319898  0.00144965  0.00790424  0.02462002]\n",
            " [-0.01227006 -0.01546905 -0.0169187  -0.02482294]] [3.55591933 3.55105308]\n",
            "153 [[ 0.00319898  0.00144965  0.00790424  0.02462002]\n",
            " [-0.01227006 -0.01546905 -0.0169187  -0.02482294]\n",
            " [-0.00500965  0.00726042  0.02272947  0.03964817]] [3.55105308 3.51244053]\n",
            "154 [[-0.01227006 -0.01546905 -0.0169187  -0.02482294]\n",
            " [-0.00500965  0.00726042  0.02272947  0.03964817]\n",
            " [-0.03977623 -0.03476658 -0.042027   -0.06475647]] [3.51244053 3.50194728]\n",
            "155 [[-0.00500965  0.00726042  0.02272947  0.03964817]\n",
            " [-0.03977623 -0.03476658 -0.042027   -0.06475647]\n",
            " [-0.01081757  0.02895866  0.06372525  0.10575225]] [3.50194728 3.49802154]\n",
            "156 [[-0.03977623 -0.03476658 -0.042027   -0.06475647]\n",
            " [-0.01081757  0.02895866  0.06372525  0.10575225]\n",
            " [-0.00404799  0.00676958 -0.02218908 -0.08591433]] [3.49802154 3.52075665]\n",
            "157 [[-0.01081757  0.02895866  0.06372525  0.10575225]\n",
            " [-0.00404799  0.00676958 -0.02218908 -0.08591433]\n",
            " [ 0.02343623  0.02748421  0.02071464  0.04290372]] [3.52075665 3.52665454]\n",
            "158 [[-0.00404799  0.00676958 -0.02218908 -0.08591433]\n",
            " [ 0.02343623  0.02748421  0.02071464  0.04290372]\n",
            " [ 0.00607711 -0.01735912 -0.04484333 -0.06555797]] [3.52665454 3.5281237 ]\n",
            "159 [[ 0.02343623  0.02748421  0.02071464  0.04290372]\n",
            " [ 0.00607711 -0.01735912 -0.04484333 -0.06555797]\n",
            " [ 0.00151364 -0.00456347  0.01279565  0.05763898]] [3.5281237  3.53834722]\n",
            "160 [[ 0.00607711 -0.01735912 -0.04484333 -0.06555797]\n",
            " [ 0.00151364 -0.00456347  0.01279565  0.05763898]\n",
            " [ 0.01053113  0.0090175   0.01358097  0.00078532]] [3.53834722 3.53543689]\n",
            "161 [[ 0.00151364 -0.00456347  0.01279565  0.05763898]\n",
            " [ 0.01053113  0.0090175   0.01358097  0.00078532]\n",
            " [-0.00299757 -0.01352871 -0.0225462  -0.03612717]] [3.53543689 3.54124908]\n",
            "162 [[ 0.01053113  0.0090175   0.01358097  0.00078532]\n",
            " [-0.00299757 -0.01352871 -0.0225462  -0.03612717]\n",
            " [ 0.00598616  0.00898373  0.02251244  0.04505865]] [3.54124908 3.53397843]\n",
            "163 [[-0.00299757 -0.01352871 -0.0225462  -0.03612717]\n",
            " [ 0.00598616  0.00898373  0.02251244  0.04505865]\n",
            " [-0.00748843 -0.01347459 -0.02245832 -0.04497076]] [3.53397843 3.54124908]\n",
            "164 [[ 0.00598616  0.00898373  0.02251244  0.04505865]\n",
            " [-0.00748843 -0.01347459 -0.02245832 -0.04497076]\n",
            " [ 0.00748843  0.01497686  0.02845145  0.05090977]] [3.54124908 3.54095932]\n",
            "165 [[-0.00748843 -0.01347459 -0.02245832 -0.04497076]\n",
            " [ 0.00748843  0.01497686  0.02845145  0.05090977]\n",
            " [-0.0002984  -0.00778683 -0.02276369 -0.05121513]] [3.54095932 3.53339461]\n",
            "166 [[ 0.00748843  0.01497686  0.02845145  0.05090977]\n",
            " [-0.0002984  -0.00778683 -0.02276369 -0.05121513]\n",
            " [-0.00779141 -0.007493    0.00029383  0.02305751]] [3.53339461 3.54731592]\n",
            "167 [[-0.0002984  -0.00778683 -0.02276369 -0.05121513]\n",
            " [-0.00779141 -0.007493    0.00029383  0.02305751]\n",
            " [ 0.01433713  0.02212854  0.02962155  0.02932772]] [3.54731592 3.55191363]\n",
            "168 [[-0.00779141 -0.007493    0.00029383  0.02305751]\n",
            " [ 0.01433713  0.02212854  0.02962155  0.02932772]\n",
            " [ 0.00473374 -0.0096034  -0.03173194 -0.06135348]] [3.55191363 3.55877074]\n",
            "169 [[ 0.01433713  0.02212854  0.02962155  0.02932772]\n",
            " [ 0.00473374 -0.0096034  -0.03173194 -0.06135348]\n",
            " [ 0.00705879  0.00232506  0.01192845  0.04366039]] [3.55877074 3.55734606]\n",
            "170 [[ 0.00473374 -0.0096034  -0.03173194 -0.06135348]\n",
            " [ 0.00705879  0.00232506  0.01192845  0.04366039]\n",
            " [-0.00146646 -0.00852526 -0.01085031 -0.02277877]] [3.55734606 3.56047775]\n",
            "171 [[ 0.00705879  0.00232506  0.01192845  0.04366039]\n",
            " [-0.00146646 -0.00852526 -0.01085031 -0.02277877]\n",
            " [ 0.00322345  0.00468992  0.01321517  0.02406549]] [3.56047775 3.57822722]\n",
            "Shape of input/output samples: X_val (22, 3, 4), y_val (22, 2)\n",
            "Input, output samples X_val, y_val:\n",
            "0 [[-0.03597404 -0.05423809 -0.06927869 -0.07962937]\n",
            " [ 0.00445693  0.04043096  0.09466905  0.16394774]\n",
            " [ 0.0029604  -0.00149653 -0.04192749 -0.13659655]] [3.55047914 3.55134006]\n",
            "1 [[ 0.00445693  0.04043096  0.09466905  0.16394774]\n",
            " [ 0.0029604  -0.00149653 -0.04192749 -0.13659655]\n",
            " [ 0.00088636 -0.00207404 -0.00057752  0.04134998]] [3.55134006 3.57262651]\n",
            "2 [[ 0.0029604  -0.00149653 -0.04192749 -0.13659655]\n",
            " [ 0.00088636 -0.00207404 -0.00057752  0.04134998]\n",
            " [ 0.02190828  0.02102192  0.02309597  0.02367348]] [3.57262651 3.57850639]\n",
            "3 [[ 0.00088636 -0.00207404 -0.00057752  0.04134998]\n",
            " [ 0.02190828  0.02102192  0.02309597  0.02367348]\n",
            " [ 0.00604925 -0.01585903 -0.03688095 -0.05997692]] [3.57850639 3.58961149]\n",
            "4 [[ 0.02190828  0.02102192  0.02309597  0.02367348]\n",
            " [ 0.00604925 -0.01585903 -0.03688095 -0.05997692]\n",
            " [ 0.01142221  0.00537296  0.02123199  0.05811295]] [3.58961149 3.58073734]\n",
            "5 [[ 0.00604925 -0.01585903 -0.03688095 -0.05997692]\n",
            " [ 0.01142221  0.00537296  0.02123199  0.05811295]\n",
            " [-0.00912727 -0.02054948 -0.02592244 -0.04715443]] [3.58073734 3.57290725]\n",
            "6 [[ 0.01142221  0.00537296  0.02123199  0.05811295]\n",
            " [-0.00912727 -0.02054948 -0.02592244 -0.04715443]\n",
            " [-0.00805535  0.00107191  0.02162139  0.04754383]] [3.57290725 3.56388294]\n",
            "7 [[-0.00912727 -0.02054948 -0.02592244 -0.04715443]\n",
            " [-0.00805535  0.00107191  0.02162139  0.04754383]\n",
            " [-0.00928619 -0.00123083 -0.00230274 -0.02392413]] [3.56388294 3.57766855]\n",
            "8 [[-0.00805535  0.00107191  0.02162139  0.04754383]\n",
            " [-0.00928619 -0.00123083 -0.00230274 -0.02392413]\n",
            " [ 0.01418468  0.02347087  0.0247017   0.02700445]] [3.57766855 3.59236855]\n",
            "9 [[-0.00928619 -0.00123083 -0.00230274 -0.02392413]\n",
            " [ 0.01418468  0.02347087  0.0247017   0.02700445]\n",
            " [ 0.01511935  0.00093467 -0.0225362  -0.04723791]] [3.59236855 3.59868117]\n",
            "10 [[ 0.01418468  0.02347087  0.0247017   0.02700445]\n",
            " [ 0.01511935  0.00093467 -0.0225362  -0.04723791]\n",
            " [ 0.00649077 -0.00862858 -0.00956325  0.01297295]] [3.59868117 3.59649004]\n",
            "11 [[ 0.01511935  0.00093467 -0.0225362  -0.04723791]\n",
            " [ 0.00649077 -0.00862858 -0.00956325  0.01297295]\n",
            " [-0.00225283 -0.0087436  -0.00011501  0.00944824]] [3.59649004 3.60468231]\n",
            "12 [[ 0.00649077 -0.00862858 -0.00956325  0.01297295]\n",
            " [-0.00225283 -0.0087436  -0.00011501  0.00944824]\n",
            " [ 0.00842226  0.01067509  0.01941869  0.01953371]] [3.60468231 3.61469454]\n",
            "13 [[-0.00225283 -0.0087436  -0.00011501  0.00944824]\n",
            " [ 0.00842226  0.01067509  0.01941869  0.01953371]\n",
            " [ 0.0102907   0.00186844 -0.00880666 -0.02822535]] [3.61469454 3.58185084]\n",
            "14 [[ 0.00842226  0.01067509  0.01941869  0.01953371]\n",
            " [ 0.0102907   0.00186844 -0.00880666 -0.02822535]\n",
            " [-0.033768   -0.0440587  -0.04592714 -0.03712049]] [3.58185084 3.55962466]\n",
            "15 [[ 0.0102907   0.00186844 -0.00880666 -0.02822535]\n",
            " [-0.033768   -0.0440587  -0.04592714 -0.03712049]\n",
            " [-0.02286963  0.01089837  0.05495707  0.10088422]] [3.55962466 3.56840527]\n",
            "16 [[-0.033768   -0.0440587  -0.04592714 -0.03712049]\n",
            " [-0.02286963  0.01089837  0.05495707  0.10088422]\n",
            " [ 0.00903657  0.03190621  0.02100784 -0.03394923]] [3.56840527 3.5695327 ]\n",
            "17 [[-0.02286963  0.01089837  0.05495707  0.10088422]\n",
            " [ 0.00903657  0.03190621  0.02100784 -0.03394923]\n",
            " [ 0.00116012 -0.00787645 -0.03978266 -0.0607905 ]] [3.5695327  3.55791619]\n",
            "18 [[ 0.00903657  0.03190621  0.02100784 -0.03394923]\n",
            " [ 0.00116012 -0.00787645 -0.03978266 -0.0607905 ]\n",
            " [-0.01195523 -0.01311535 -0.0052389   0.03454376]] [3.55791619 3.55420452]\n",
            "19 [[ 0.00116012 -0.00787645 -0.03978266 -0.0607905 ]\n",
            " [-0.01195523 -0.01311535 -0.0052389   0.03454376]\n",
            " [-0.00382076  0.00813448  0.02124983  0.02648873]] [3.55420452 3.58073734]\n",
            "20 [[-0.01195523 -0.01311535 -0.0052389   0.03454376]\n",
            " [-0.00382076  0.00813448  0.02124983  0.02648873]\n",
            " [ 0.02730354  0.03112429  0.02298982  0.00173999]] [3.58073734 3.58157261]\n",
            "21 [[-0.00382076  0.00813448  0.02124983  0.02648873]\n",
            " [ 0.02730354  0.03112429  0.02298982  0.00173999]\n",
            " [ 0.00085919 -0.02644434 -0.05756864 -0.08055846]] [3.58157261 3.58240718]\n",
            "Shape of input/output samples: X_test (22, 3, 4), y_test (22, 2)\n",
            "Input, output samples X_test, y_test:\n",
            "0 [[ 0.00456621  0.00370776  0.00370849 -0.02273511]\n",
            " [ 0.01273184  0.00816563  0.00445787  0.00074937]\n",
            " [-0.01358638 -0.02631822 -0.03448385 -0.03894172]] [3.58601582 3.5810158 ]\n",
            "1 [[ 0.01273184  0.00816563  0.00445787  0.00074937]\n",
            " [-0.01358638 -0.02631822 -0.03448385 -0.03894172]\n",
            " [-0.00514288  0.00844351  0.03476173  0.06924558]] [3.5810158  3.59209322]\n",
            "2 [[-0.01358638 -0.02631822 -0.03448385 -0.03894172]\n",
            " [-0.00514288  0.00844351  0.03476173  0.06924558]\n",
            " [ 0.01139293  0.01653581  0.0080923  -0.02666943]] [3.59209322 3.5698143 ]\n",
            "3 [[-0.00514288  0.00844351  0.03476173  0.06924558]\n",
            " [ 0.01139293  0.01653581  0.0080923  -0.02666943]\n",
            " [-0.02291716 -0.03431009 -0.0508459  -0.05893821]] [3.5698143  3.58129419]\n",
            "4 [[ 0.01139293  0.01653581  0.0080923  -0.02666943]\n",
            " [-0.02291716 -0.03431009 -0.0508459  -0.05893821]\n",
            " [ 0.01181059  0.03472775  0.06903784  0.11988374]] [3.58129419 3.57234559]\n",
            "5 [[-0.02291716 -0.03431009 -0.0508459  -0.05893821]\n",
            " [ 0.01181059  0.03472775  0.06903784  0.11988374]\n",
            " [-0.00920604 -0.02101663 -0.05574438 -0.12478222]] [3.57234559 3.57571079]\n",
            "6 [[ 0.01181059  0.03472775  0.06903784  0.11988374]\n",
            " [-0.00920604 -0.02101663 -0.05574438 -0.12478222]\n",
            " [ 0.00346229  0.01266833  0.03368496  0.08942934]] [3.57571079 3.5562048 ]\n",
            "7 [[-0.00920604 -0.02101663 -0.05574438 -0.12478222]\n",
            " [ 0.00346229  0.01266833  0.03368496  0.08942934]\n",
            " [-0.02007347 -0.02353576 -0.03620409 -0.06988904]] [3.5562048  3.56189798]\n",
            "8 [[ 0.00346229  0.01266833  0.03368496  0.08942934]\n",
            " [-0.02007347 -0.02353576 -0.03620409 -0.06988904]\n",
            " [ 0.00585999  0.02593346  0.04946922  0.08567331]] [3.56189798 3.58462946]\n",
            "9 [[-0.02007347 -0.02353576 -0.03620409 -0.06988904]\n",
            " [ 0.00585999  0.02593346  0.04946922  0.08567331]\n",
            " [ 0.02338785  0.01752787 -0.00840559 -0.05787481]] [3.58462946 3.5887828 ]\n",
            "10 [[ 0.00585999  0.02593346  0.04946922  0.08567331]\n",
            " [ 0.02338785  0.01752787 -0.00840559 -0.05787481]\n",
            " [ 0.00427162 -0.01911623 -0.0366441  -0.02823851]] [3.5887828  3.61172836]\n",
            "11 [[ 0.02338785  0.01752787 -0.00840559 -0.05787481]\n",
            " [ 0.00427162 -0.01911623 -0.0366441  -0.02823851]\n",
            " [ 0.02358998  0.01931836  0.03843459  0.07507869]] [3.61172836 3.62434093]\n",
            "12 [[ 0.00427162 -0.01911623 -0.0366441  -0.02823851]\n",
            " [ 0.02358998  0.01931836  0.03843459  0.07507869]\n",
            " [ 0.01296037 -0.01062961 -0.02994797 -0.06838256]] [3.62434093 3.63968926]\n",
            "13 [[ 0.02358998  0.01931836  0.03843459  0.07507869]\n",
            " [ 0.01296037 -0.01062961 -0.02994797 -0.06838256]\n",
            " [ 0.01576553  0.00280516  0.01343477  0.04338274]] [3.63968926 3.63890106]\n",
            "14 [[ 0.01296037 -0.01062961 -0.02994797 -0.06838256]\n",
            " [ 0.01576553  0.00280516  0.01343477  0.04338274]\n",
            " [-0.00080946 -0.01657499 -0.01938015 -0.03281492]] [3.63890106 3.63230908]\n",
            "15 [[ 0.01576553  0.00280516  0.01343477  0.04338274]\n",
            " [-0.00080946 -0.01657499 -0.01938015 -0.03281492]\n",
            " [-0.00677051 -0.00596105  0.01061394  0.02999409]] [3.63230908 3.63968926]\n",
            "16 [[-0.00080946 -0.01657499 -0.01938015 -0.03281492]\n",
            " [-0.00677051 -0.00596105  0.01061394  0.02999409]\n",
            " [ 0.00757997  0.01435047  0.02031152  0.00969758]] [3.63968926 3.64231183]\n",
            "17 [[-0.00677051 -0.00596105  0.01061394  0.02999409]\n",
            " [ 0.00757997  0.01435047  0.02031152  0.00969758]\n",
            " [ 0.0026932  -0.00488677 -0.01923724 -0.03954876]] [3.64231183 3.62726999]\n",
            "18 [[ 0.00757997  0.01435047  0.02031152  0.00969758]\n",
            " [ 0.0026932  -0.00488677 -0.01923724 -0.03954876]\n",
            " [-0.01544955 -0.01814275 -0.01325598  0.00598126]] [3.62726999 3.62113583]\n",
            "19 [[ 0.0026932  -0.00488677 -0.01923724 -0.03954876]\n",
            " [-0.01544955 -0.01814275 -0.01325598  0.00598126]\n",
            " [-0.00630224  0.00914731  0.02729005  0.04054603]] [3.62113583 3.61952937]\n",
            "20 [[-0.01544955 -0.01814275 -0.01325598  0.00598126]\n",
            " [-0.00630224  0.00914731  0.02729005  0.04054603]\n",
            " [-0.00165066  0.00465158 -0.00449572 -0.03178577]] [3.61952937 3.63863826]\n",
            "21 [[-0.00630224  0.00914731  0.02729005  0.04054603]\n",
            " [-0.00165066  0.00465158 -0.00449572 -0.03178577]\n",
            " [ 0.01962989  0.02128055  0.01662896  0.02112468]] [3.63863826 3.61550194]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpxJUYy612rm",
        "colab_type": "text"
      },
      "source": [
        "## MLP Test Harness: Early Stopping, Model Checkpoint for Hyper Parameter Optimization "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQMNrMFRgklD",
        "colab_type": "text"
      },
      "source": [
        "### Step 1. Define, Fit and Evaluate Performance of Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtfmOB_P13AZ",
        "colab_type": "code",
        "outputId": "0dfbe527-3ba9-4dfc-98c3-845e567d67a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# multivariate multi-step mlp\n",
        "\n",
        "# import libs\n",
        "from pandas import DataFrame\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "# from numpy import hstack\n",
        "from sklearn.metrics import explained_variance_score,r2_score,mean_squared_error,mean_absolute_error,median_absolute_error \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.optimizers import Adam\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# flatten input data for MLP (TO SPEED UP CALCULATIONS IN TF)\n",
        "def flatten(nsteps_input,nfeatures,X_train,X_val,X_test):\n",
        "  \"\"\"MLP: vectorize input samples\"\"\"\n",
        "  # calc new n_input dim\n",
        "  n_input = nsteps_input*nfeatures  # - FLATTENED NUMBER OF COLUMNS IN INPUT SAMPLE\n",
        "  print('n_input: ', n_input)\n",
        "  # flatten train, val, test samples\n",
        "  X_train_flat = X_train.reshape((X_train.shape[0],n_input))\n",
        "  print('X_train shape after reshape vectorization: ', X_train_flat.shape)\n",
        "  X_val_flat = X_val.reshape((X_val.shape[0], n_input)) # CREATES FLATTENED INPUT SAMPLE\n",
        "  print('X_val_flat shape after reshape vectorization: ', X_val_flat.shape)\n",
        "  X_test_flat = X_test.reshape((X_test.shape[0], n_input)) # CREATES FLATTENED INPUT SAMPLE\n",
        "  print('X_test_flat shape after reshape vectorization: ', X_test_flat.shape)\n",
        "  \n",
        "  return n_input,X_train_flat,X_val_flat,X_test_flat \n",
        "\n",
        "\n",
        "# set parameters\n",
        "nsteps_input = 3\n",
        "nfeatures = 4\n",
        "# create flattened X train, val, test samples\n",
        "n_input, X_train_flat, X_val_flat, X_test_flat = flatten(nsteps_input,nfeatures,X_train,X_val,X_test)\n",
        "\n",
        "# model definition, fit, eval\n",
        "def model_def_fit_eval(X_train_flat,y_train,X_val_flat,y_val,wts_fpath,n_nodes=100,n_epochs=200,n_patience=2):\n",
        "  \"Define and Fit - hyper params: n_nodes, n_epochs, estop_criteria, n_patience\"\n",
        "  # model define\n",
        "  model = Sequential()\n",
        "  model.add(Dense(n_nodes, activation='relu', input_dim=n_input))\n",
        "  model.add(Dense(nsteps_output))\n",
        "  # model compile\n",
        "  model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "  # early stopping\n",
        "  early_stop = EarlyStopping(monitor='val_loss', patience=n_patience, verbose=1)\n",
        "  # checkpoint\n",
        "  # filepath=\"weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
        "  filepath= wts_fpath  # \"weights.best_mlp.hdf5\"\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "  callbacks_list = [early_stop, checkpoint]\n",
        "  # model fit\n",
        "  history = model.fit(X_train_flat,y_train,validation_data=(X_val_flat,y_val),epochs=n_epochs,batch_size=1,verbose=1,callbacks=callbacks_list,shuffle=False)\n",
        "  # hyper parameter values\n",
        "  print(f'Number of nodes in the hidden layer: {n_nodes}')\n",
        "  print(f'Number of epoches: {n_epochs}')\n",
        "  print(f'Patience: Epochs before early stop: {n_patience}')\n",
        "  # model evaluate\n",
        "  _, train_mse = model.evaluate(X_train_flat, y_train, verbose=0)\n",
        "  _, val_mse = model.evaluate(X_val_flat, y_val, verbose=0)\n",
        "  print(f'Train: {train_mse:.6f}, Validation: {val_mse:.6f}')\n",
        "  # plot accuracy of model learning\n",
        "  pyplot.plot(history.history['mean_squared_error'], label='train')\n",
        "  pyplot.plot(history.history['val_mean_squared_error'], label='val')\n",
        "  pyplot.legend()\n",
        "  pyplot.show()\n",
        "  return\n",
        "\n",
        "\n",
        "# set parameters and run model\n",
        "if __name__ == '__main__':\n",
        "  # initialize flattened X train, val, test samples\n",
        "  n_input, X_train_flat, X_val_flat, X_test_flat = flatten(nsteps_input,nfeatures,X_train,X_val,X_test)\n",
        "  # file path for weights\n",
        "  wts_fpath = \"weights.best_mlp.hdf5\"\n",
        "  # hyper parameter combos\n",
        "  n_nodes = [50,100,200]\n",
        "  n_epochs = [100,1000,2000]\n",
        "  n_patience = [2,3,4]\n",
        "  # run model\n",
        "  for n in n_nodes:\n",
        "    for e in n_epochs:\n",
        "      for p in n_patience:\n",
        "        model_output = model_def_fit_eval(X_train_flat,y_train,X_val_flat,y_val,wts_fpath,n_nodes=n,n_patience=p)\n",
        "        print(model_output)\n",
        "          \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0815 17:23:59.460281 140656540714880 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0815 17:23:59.500183 140656540714880 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0815 17:23:59.508078 140656540714880 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0815 17:23:59.543820 140656540714880 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "n_input:  12\n",
            "X_train shape after reshape vectorization:  (172, 12)\n",
            "X_val_flat shape after reshape vectorization:  (22, 12)\n",
            "X_test_flat shape after reshape vectorization:  (22, 12)\n",
            "n_input:  12\n",
            "X_train shape after reshape vectorization:  (172, 12)\n",
            "X_val_flat shape after reshape vectorization:  (22, 12)\n",
            "X_test_flat shape after reshape vectorization:  (22, 12)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0815 17:23:59.718052 140656540714880 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "W0815 17:23:59.788584 140656540714880 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 8.2333 - mean_squared_error: 8.2333 - val_loss: 3.2615 - val_mean_squared_error: 3.2615\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.26148, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.6770 - mean_squared_error: 0.6770 - val_loss: 0.0224 - val_mean_squared_error: 0.0224\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.26148 to 0.02238, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0075 - mean_squared_error: 0.0075 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.02238 to 0.00304, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00304 to 0.00219, saving model to weights.best_mlp.hdf5\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00219 to 0.00185, saving model to weights.best_mlp.hdf5\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00185 to 0.00166, saving model to weights.best_mlp.hdf5\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00166 to 0.00158, saving model to weights.best_mlp.hdf5\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00158 to 0.00156, saving model to weights.best_mlp.hdf5\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00156\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00156\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00156\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00156\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00156\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00156\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00156\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00156\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00156\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00156\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.8864e-04 - mean_squared_error: 9.8864e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00156\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.7006e-04 - mean_squared_error: 9.7006e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00156\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.5486e-04 - mean_squared_error: 9.5486e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00156\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.3956e-04 - mean_squared_error: 9.3956e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00156\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.2131e-04 - mean_squared_error: 9.2131e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00156\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.9972e-04 - mean_squared_error: 8.9972e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00156\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.7336e-04 - mean_squared_error: 8.7336e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00156\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.4592e-04 - mean_squared_error: 8.4592e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00156\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.1811e-04 - mean_squared_error: 8.1811e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00156\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.9357e-04 - mean_squared_error: 7.9357e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00156\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.7346e-04 - mean_squared_error: 7.7346e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00156\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.5915e-04 - mean_squared_error: 7.5915e-04 - val_loss: 0.0034 - val_mean_squared_error: 0.0034\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00156\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.4839e-04 - mean_squared_error: 7.4839e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00156\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.3897e-04 - mean_squared_error: 7.3897e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00156\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.3419e-04 - mean_squared_error: 7.3419e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00156\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.2767e-04 - mean_squared_error: 7.2767e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00156\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 996us/step - loss: 7.2043e-04 - mean_squared_error: 7.2043e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00156\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.1272e-04 - mean_squared_error: 7.1272e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00156\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.9319e-04 - mean_squared_error: 6.9319e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00156\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.8208e-04 - mean_squared_error: 6.8208e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00156\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.6016e-04 - mean_squared_error: 6.6016e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00156\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.4405e-04 - mean_squared_error: 6.4405e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00156\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.1922e-04 - mean_squared_error: 6.1922e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00156\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.0039e-04 - mean_squared_error: 6.0039e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00156\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.8664e-04 - mean_squared_error: 5.8664e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00156\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.5975e-04 - mean_squared_error: 5.5975e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.00156 to 0.00147, saving model to weights.best_mlp.hdf5\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 1000us/step - loss: 5.3768e-04 - mean_squared_error: 5.3768e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.00147 to 0.00137, saving model to weights.best_mlp.hdf5\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.1907e-04 - mean_squared_error: 5.1907e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00137 to 0.00128, saving model to weights.best_mlp.hdf5\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.0281e-04 - mean_squared_error: 5.0281e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.00128 to 0.00119, saving model to weights.best_mlp.hdf5\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.8644e-04 - mean_squared_error: 4.8644e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.00119 to 0.00112, saving model to weights.best_mlp.hdf5\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.7587e-04 - mean_squared_error: 4.7587e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.00112 to 0.00105, saving model to weights.best_mlp.hdf5\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.6561e-04 - mean_squared_error: 4.6561e-04 - val_loss: 9.8706e-04 - val_mean_squared_error: 9.8706e-04\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.00105 to 0.00099, saving model to weights.best_mlp.hdf5\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5425e-04 - mean_squared_error: 4.5425e-04 - val_loss: 9.3619e-04 - val_mean_squared_error: 9.3619e-04\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.00099 to 0.00094, saving model to weights.best_mlp.hdf5\n",
            "Epoch 52/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4668e-04 - mean_squared_error: 4.4668e-04 - val_loss: 8.9536e-04 - val_mean_squared_error: 8.9536e-04\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.00094 to 0.00090, saving model to weights.best_mlp.hdf5\n",
            "Epoch 53/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4250e-04 - mean_squared_error: 4.4250e-04 - val_loss: 8.5250e-04 - val_mean_squared_error: 8.5250e-04\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.00090 to 0.00085, saving model to weights.best_mlp.hdf5\n",
            "Epoch 54/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3352e-04 - mean_squared_error: 4.3352e-04 - val_loss: 8.0963e-04 - val_mean_squared_error: 8.0963e-04\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.00085 to 0.00081, saving model to weights.best_mlp.hdf5\n",
            "Epoch 55/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2819e-04 - mean_squared_error: 4.2819e-04 - val_loss: 7.7939e-04 - val_mean_squared_error: 7.7939e-04\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.00081 to 0.00078, saving model to weights.best_mlp.hdf5\n",
            "Epoch 56/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2495e-04 - mean_squared_error: 4.2495e-04 - val_loss: 7.4575e-04 - val_mean_squared_error: 7.4575e-04\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.00078 to 0.00075, saving model to weights.best_mlp.hdf5\n",
            "Epoch 57/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1952e-04 - mean_squared_error: 4.1952e-04 - val_loss: 7.2480e-04 - val_mean_squared_error: 7.2480e-04\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.00075 to 0.00072, saving model to weights.best_mlp.hdf5\n",
            "Epoch 58/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1831e-04 - mean_squared_error: 4.1831e-04 - val_loss: 6.9709e-04 - val_mean_squared_error: 6.9709e-04\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.00072 to 0.00070, saving model to weights.best_mlp.hdf5\n",
            "Epoch 59/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1522e-04 - mean_squared_error: 4.1522e-04 - val_loss: 6.8020e-04 - val_mean_squared_error: 6.8020e-04\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.00070 to 0.00068, saving model to weights.best_mlp.hdf5\n",
            "Epoch 60/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1356e-04 - mean_squared_error: 4.1356e-04 - val_loss: 6.5720e-04 - val_mean_squared_error: 6.5720e-04\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.00068 to 0.00066, saving model to weights.best_mlp.hdf5\n",
            "Epoch 61/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0660e-04 - mean_squared_error: 4.0660e-04 - val_loss: 6.4489e-04 - val_mean_squared_error: 6.4489e-04\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.00066 to 0.00064, saving model to weights.best_mlp.hdf5\n",
            "Epoch 62/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0640e-04 - mean_squared_error: 4.0640e-04 - val_loss: 6.4261e-04 - val_mean_squared_error: 6.4261e-04\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.00064 to 0.00064, saving model to weights.best_mlp.hdf5\n",
            "Epoch 63/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0422e-04 - mean_squared_error: 4.0422e-04 - val_loss: 6.2095e-04 - val_mean_squared_error: 6.2095e-04\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.00064 to 0.00062, saving model to weights.best_mlp.hdf5\n",
            "Epoch 64/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0369e-04 - mean_squared_error: 4.0369e-04 - val_loss: 6.1357e-04 - val_mean_squared_error: 6.1357e-04\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.00062 to 0.00061, saving model to weights.best_mlp.hdf5\n",
            "Epoch 65/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0258e-04 - mean_squared_error: 4.0258e-04 - val_loss: 6.1256e-04 - val_mean_squared_error: 6.1256e-04\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.00061 to 0.00061, saving model to weights.best_mlp.hdf5\n",
            "Epoch 66/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9981e-04 - mean_squared_error: 3.9981e-04 - val_loss: 6.0227e-04 - val_mean_squared_error: 6.0227e-04\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.00061 to 0.00060, saving model to weights.best_mlp.hdf5\n",
            "Epoch 67/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9871e-04 - mean_squared_error: 3.9871e-04 - val_loss: 5.9340e-04 - val_mean_squared_error: 5.9340e-04\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.00060 to 0.00059, saving model to weights.best_mlp.hdf5\n",
            "Epoch 68/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8489e-04 - mean_squared_error: 3.8489e-04 - val_loss: 5.8952e-04 - val_mean_squared_error: 5.8952e-04\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.00059 to 0.00059, saving model to weights.best_mlp.hdf5\n",
            "Epoch 69/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8765e-04 - mean_squared_error: 3.8765e-04 - val_loss: 5.7835e-04 - val_mean_squared_error: 5.7835e-04\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.00059 to 0.00058, saving model to weights.best_mlp.hdf5\n",
            "Epoch 70/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.7640e-04 - mean_squared_error: 3.7640e-04 - val_loss: 5.6393e-04 - val_mean_squared_error: 5.6393e-04\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.00058 to 0.00056, saving model to weights.best_mlp.hdf5\n",
            "Epoch 71/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8119e-04 - mean_squared_error: 3.8119e-04 - val_loss: 5.6730e-04 - val_mean_squared_error: 5.6730e-04\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.00056\n",
            "Epoch 72/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8234e-04 - mean_squared_error: 3.8234e-04 - val_loss: 5.5779e-04 - val_mean_squared_error: 5.5779e-04\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.00056 to 0.00056, saving model to weights.best_mlp.hdf5\n",
            "Epoch 00072: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 50\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 2\n",
            "MLP: Train: 0.009634, Validation: 0.000558\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGwDqDMENAHWG4AaAOvP/BUR062FeQ0UAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.3794 - mean_squared_error: 8.3794 - val_loss: 3.7453 - val_mean_squared_error: 3.7453\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.74532, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.8649 - mean_squared_error: 0.8649 - val_loss: 0.0225 - val_mean_squared_error: 0.0225\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.74532 to 0.02255, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.02255 to 0.00271, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0040 - mean_squared_error: 0.0040 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00271 to 0.00223, saving model to weights.best_mlp.hdf5\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00223 to 0.00196, saving model to weights.best_mlp.hdf5\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00196 to 0.00181, saving model to weights.best_mlp.hdf5\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00181 to 0.00174, saving model to weights.best_mlp.hdf5\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00174 to 0.00174, saving model to weights.best_mlp.hdf5\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00174\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00174\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00174\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00174\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00174\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00174\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00174\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00174\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00174\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00174\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00174\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00174\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.8933e-04 - mean_squared_error: 9.8933e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00174\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.7085e-04 - mean_squared_error: 9.7085e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00174\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.5110e-04 - mean_squared_error: 9.5110e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00174\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.2996e-04 - mean_squared_error: 9.2996e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00174\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.0681e-04 - mean_squared_error: 9.0681e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00174\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.7954e-04 - mean_squared_error: 8.7954e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00174\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.5097e-04 - mean_squared_error: 8.5097e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00174\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.2344e-04 - mean_squared_error: 8.2344e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00174\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.9827e-04 - mean_squared_error: 7.9827e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00174\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.7700e-04 - mean_squared_error: 7.7700e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00174\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.5652e-04 - mean_squared_error: 7.5652e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00174\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.4063e-04 - mean_squared_error: 7.4063e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00174\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.3300e-04 - mean_squared_error: 7.3300e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00174\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.2954e-04 - mean_squared_error: 7.2954e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00174\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.2749e-04 - mean_squared_error: 7.2749e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00174\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.1495e-04 - mean_squared_error: 7.1495e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00174\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.1242e-04 - mean_squared_error: 7.1242e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00174\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.0018e-04 - mean_squared_error: 7.0018e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00174\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.7926e-04 - mean_squared_error: 6.7926e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00174\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.6458e-04 - mean_squared_error: 6.6458e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00174\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.4574e-04 - mean_squared_error: 6.4574e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00174\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.3166e-04 - mean_squared_error: 6.3166e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00174\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.1126e-04 - mean_squared_error: 6.1126e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00174\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.9348e-04 - mean_squared_error: 5.9348e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00174\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.7200e-04 - mean_squared_error: 5.7200e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.00174 to 0.00162, saving model to weights.best_mlp.hdf5\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.4937e-04 - mean_squared_error: 5.4937e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00162 to 0.00151, saving model to weights.best_mlp.hdf5\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.3030e-04 - mean_squared_error: 5.3030e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.00151 to 0.00143, saving model to weights.best_mlp.hdf5\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.1472e-04 - mean_squared_error: 5.1472e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.00143 to 0.00134, saving model to weights.best_mlp.hdf5\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.9377e-04 - mean_squared_error: 4.9377e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.00134 to 0.00125, saving model to weights.best_mlp.hdf5\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.7555e-04 - mean_squared_error: 4.7555e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.00125 to 0.00119, saving model to weights.best_mlp.hdf5\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.6188e-04 - mean_squared_error: 4.6188e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.00119 to 0.00112, saving model to weights.best_mlp.hdf5\n",
            "Epoch 52/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4821e-04 - mean_squared_error: 4.4821e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.00112 to 0.00107, saving model to weights.best_mlp.hdf5\n",
            "Epoch 53/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3646e-04 - mean_squared_error: 4.3646e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.00107 to 0.00103, saving model to weights.best_mlp.hdf5\n",
            "Epoch 54/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3245e-04 - mean_squared_error: 4.3245e-04 - val_loss: 9.8295e-04 - val_mean_squared_error: 9.8295e-04\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.00103 to 0.00098, saving model to weights.best_mlp.hdf5\n",
            "Epoch 55/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1401e-04 - mean_squared_error: 4.1401e-04 - val_loss: 9.3551e-04 - val_mean_squared_error: 9.3551e-04\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.00098 to 0.00094, saving model to weights.best_mlp.hdf5\n",
            "Epoch 56/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1098e-04 - mean_squared_error: 4.1098e-04 - val_loss: 8.9821e-04 - val_mean_squared_error: 8.9821e-04\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.00094 to 0.00090, saving model to weights.best_mlp.hdf5\n",
            "Epoch 57/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0193e-04 - mean_squared_error: 4.0193e-04 - val_loss: 8.6083e-04 - val_mean_squared_error: 8.6083e-04\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.00090 to 0.00086, saving model to weights.best_mlp.hdf5\n",
            "Epoch 58/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9937e-04 - mean_squared_error: 3.9937e-04 - val_loss: 8.1632e-04 - val_mean_squared_error: 8.1632e-04\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.00086 to 0.00082, saving model to weights.best_mlp.hdf5\n",
            "Epoch 59/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9757e-04 - mean_squared_error: 3.9757e-04 - val_loss: 7.9417e-04 - val_mean_squared_error: 7.9417e-04\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.00082 to 0.00079, saving model to weights.best_mlp.hdf5\n",
            "Epoch 60/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8521e-04 - mean_squared_error: 3.8521e-04 - val_loss: 7.6056e-04 - val_mean_squared_error: 7.6056e-04\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.00079 to 0.00076, saving model to weights.best_mlp.hdf5\n",
            "Epoch 61/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8894e-04 - mean_squared_error: 3.8894e-04 - val_loss: 7.5044e-04 - val_mean_squared_error: 7.5044e-04\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.00076 to 0.00075, saving model to weights.best_mlp.hdf5\n",
            "Epoch 62/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8368e-04 - mean_squared_error: 3.8368e-04 - val_loss: 7.3014e-04 - val_mean_squared_error: 7.3014e-04\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.00075 to 0.00073, saving model to weights.best_mlp.hdf5\n",
            "Epoch 63/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8299e-04 - mean_squared_error: 3.8299e-04 - val_loss: 7.1222e-04 - val_mean_squared_error: 7.1222e-04\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.00073 to 0.00071, saving model to weights.best_mlp.hdf5\n",
            "Epoch 64/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8634e-04 - mean_squared_error: 3.8634e-04 - val_loss: 6.8037e-04 - val_mean_squared_error: 6.8037e-04\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.00071 to 0.00068, saving model to weights.best_mlp.hdf5\n",
            "Epoch 65/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.7395e-04 - mean_squared_error: 3.7395e-04 - val_loss: 6.6419e-04 - val_mean_squared_error: 6.6419e-04\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.00068 to 0.00066, saving model to weights.best_mlp.hdf5\n",
            "Epoch 66/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.7797e-04 - mean_squared_error: 3.7797e-04 - val_loss: 6.5029e-04 - val_mean_squared_error: 6.5029e-04\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.00066 to 0.00065, saving model to weights.best_mlp.hdf5\n",
            "Epoch 67/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.7636e-04 - mean_squared_error: 3.7636e-04 - val_loss: 6.4454e-04 - val_mean_squared_error: 6.4454e-04\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.00065 to 0.00064, saving model to weights.best_mlp.hdf5\n",
            "Epoch 68/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.7657e-04 - mean_squared_error: 3.7657e-04 - val_loss: 6.3640e-04 - val_mean_squared_error: 6.3640e-04\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.00064 to 0.00064, saving model to weights.best_mlp.hdf5\n",
            "Epoch 00068: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 50\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 3\n",
            "MLP: Train: 0.009428, Validation: 0.000636\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAD8CAYAAAC4uSVNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOtmVtVZkWhYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.7979 - mean_squared_error: 8.7979 - val_loss: 4.4177 - val_mean_squared_error: 4.4177\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 4.41769, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 1.1026 - mean_squared_error: 1.1026 - val_loss: 0.0455 - val_mean_squared_error: 0.0455\n",
            "\n",
            "Epoch 00002: val_loss improved from 4.41769 to 0.04549, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0067 - mean_squared_error: 0.0067 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.04549 to 0.00399, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00399 to 0.00291, saving model to weights.best_mlp.hdf5\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0040 - mean_squared_error: 0.0040 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00291 to 0.00237, saving model to weights.best_mlp.hdf5\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00237 to 0.00204, saving model to weights.best_mlp.hdf5\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00204 to 0.00184, saving model to weights.best_mlp.hdf5\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00184 to 0.00173, saving model to weights.best_mlp.hdf5\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.00173 to 0.00170, saving model to weights.best_mlp.hdf5\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00170\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00170\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00170\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00170\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00170\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00170\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00170\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00170\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00170\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00170\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00170\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00170\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00170\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.9471e-04 - mean_squared_error: 9.9471e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00170\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.7766e-04 - mean_squared_error: 9.7766e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00170\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.5802e-04 - mean_squared_error: 9.5802e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00170\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.3394e-04 - mean_squared_error: 9.3394e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00170\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.0719e-04 - mean_squared_error: 9.0719e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00170\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.7552e-04 - mean_squared_error: 8.7552e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00170\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.4533e-04 - mean_squared_error: 8.4533e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00170\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.1833e-04 - mean_squared_error: 8.1833e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00170\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.9542e-04 - mean_squared_error: 7.9542e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00170\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.7904e-04 - mean_squared_error: 7.7904e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00170\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.6744e-04 - mean_squared_error: 7.6744e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00170\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.5768e-04 - mean_squared_error: 7.5768e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00170\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.5258e-04 - mean_squared_error: 7.5258e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00170\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.4196e-04 - mean_squared_error: 7.4196e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00170\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.3877e-04 - mean_squared_error: 7.3877e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00170\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.2903e-04 - mean_squared_error: 7.2903e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00170\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.2067e-04 - mean_squared_error: 7.2067e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00170\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.0946e-04 - mean_squared_error: 7.0946e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00170\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.9541e-04 - mean_squared_error: 6.9541e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00170\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.8131e-04 - mean_squared_error: 6.8131e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00170\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.7065e-04 - mean_squared_error: 6.7065e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00170\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.5141e-04 - mean_squared_error: 6.5141e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00170\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.3582e-04 - mean_squared_error: 6.3582e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00170\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.1422e-04 - mean_squared_error: 6.1422e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.00170\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.0294e-04 - mean_squared_error: 6.0294e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00170\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.7919e-04 - mean_squared_error: 5.7919e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.00170 to 0.00167, saving model to weights.best_mlp.hdf5\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.6154e-04 - mean_squared_error: 5.6154e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.00167 to 0.00159, saving model to weights.best_mlp.hdf5\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.4389e-04 - mean_squared_error: 5.4389e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.00159 to 0.00151, saving model to weights.best_mlp.hdf5\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.2797e-04 - mean_squared_error: 5.2797e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.00151 to 0.00143, saving model to weights.best_mlp.hdf5\n",
            "Epoch 52/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.1438e-04 - mean_squared_error: 5.1438e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.00143 to 0.00135, saving model to weights.best_mlp.hdf5\n",
            "Epoch 53/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.0845e-04 - mean_squared_error: 5.0845e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.00135 to 0.00130, saving model to weights.best_mlp.hdf5\n",
            "Epoch 54/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.8516e-04 - mean_squared_error: 4.8516e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.00130 to 0.00124, saving model to weights.best_mlp.hdf5\n",
            "Epoch 55/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.8773e-04 - mean_squared_error: 4.8773e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.00124 to 0.00118, saving model to weights.best_mlp.hdf5\n",
            "Epoch 56/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.6712e-04 - mean_squared_error: 4.6712e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.00118 to 0.00115, saving model to weights.best_mlp.hdf5\n",
            "Epoch 57/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.7092e-04 - mean_squared_error: 4.7092e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.00115 to 0.00111, saving model to weights.best_mlp.hdf5\n",
            "Epoch 58/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3918e-04 - mean_squared_error: 4.3918e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.00111 to 0.00107, saving model to weights.best_mlp.hdf5\n",
            "Epoch 59/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3484e-04 - mean_squared_error: 4.3484e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.00107 to 0.00103, saving model to weights.best_mlp.hdf5\n",
            "Epoch 60/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3277e-04 - mean_squared_error: 4.3277e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.00103 to 0.00101, saving model to weights.best_mlp.hdf5\n",
            "Epoch 61/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1941e-04 - mean_squared_error: 4.1941e-04 - val_loss: 9.7831e-04 - val_mean_squared_error: 9.7831e-04\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.00101 to 0.00098, saving model to weights.best_mlp.hdf5\n",
            "Epoch 62/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1827e-04 - mean_squared_error: 4.1827e-04 - val_loss: 9.6340e-04 - val_mean_squared_error: 9.6340e-04\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.00098 to 0.00096, saving model to weights.best_mlp.hdf5\n",
            "Epoch 63/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0357e-04 - mean_squared_error: 4.0357e-04 - val_loss: 9.3771e-04 - val_mean_squared_error: 9.3771e-04\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.00096 to 0.00094, saving model to weights.best_mlp.hdf5\n",
            "Epoch 64/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0640e-04 - mean_squared_error: 4.0640e-04 - val_loss: 9.0109e-04 - val_mean_squared_error: 9.0109e-04\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.00094 to 0.00090, saving model to weights.best_mlp.hdf5\n",
            "Epoch 65/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8805e-04 - mean_squared_error: 3.8805e-04 - val_loss: 8.8178e-04 - val_mean_squared_error: 8.8178e-04\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.00090 to 0.00088, saving model to weights.best_mlp.hdf5\n",
            "Epoch 66/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9161e-04 - mean_squared_error: 3.9161e-04 - val_loss: 8.5012e-04 - val_mean_squared_error: 8.5012e-04\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.00088 to 0.00085, saving model to weights.best_mlp.hdf5\n",
            "Epoch 67/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8087e-04 - mean_squared_error: 3.8087e-04 - val_loss: 8.3814e-04 - val_mean_squared_error: 8.3814e-04\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.00085 to 0.00084, saving model to weights.best_mlp.hdf5\n",
            "Epoch 68/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9157e-04 - mean_squared_error: 3.9157e-04 - val_loss: 8.0493e-04 - val_mean_squared_error: 8.0493e-04\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.00084 to 0.00080, saving model to weights.best_mlp.hdf5\n",
            "Epoch 69/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.6573e-04 - mean_squared_error: 3.6573e-04 - val_loss: 8.1049e-04 - val_mean_squared_error: 8.1049e-04\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.00080\n",
            "Epoch 70/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8645e-04 - mean_squared_error: 3.8645e-04 - val_loss: 7.7586e-04 - val_mean_squared_error: 7.7586e-04\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.00080 to 0.00078, saving model to weights.best_mlp.hdf5\n",
            "Epoch 71/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.7258e-04 - mean_squared_error: 3.7258e-04 - val_loss: 7.5736e-04 - val_mean_squared_error: 7.5736e-04\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.00078 to 0.00076, saving model to weights.best_mlp.hdf5\n",
            "Epoch 72/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.7439e-04 - mean_squared_error: 3.7439e-04 - val_loss: 7.4747e-04 - val_mean_squared_error: 7.4747e-04\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.00076 to 0.00075, saving model to weights.best_mlp.hdf5\n",
            "Epoch 73/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.7351e-04 - mean_squared_error: 3.7351e-04 - val_loss: 7.5904e-04 - val_mean_squared_error: 7.5904e-04\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.00075\n",
            "Epoch 00073: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 50\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 4\n",
            "MLP: Train: 0.008942, Validation: 0.000759\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRbaEs+wuWaQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 0s 3ms/step - loss: 8.5611 - mean_squared_error: 8.5611 - val_loss: 3.9155 - val_mean_squared_error: 3.9155\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.91550, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.9275 - mean_squared_error: 0.9275 - val_loss: 0.0541 - val_mean_squared_error: 0.0541\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.91550 to 0.05409, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0114 - mean_squared_error: 0.0114 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.05409 to 0.00426, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00426 to 0.00259, saving model to weights.best_mlp.hdf5\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00259 to 0.00206, saving model to weights.best_mlp.hdf5\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00206 to 0.00182, saving model to weights.best_mlp.hdf5\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00182 to 0.00172, saving model to weights.best_mlp.hdf5\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00172 to 0.00171, saving model to weights.best_mlp.hdf5\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00171\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00171\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00171\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00171\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00171\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00171\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00171\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00171\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00171\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00171\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00171\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00171\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.9526e-04 - mean_squared_error: 9.9526e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00171\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.7392e-04 - mean_squared_error: 9.7392e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00171\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.4871e-04 - mean_squared_error: 9.4871e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00171\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.2090e-04 - mean_squared_error: 9.2090e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00171\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.9172e-04 - mean_squared_error: 8.9172e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00171\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.6285e-04 - mean_squared_error: 8.6285e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00171\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.3628e-04 - mean_squared_error: 8.3628e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00171\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.1408e-04 - mean_squared_error: 8.1408e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00171\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.9623e-04 - mean_squared_error: 7.9623e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00171\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.8129e-04 - mean_squared_error: 7.8129e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00171\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.7141e-04 - mean_squared_error: 7.7141e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00171\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.6310e-04 - mean_squared_error: 7.6310e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00171\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.5747e-04 - mean_squared_error: 7.5747e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00171\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.5178e-04 - mean_squared_error: 7.5178e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00171\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.4539e-04 - mean_squared_error: 7.4539e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00171\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.3409e-04 - mean_squared_error: 7.3409e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00171\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.2511e-04 - mean_squared_error: 7.2511e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00171\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.1003e-04 - mean_squared_error: 7.1003e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00171\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.9385e-04 - mean_squared_error: 6.9385e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00171\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.6923e-04 - mean_squared_error: 6.6923e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00171\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.5026e-04 - mean_squared_error: 6.5026e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00171\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.2894e-04 - mean_squared_error: 6.2894e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.00171 to 0.00161, saving model to weights.best_mlp.hdf5\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.0231e-04 - mean_squared_error: 6.0231e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.00161 to 0.00151, saving model to weights.best_mlp.hdf5\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.8111e-04 - mean_squared_error: 5.8111e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.00151 to 0.00141, saving model to weights.best_mlp.hdf5\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.5987e-04 - mean_squared_error: 5.5987e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.00141 to 0.00130, saving model to weights.best_mlp.hdf5\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.3915e-04 - mean_squared_error: 5.3915e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00130 to 0.00122, saving model to weights.best_mlp.hdf5\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.2300e-04 - mean_squared_error: 5.2300e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.00122 to 0.00114, saving model to weights.best_mlp.hdf5\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.0751e-04 - mean_squared_error: 5.0751e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.00114 to 0.00108, saving model to weights.best_mlp.hdf5\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.9583e-04 - mean_squared_error: 4.9583e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.00108 to 0.00101, saving model to weights.best_mlp.hdf5\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.8350e-04 - mean_squared_error: 4.8350e-04 - val_loss: 9.6266e-04 - val_mean_squared_error: 9.6266e-04\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.00101 to 0.00096, saving model to weights.best_mlp.hdf5\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.7031e-04 - mean_squared_error: 4.7031e-04 - val_loss: 9.0918e-04 - val_mean_squared_error: 9.0918e-04\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.00096 to 0.00091, saving model to weights.best_mlp.hdf5\n",
            "Epoch 52/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.6143e-04 - mean_squared_error: 4.6143e-04 - val_loss: 8.6454e-04 - val_mean_squared_error: 8.6454e-04\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.00091 to 0.00086, saving model to weights.best_mlp.hdf5\n",
            "Epoch 53/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4625e-04 - mean_squared_error: 4.4625e-04 - val_loss: 8.1788e-04 - val_mean_squared_error: 8.1788e-04\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.00086 to 0.00082, saving model to weights.best_mlp.hdf5\n",
            "Epoch 54/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3874e-04 - mean_squared_error: 4.3874e-04 - val_loss: 7.7474e-04 - val_mean_squared_error: 7.7474e-04\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.00082 to 0.00077, saving model to weights.best_mlp.hdf5\n",
            "Epoch 55/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3409e-04 - mean_squared_error: 4.3409e-04 - val_loss: 7.4002e-04 - val_mean_squared_error: 7.4002e-04\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.00077 to 0.00074, saving model to weights.best_mlp.hdf5\n",
            "Epoch 56/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1721e-04 - mean_squared_error: 4.1721e-04 - val_loss: 7.0538e-04 - val_mean_squared_error: 7.0538e-04\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.00074 to 0.00071, saving model to weights.best_mlp.hdf5\n",
            "Epoch 57/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2291e-04 - mean_squared_error: 4.2291e-04 - val_loss: 6.6959e-04 - val_mean_squared_error: 6.6959e-04\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.00071 to 0.00067, saving model to weights.best_mlp.hdf5\n",
            "Epoch 58/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1615e-04 - mean_squared_error: 4.1615e-04 - val_loss: 6.5186e-04 - val_mean_squared_error: 6.5186e-04\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.00067 to 0.00065, saving model to weights.best_mlp.hdf5\n",
            "Epoch 59/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2710e-04 - mean_squared_error: 4.2710e-04 - val_loss: 6.2372e-04 - val_mean_squared_error: 6.2372e-04\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.00065 to 0.00062, saving model to weights.best_mlp.hdf5\n",
            "Epoch 60/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0635e-04 - mean_squared_error: 4.0635e-04 - val_loss: 6.1363e-04 - val_mean_squared_error: 6.1363e-04\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.00062 to 0.00061, saving model to weights.best_mlp.hdf5\n",
            "Epoch 61/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3610e-04 - mean_squared_error: 4.3610e-04 - val_loss: 5.9465e-04 - val_mean_squared_error: 5.9465e-04\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.00061 to 0.00059, saving model to weights.best_mlp.hdf5\n",
            "Epoch 62/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0134e-04 - mean_squared_error: 4.0134e-04 - val_loss: 5.8696e-04 - val_mean_squared_error: 5.8696e-04\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.00059 to 0.00059, saving model to weights.best_mlp.hdf5\n",
            "Epoch 63/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2156e-04 - mean_squared_error: 4.2156e-04 - val_loss: 5.7519e-04 - val_mean_squared_error: 5.7519e-04\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.00059 to 0.00058, saving model to weights.best_mlp.hdf5\n",
            "Epoch 64/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9729e-04 - mean_squared_error: 3.9729e-04 - val_loss: 5.7049e-04 - val_mean_squared_error: 5.7049e-04\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.00058 to 0.00057, saving model to weights.best_mlp.hdf5\n",
            "Epoch 65/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2326e-04 - mean_squared_error: 4.2326e-04 - val_loss: 5.6364e-04 - val_mean_squared_error: 5.6364e-04\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.00057 to 0.00056, saving model to weights.best_mlp.hdf5\n",
            "Epoch 66/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9742e-04 - mean_squared_error: 3.9742e-04 - val_loss: 5.6331e-04 - val_mean_squared_error: 5.6331e-04\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.00056 to 0.00056, saving model to weights.best_mlp.hdf5\n",
            "Epoch 00066: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 50\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 2\n",
            "MLP: Train: 0.009549, Validation: 0.000563\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 1s 3ms/step - loss: 8.3656 - mean_squared_error: 8.3656 - val_loss: 3.6678 - val_mean_squared_error: 3.6678\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.66783, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.7999 - mean_squared_error: 0.7999 - val_loss: 0.0149 - val_mean_squared_error: 0.0149\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.66783 to 0.01488, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.01488 to 0.00228, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00228 to 0.00165, saving model to weights.best_mlp.hdf5\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00165 to 0.00140, saving model to weights.best_mlp.hdf5\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00140 to 0.00132, saving model to weights.best_mlp.hdf5\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00132\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00132\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00132\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00132\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00132\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00132\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00132\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00132\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00132\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00132\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00132\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00132\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00132\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00132\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00132\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00132\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00132\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.8593e-04 - mean_squared_error: 9.8593e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00132\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.6049e-04 - mean_squared_error: 9.6049e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00132\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.3204e-04 - mean_squared_error: 9.3204e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00132\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.0324e-04 - mean_squared_error: 9.0324e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00132\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.7526e-04 - mean_squared_error: 8.7526e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00132\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.5209e-04 - mean_squared_error: 8.5209e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00132\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.3289e-04 - mean_squared_error: 8.3289e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00132\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.1747e-04 - mean_squared_error: 8.1747e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00132\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.0857e-04 - mean_squared_error: 8.0857e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00132\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.9983e-04 - mean_squared_error: 7.9983e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00132\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.9159e-04 - mean_squared_error: 7.9159e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00132\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.8421e-04 - mean_squared_error: 7.8421e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00132\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.7701e-04 - mean_squared_error: 7.7701e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00132\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.6996e-04 - mean_squared_error: 7.6996e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00132\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.5569e-04 - mean_squared_error: 7.5569e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00132\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.4311e-04 - mean_squared_error: 7.4311e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00132\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.3221e-04 - mean_squared_error: 7.3221e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00132\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.1721e-04 - mean_squared_error: 7.1721e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00132\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.9695e-04 - mean_squared_error: 6.9695e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00132\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.7827e-04 - mean_squared_error: 6.7827e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00132\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.6672e-04 - mean_squared_error: 6.6672e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00132\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.4519e-04 - mean_squared_error: 6.4519e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00132\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.3157e-04 - mean_squared_error: 6.3157e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.00132\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.0778e-04 - mean_squared_error: 6.0778e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00132\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.9114e-04 - mean_squared_error: 5.9114e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.00132\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.6340e-04 - mean_squared_error: 5.6340e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.00132 to 0.00127, saving model to weights.best_mlp.hdf5\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.5266e-04 - mean_squared_error: 5.5266e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.00127 to 0.00120, saving model to weights.best_mlp.hdf5\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.3309e-04 - mean_squared_error: 5.3309e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.00120 to 0.00116, saving model to weights.best_mlp.hdf5\n",
            "Epoch 52/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.2660e-04 - mean_squared_error: 5.2660e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.00116 to 0.00110, saving model to weights.best_mlp.hdf5\n",
            "Epoch 53/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.0894e-04 - mean_squared_error: 5.0894e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.00110 to 0.00106, saving model to weights.best_mlp.hdf5\n",
            "Epoch 54/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.9431e-04 - mean_squared_error: 4.9431e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.00106 to 0.00102, saving model to weights.best_mlp.hdf5\n",
            "Epoch 55/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.8087e-04 - mean_squared_error: 4.8087e-04 - val_loss: 9.7258e-04 - val_mean_squared_error: 9.7258e-04\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.00102 to 0.00097, saving model to weights.best_mlp.hdf5\n",
            "Epoch 56/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.7714e-04 - mean_squared_error: 4.7714e-04 - val_loss: 9.3756e-04 - val_mean_squared_error: 9.3756e-04\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.00097 to 0.00094, saving model to weights.best_mlp.hdf5\n",
            "Epoch 57/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.6486e-04 - mean_squared_error: 4.6486e-04 - val_loss: 9.0893e-04 - val_mean_squared_error: 9.0893e-04\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.00094 to 0.00091, saving model to weights.best_mlp.hdf5\n",
            "Epoch 58/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.6182e-04 - mean_squared_error: 4.6182e-04 - val_loss: 8.7249e-04 - val_mean_squared_error: 8.7249e-04\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.00091 to 0.00087, saving model to weights.best_mlp.hdf5\n",
            "Epoch 59/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5106e-04 - mean_squared_error: 4.5106e-04 - val_loss: 8.5165e-04 - val_mean_squared_error: 8.5165e-04\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.00087 to 0.00085, saving model to weights.best_mlp.hdf5\n",
            "Epoch 60/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4130e-04 - mean_squared_error: 4.4130e-04 - val_loss: 8.2421e-04 - val_mean_squared_error: 8.2421e-04\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.00085 to 0.00082, saving model to weights.best_mlp.hdf5\n",
            "Epoch 61/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3398e-04 - mean_squared_error: 4.3398e-04 - val_loss: 8.0074e-04 - val_mean_squared_error: 8.0074e-04\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.00082 to 0.00080, saving model to weights.best_mlp.hdf5\n",
            "Epoch 62/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2501e-04 - mean_squared_error: 4.2501e-04 - val_loss: 7.6625e-04 - val_mean_squared_error: 7.6625e-04\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.00080 to 0.00077, saving model to weights.best_mlp.hdf5\n",
            "Epoch 63/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1661e-04 - mean_squared_error: 4.1661e-04 - val_loss: 7.5175e-04 - val_mean_squared_error: 7.5175e-04\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.00077 to 0.00075, saving model to weights.best_mlp.hdf5\n",
            "Epoch 64/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1565e-04 - mean_squared_error: 4.1565e-04 - val_loss: 7.3262e-04 - val_mean_squared_error: 7.3262e-04\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.00075 to 0.00073, saving model to weights.best_mlp.hdf5\n",
            "Epoch 65/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0475e-04 - mean_squared_error: 4.0475e-04 - val_loss: 7.1267e-04 - val_mean_squared_error: 7.1267e-04\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.00073 to 0.00071, saving model to weights.best_mlp.hdf5\n",
            "Epoch 66/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0695e-04 - mean_squared_error: 4.0695e-04 - val_loss: 6.9998e-04 - val_mean_squared_error: 6.9998e-04\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.00071 to 0.00070, saving model to weights.best_mlp.hdf5\n",
            "Epoch 67/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0103e-04 - mean_squared_error: 4.0103e-04 - val_loss: 6.8736e-04 - val_mean_squared_error: 6.8736e-04\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.00070 to 0.00069, saving model to weights.best_mlp.hdf5\n",
            "Epoch 68/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0334e-04 - mean_squared_error: 4.0334e-04 - val_loss: 6.7363e-04 - val_mean_squared_error: 6.7363e-04\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.00069 to 0.00067, saving model to weights.best_mlp.hdf5\n",
            "Epoch 69/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9891e-04 - mean_squared_error: 3.9891e-04 - val_loss: 6.6420e-04 - val_mean_squared_error: 6.6420e-04\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.00067 to 0.00066, saving model to weights.best_mlp.hdf5\n",
            "Epoch 70/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9097e-04 - mean_squared_error: 3.9097e-04 - val_loss: 6.5856e-04 - val_mean_squared_error: 6.5856e-04\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.00066 to 0.00066, saving model to weights.best_mlp.hdf5\n",
            "Epoch 71/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9135e-04 - mean_squared_error: 3.9135e-04 - val_loss: 6.5244e-04 - val_mean_squared_error: 6.5244e-04\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.00066 to 0.00065, saving model to weights.best_mlp.hdf5\n",
            "Epoch 72/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8154e-04 - mean_squared_error: 3.8154e-04 - val_loss: 6.5216e-04 - val_mean_squared_error: 6.5216e-04\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.00065 to 0.00065, saving model to weights.best_mlp.hdf5\n",
            "Epoch 73/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9280e-04 - mean_squared_error: 3.9280e-04 - val_loss: 6.4649e-04 - val_mean_squared_error: 6.4649e-04\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.00065 to 0.00065, saving model to weights.best_mlp.hdf5\n",
            "Epoch 74/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8280e-04 - mean_squared_error: 3.8280e-04 - val_loss: 6.3452e-04 - val_mean_squared_error: 6.3452e-04\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.00065 to 0.00063, saving model to weights.best_mlp.hdf5\n",
            "Epoch 75/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8372e-04 - mean_squared_error: 3.8372e-04 - val_loss: 6.4822e-04 - val_mean_squared_error: 6.4822e-04\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.00063\n",
            "Epoch 00075: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 50\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 3\n",
            "MLP: Train: 0.009359, Validation: 0.000648\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 1s 3ms/step - loss: 8.3467 - mean_squared_error: 8.3467 - val_loss: 3.5440 - val_mean_squared_error: 3.5440\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.54404, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.7561 - mean_squared_error: 0.7561 - val_loss: 0.0170 - val_mean_squared_error: 0.0170\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.54404 to 0.01697, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0049 - mean_squared_error: 0.0049 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.01697 to 0.00317, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00317 to 0.00219, saving model to weights.best_mlp.hdf5\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00219 to 0.00169, saving model to weights.best_mlp.hdf5\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00169 to 0.00148, saving model to weights.best_mlp.hdf5\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00148 to 0.00141, saving model to weights.best_mlp.hdf5\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00141\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00141\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00141\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00141\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00141\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00141\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00141\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00141\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00141\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00141\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00141\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.9990e-04 - mean_squared_error: 9.9990e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00141\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.8309e-04 - mean_squared_error: 9.8309e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00141\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.6845e-04 - mean_squared_error: 9.6845e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00141\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.5206e-04 - mean_squared_error: 9.5206e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00141\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.3398e-04 - mean_squared_error: 9.3398e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00141\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.1188e-04 - mean_squared_error: 9.1188e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00141\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.8557e-04 - mean_squared_error: 8.8557e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00141\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.5648e-04 - mean_squared_error: 8.5648e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00141\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.2658e-04 - mean_squared_error: 8.2658e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00141\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.9992e-04 - mean_squared_error: 7.9992e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00141\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.7671e-04 - mean_squared_error: 7.7671e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00141\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.5962e-04 - mean_squared_error: 7.5962e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00141\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.4471e-04 - mean_squared_error: 7.4471e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00141\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.3633e-04 - mean_squared_error: 7.3633e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00141\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.3034e-04 - mean_squared_error: 7.3034e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00141\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.2405e-04 - mean_squared_error: 7.2405e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00141\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.1919e-04 - mean_squared_error: 7.1919e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00141\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.0906e-04 - mean_squared_error: 7.0906e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00141\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.9729e-04 - mean_squared_error: 6.9729e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00141\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.8429e-04 - mean_squared_error: 6.8429e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00141\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.6692e-04 - mean_squared_error: 6.6692e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00141\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.5764e-04 - mean_squared_error: 6.5764e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00141\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.3810e-04 - mean_squared_error: 6.3810e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00141\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.1911e-04 - mean_squared_error: 6.1911e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00141\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.0513e-04 - mean_squared_error: 6.0513e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00141\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.8844e-04 - mean_squared_error: 5.8844e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00141\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.7001e-04 - mean_squared_error: 5.7001e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00141\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.4524e-04 - mean_squared_error: 5.4524e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00141 to 0.00135, saving model to weights.best_mlp.hdf5\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.2545e-04 - mean_squared_error: 5.2545e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.00135 to 0.00128, saving model to weights.best_mlp.hdf5\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.0782e-04 - mean_squared_error: 5.0782e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.00128 to 0.00120, saving model to weights.best_mlp.hdf5\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.9192e-04 - mean_squared_error: 4.9192e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.00120 to 0.00113, saving model to weights.best_mlp.hdf5\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.8017e-04 - mean_squared_error: 4.8017e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.00113 to 0.00106, saving model to weights.best_mlp.hdf5\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.6144e-04 - mean_squared_error: 4.6144e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.00106 to 0.00101, saving model to weights.best_mlp.hdf5\n",
            "Epoch 52/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5802e-04 - mean_squared_error: 4.5802e-04 - val_loss: 9.6090e-04 - val_mean_squared_error: 9.6090e-04\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.00101 to 0.00096, saving model to weights.best_mlp.hdf5\n",
            "Epoch 53/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4934e-04 - mean_squared_error: 4.4934e-04 - val_loss: 9.0026e-04 - val_mean_squared_error: 9.0026e-04\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.00096 to 0.00090, saving model to weights.best_mlp.hdf5\n",
            "Epoch 54/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3209e-04 - mean_squared_error: 4.3209e-04 - val_loss: 8.5306e-04 - val_mean_squared_error: 8.5306e-04\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.00090 to 0.00085, saving model to weights.best_mlp.hdf5\n",
            "Epoch 55/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2455e-04 - mean_squared_error: 4.2455e-04 - val_loss: 8.2229e-04 - val_mean_squared_error: 8.2229e-04\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.00085 to 0.00082, saving model to weights.best_mlp.hdf5\n",
            "Epoch 56/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2425e-04 - mean_squared_error: 4.2425e-04 - val_loss: 7.8858e-04 - val_mean_squared_error: 7.8858e-04\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.00082 to 0.00079, saving model to weights.best_mlp.hdf5\n",
            "Epoch 57/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2106e-04 - mean_squared_error: 4.2106e-04 - val_loss: 7.5694e-04 - val_mean_squared_error: 7.5694e-04\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.00079 to 0.00076, saving model to weights.best_mlp.hdf5\n",
            "Epoch 58/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2048e-04 - mean_squared_error: 4.2048e-04 - val_loss: 7.3664e-04 - val_mean_squared_error: 7.3664e-04\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.00076 to 0.00074, saving model to weights.best_mlp.hdf5\n",
            "Epoch 59/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0936e-04 - mean_squared_error: 4.0936e-04 - val_loss: 7.1384e-04 - val_mean_squared_error: 7.1384e-04\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.00074 to 0.00071, saving model to weights.best_mlp.hdf5\n",
            "Epoch 60/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0016e-04 - mean_squared_error: 4.0016e-04 - val_loss: 6.8852e-04 - val_mean_squared_error: 6.8852e-04\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.00071 to 0.00069, saving model to weights.best_mlp.hdf5\n",
            "Epoch 61/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9542e-04 - mean_squared_error: 3.9542e-04 - val_loss: 6.7987e-04 - val_mean_squared_error: 6.7987e-04\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.00069 to 0.00068, saving model to weights.best_mlp.hdf5\n",
            "Epoch 62/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1154e-04 - mean_squared_error: 4.1154e-04 - val_loss: 6.6728e-04 - val_mean_squared_error: 6.6728e-04\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.00068 to 0.00067, saving model to weights.best_mlp.hdf5\n",
            "Epoch 63/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9523e-04 - mean_squared_error: 3.9523e-04 - val_loss: 6.4929e-04 - val_mean_squared_error: 6.4929e-04\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.00067 to 0.00065, saving model to weights.best_mlp.hdf5\n",
            "Epoch 64/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9884e-04 - mean_squared_error: 3.9884e-04 - val_loss: 6.5093e-04 - val_mean_squared_error: 6.5093e-04\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.00065\n",
            "Epoch 65/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8175e-04 - mean_squared_error: 3.8175e-04 - val_loss: 6.4164e-04 - val_mean_squared_error: 6.4164e-04\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.00065 to 0.00064, saving model to weights.best_mlp.hdf5\n",
            "Epoch 66/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9503e-04 - mean_squared_error: 3.9503e-04 - val_loss: 6.4003e-04 - val_mean_squared_error: 6.4003e-04\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.00064 to 0.00064, saving model to weights.best_mlp.hdf5\n",
            "Epoch 67/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8285e-04 - mean_squared_error: 3.8285e-04 - val_loss: 6.3511e-04 - val_mean_squared_error: 6.3511e-04\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.00064 to 0.00064, saving model to weights.best_mlp.hdf5\n",
            "Epoch 68/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0511e-04 - mean_squared_error: 4.0511e-04 - val_loss: 6.4422e-04 - val_mean_squared_error: 6.4422e-04\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.00064\n",
            "Epoch 69/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8310e-04 - mean_squared_error: 3.8310e-04 - val_loss: 6.2966e-04 - val_mean_squared_error: 6.2966e-04\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.00064 to 0.00063, saving model to weights.best_mlp.hdf5\n",
            "Epoch 00069: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 50\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 4\n",
            "MLP: Train: 0.009324, Validation: 0.000630\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAFh9JREFUeJzt3X+M3PWd3/Hne2dndm3vGpu1A5wN\ntblGQBJ6JtlQU9A1Ta4nIBeIlEucU1JdT9f4H9pAdNHJ0UlNTsofqVRdL6maS0mOa6USchSS5hqR\nIwlniK4BUpv4isEODgRiQ4zXBsdrvN6fn/4x313vzs7Mjn+M5zP4+ZBWO/Od2e++d/z1az/7/nzm\n+42UEpKk7tHT6QIkSafH4JakLmNwS1KXMbglqcsY3JLUZQxuSeoyBrckdRmDW5K6jMEtSV2mtx07\nXbNmTdqwYUM7di1Jb0o7d+48nFJa28pz2xLcGzZsYMeOHe3YtSS9KUXES60+11aJJHUZg1uSuozB\nLUldpi09bkk6XZOTkxw4cICTJ092upS26u/vZ/369ZTL5TPeh8EtKQsHDhxgcHCQDRs2EBGdLqct\nUkocOXKEAwcOsHHjxjPej60SSVk4efIkQ0NDb9rQBogIhoaGzvqvCoNbUjbezKE961z8jFkF95ce\n2cdjz410ugxJylpWwf2Vx57n7/cZ3JLOv6NHj/LlL3/5tL/u1ltv5ejRo22oqLGsgrtc6mFiaqbT\nZUi6ADUK7qmpqaZf99BDD7Fq1ap2lVVXVqtKyqUeJqa96ryk82/btm08//zzbNq0iXK5TH9/P6tX\nr2bv3r0899xzfPCDH2T//v2cPHmSO++8k61btwKnTvFx/PhxbrnlFm666SZ+9KMfsW7dOr797W+z\nbNmyc15rVsHd19vD5LQjbulC96f/+xmefeXYOd3n235tJZ/9wNsbPv6FL3yB3bt3s2vXLh599FHe\n//73s3v37rlle/fccw8XX3wxY2NjvPvd7+ZDH/oQQ0NDC/axb98+7rvvPr761a/ykY98hAcffJCP\nf/zj5/TngMyCu1wKWyWSsnD99dcvWGv9pS99iW9961sA7N+/n3379i0K7o0bN7Jp0yYA3vWud/Hi\niy+2pbaWgjsiPgX8GyABTwN/kFI6529vKpcccUui6cj4fFmxYsXc7UcffZQf/OAHPP744yxfvpz3\nvOc9dddi9/X1zd0ulUqMjY21pbYlJycjYh3wSWA4pfQOoAR8tB3FVGyVSOqQwcFBRkdH6z72q1/9\nitWrV7N8+XL27t3LE088cZ6rW6jVVkkvsCwiJoHlwCvtKKZc6mHcVomkDhgaGuLGG2/kHe94B8uW\nLeOSSy6Ze+zmm2/mK1/5Ctdccw1XXXUVmzdv7mClLQR3SunliPiPwC+AMeB7KaXv1T4vIrYCWwGu\nuOKKMyrGEbekTvr6179ed3tfXx/f/e536z4228des2YNu3fvntv+6U9/+pzXN6uVVslq4HZgI/Br\nwIqIWDRNmlK6O6U0nFIaXru2pavvLFJxHbckLamVN+D8FvDzlNJISmkS+Cbwz9pRTLkUTLqOW5Ka\naiW4fwFsjojlUT07yvuAPe0oxlaJJC1tyeBOKT0JPAA8RXUpYA9wdzuK8S3vkrS0llaVpJQ+C3y2\nzbVUe9yOuCWpqaxOMmWrRJKWllVw2yqR1C0GBgY69r2zCu7qiNtVJZLUTGYnmXLELakztm3bxuWX\nX84dd9wBwOc+9zl6e3vZvn07r7/+OpOTk3z+85/n9ttv73ClmQV3pRRMTM+QUrogrj0nqYHvboOD\nT5/bfV56LdzyhYYPb9myhbvuumsuuO+//34efvhhPvnJT7Jy5UoOHz7M5s2bue222zqeT3kFd2+1\nczM1kyiXDG5J5891113HoUOHeOWVVxgZGWH16tVceumlfOpTn+KHP/whPT09vPzyy7z66qtceuml\nHa01q+Aul6rBPTE1M3db0gWoyci4nT784Q/zwAMPcPDgQbZs2cK9997LyMgIO3fupFwus2HDhrqn\ncz3fsgru2RG3SwIldcKWLVv4xCc+weHDh3nssce4//77ectb3kK5XGb79u289NJLnS4RyCy450bc\nBrekDnj729/O6Ogo69at47LLLuNjH/sYH/jAB7j22msZHh7m6quv7nSJQGbBXZnXKpGkTnj66VOT\nomvWrOHxxx+v+7zjx4+fr5IWyaqRfKpV4lpuSWokq+AuO+KWpCVlFtzVJYBOTkoXppTe/H9tn4uf\nMavgnm2VODkpXXj6+/s5cuTImzq8U0ocOXKE/v7+s9qPk5OSsrB+/XoOHDjAyMhIp0tpq/7+ftav\nX39W+8gruF3HLV2wyuUyGzdu7HQZXSGrVsns5KTBLUmNZRnctkokqbGsgvvU5OSbd3JCks5WXsHt\niFuSlpRVcJd7XcctSUvJKrgrTk5K0pKyCu5yr60SSVpKVsFd8bSukrSkrIJ7bh33lKtKJKmRrIK7\n1BOUeoKJ6elOlyJJ2coquKHaLvF83JLUWHbBXS6Fk5OS1ER2wV3p7XFyUpKayC+4Sz1MOuKWpIay\nC+6yI25Jaiq74K5OThrcktRIdsFdLvUw4TpuSWoov+C2VSJJTWUX3H1OTkpSU9kFd7k37HFLUhP5\nBXfJVokkNZNdcFdKPb5zUpKaaCm4I2JVRDwQEXsjYk9E3NCugpyclKTmelt83heBv00p/W5EVIDl\n7Sqoz3XcktTUksEdERcBvwn8a4CU0gQw0a6CyqUez8ctSU200irZCIwAfxURP4mIr0XEinYVVO4N\nWyWS1EQrwd0LvBP4i5TSdcAbwLbaJ0XE1ojYERE7RkZGzrigSqnkOm5JaqKV4D4AHEgpPVncf4Bq\nkC+QUro7pTScUhpeu3btGRfkiFuSmlsyuFNKB4H9EXFVsel9wLPtKqhSrONOyT63JNXT6qqSfwfc\nW6woeQH4g3YVVCn1kBJMzyR6S9GubyNJXaul4E4p7QKG21wLUF3HDTAxPUNvKbv3B0lSx2WXjJUi\nrF0SKEn1ZRfc80fckqTFsgvuStHXNrglqb78grt3tlVicEtSPdkFd3m2x+2IW5Lqyja4xx1xS1Jd\n2QX3XKvEEbck1ZVfcBcjbi+mIEn15RfccyNu13FLUj3ZBbeTk5LUXIbBXV3H7eSkJNWXXXD3OTkp\nSU1lF9y2SiSpuWyD21UlklRfdsHtOm5Jai674Padk5LUXHbB3ec6bklqKrvgdnJSkprLLrhLPUFP\nODkpSY1kF9xQnaB0xC1J9WUZ3OVSj1fAkaQGsgzuSqnHVokkNZBncNsqkaSGsgzusiNuSWooy+Cu\njrhdxy1J9WQZ3E5OSlJjWQZ3pRS2SiSpgTyD28lJSWooy+AulwxuSWok2+C2VSJJ9WUZ3JXeHiZc\nVSJJdeUZ3KUeJqamO12GJGUpz+B2HbckNZRlcJdL4eSkJDWQaXA7OSlJjWQZ3NXJSYNbkurJM7hd\nxy1JDWUZ3LZKJKmxLIO70tvDTILpGVeWSFKtloM7IkoR8ZOI+E47C4JTV3p31C1Ji53OiPtOYE+7\nCpmv0lsEt31uSVqkpeCOiPXA+4GvtbecqkopAJyglKQ6Wh1x/znwx0DDJI2IrRGxIyJ2jIyMnFVR\ntkokqbElgzsifgc4lFLa2ex5KaW7U0rDKaXhtWvXnlVRs60SR9yStFgrI+4bgdsi4kXgG8B7I+J/\ntLOo2RG3wS1Jiy0Z3Cmlz6SU1qeUNgAfBf4upfTxdhY1G9zjtkokaZEs13H3zbVKXMctSbV6T+fJ\nKaVHgUfbUsk8Tk5KUmNZjridnJSkxrIM7nKxjts34EjSYpkGt60SSWoky+Dus1UiSQ3lFdz//Tb4\n8Vddxy1JTeQV3L/cBYf3Ue61VSJJjeQV3H0rYXyUymyP23XckrRIZsE9COPHTgW3I25JWiSv4K4M\nVEfcTk5KUkN5BXffIEwcn1vHPemIW5IWyS+4x0cp9QQRvgFHkurJLLirrZKIoFLqMbglqY7Mgnsl\njB8HoFLqYXLKVSWSVCuz4B6EiVGYmaHc28PE9HSnK5Kk7OQX3AATxx1xS1IDeQV3ZaD6eXyUcm+4\nHFCS6sgruGtG3OMGtyQtkllwr6x+Hh+lXOpxHbck1ZFZcM+2So5R6XU5oCTVk1lwF62S4kRT9rgl\nabFMg/t40SpxVYkk1cosuOf1uHudnJSkevIK7nnLAStOTkpSXXkFd28FSn0wMUrFddySVFdewQ1z\nZwj0JFOSVF+GwT3gOm5JaiLD4B6cm5x0xC1Ji2UY3NVTu1ZKPV5zUpLqyDC4B+feOTnpVd4laZH8\ngrsy2+MOWyWSVEd+wV1cMLhSKjE9k5iecdQtSfPlGdzF+bgB13JLUo08g3vqJP1RvWyZ7RJJWijP\n4AZWMAbgWm5JqpFtcC9L1eB2xC1JC2Uc3CcAPLWrJNXIL7iLMwQum6kGtyNuSVoov+AuzsndPxvc\n9rglaYElgzsiLo+I7RHxbEQ8ExF3trWiolXSn94AXA4oSbV6W3jOFPBHKaWnImIQ2BkR308pPduW\niooLBvdNG9ySVM+SI+6U0i9TSk8Vt0eBPcC6tlVUjLgrU7ZKJKme0+pxR8QG4DrgyXYUA8xNTlaK\nEbeTk5K0UMvBHREDwIPAXSmlY3Ue3xoROyJix8jIyFlUVILKAOWpIrgdcUvSAi0Fd0SUqYb2vSml\nb9Z7Tkrp7pTScEppeO3atWdXVWWA8tRxAE/tKkk1WllVEsBfAntSSn/W/pKAvkFKU05OSlI9rYy4\nbwT+FfDeiNhVfNza1qr6BumdrI64bZVI0kJLLgdMKf09EOehllP6BugZL4LbEbckLZDfOycB+lZS\nmpjtcRvckjRfpsE9SNgqkaS68g3u8VHAEbck1cozuIsLBkNyxC1JNfIM7r5BIk0z2DvNhOu4JWmB\nbIMb4OLSSVslklQj6+Be1XPSVokk1cg7uB1xS9IiWQf3yp5x34AjSTWyDu6LSmO2SiSpRp7BXSlG\n3GGrRJJq5RncxYh7IJyclKRaWQf3YIx5Pm5JqpFncJeXQfQwwJiTk5JUI8/gjoC+QVbg5KQk1coz\nuAH6VrKCMScnJalGxsE9yPJ0wuCWpBr5BndlgOXphK0SSaqRb3D3DbIsuapEkmotec3JjukbpH/m\nBOO2SiRpgYxH3AP0z7xhj1uSamQc3Cvpm3ZyUpJqZRzcg/TNnGByaqrTlUhSVrIOboC+mTFmZpyg\nlKRZ+QZ3ZQCAAcZ44fDxDhcjSfnIN7iLEfeKOMnjzx/pcDGSlI+Mg3slABsHpnnihdc6XIwk5SPj\n4K62SoYv6+WJF46Qkn1uSYKsg7vaKrl2bYkjb0yw75B9bkmCLgjuq1cHAE+8YJ9bkiDr4K72uId6\nx1m3apkTlJJUyDe4i+WAjI+y+cohnvz5a67nliRyDu7eCpT6YGKUG359iNfemOC5Q6OdrkqSOi7f\n4IZqn3t8lM1XXgzAE7ZLJCn34B6A8VHWr17O5Rcv43EnKCUp9+CujrgBNm+0zy1JkH1wr4Tx6vrt\nzVcOcfTEJHsP2ueWdGHLPLgHYfwYAJt/fQhwPbcktRTcEXFzRPw0In4WEdvaXdScysBcq2TdqmVc\ncfFy+9ySLnhLBndElID/AtwCvA34vYh4W7sLA6oj7olTb3W/4cohfmyfW9IFrpUR9/XAz1JKL6SU\nJoBvALe3t6xC3yCcPAZHnoex19l85Sp+NTbJnoPHzsu3l6QctXKV93XA/nn3DwD/tD3l1Bh4C0yP\nw39+JwAfjB7+ed9ypv5rmVcjSAQJSAQUt+d/rpVqti1+xsKNs3s6XXX3m5Han6tevYt/9sWvRaP9\nLNy+9Pc6l+pVufDxesdF7f36r8jSz1mqlnpfd4b7iVZeydN/tVv5uep/XXucaT1nYqnv1MrPeKJ0\nEdf8yY/ORTlNtRLcLYmIrcBWgCuuuOLc7PTdn4C118CJw3DiNWLsNQ79/BecGDsBCYIZSNXohlS8\nsvNe3jT/RvVOs7PD1v+P1sKT6j4lau7npbXgWSqsUsMgSgt/A5729z4Ti37ZtPCPt/iXz9L/Wq39\nQq/zvdKZ/BJbuub6X3b6R92Z/6u05wg/04FTJ02VB8/L92kluF8GLp93f32xbYGU0t3A3QDDw8Pn\n5hUv98Nbf2vBpqvPyY4lqXu10uP+v8BbI2JjRFSAjwJ/096yJEmNLDniTilNRcS/BR4GSsA9KaVn\n2l6ZJKmulnrcKaWHgIfaXIskqQV5v3NSkrSIwS1JXcbglqQuY3BLUpcxuCWpy0Q6g3dYLbnTiBHg\npTP88jXA4XNYzvnQbTV3W71gzedLt9XcbfVC45r/UUppbSs7aEtwn42I2JFSGu50Haej22rutnrB\nms+Xbqu52+qFc1OzrRJJ6jIGtyR1mRyD++5OF3AGuq3mbqsXrPl86baau61eOAc1Z9fjliQ1l+OI\nW5LURDbB3bELEp+GiLgnIg5FxO552y6OiO9HxL7i8+pO1lgrIi6PiO0R8WxEPBMRdxbbs607Ivoj\n4scR8Q9FzX9abN8YEU8Wx8hfF6cZzkZElCLiJxHxneJ+7vW+GBFPR8SuiNhRbMv2uACIiFUR8UBE\n7I2IPRFxQ841R8RVxes7+3EsIu4625qzCO6OXpD49Pw34OaabduAR1JKbwUeKe7nZAr4o5TS24DN\nwB3Fa5tz3ePAe1NKvwFsAm6OiM3AfwD+U0rpHwOvA3/YwRrruRPYM+9+7vUC/IuU0qZ5y9NyPi4A\nvgj8bUrpauA3qL7e2dacUvpp8fpuAt4FnAC+xdnWnFLq+AdwA/DwvPufAT7T6boa1LoB2D3v/k+B\ny4rblwE/7XSNS9T/beBfdkvdwHLgKarXOT0M9NY7Zjr9QfXKUI8A7wW+Q/VKYNnWW9T0IrCmZlu2\nxwVwEfBzirm5bqi5ps7fBv7Puag5ixE39S9IvK5DtZyuS1JKvyxuHwQu6WQxzUTEBuA64Ekyr7to\nO+wCDgHfB54HjqaUpoqn5HaM/Dnwx8BMcX+IvOuF6sUivxcRO4trxkLex8VGYAT4q6Il9bWIWEHe\nNc/3UeC+4vZZ1ZxLcL8ppOqvzyyX6UTEAPAgcFdK6dj8x3KsO6U0nap/Xq4Hrifjy41GxO8Ah1JK\nOztdy2m6KaX0Tqotyjsi4jfnP5jhcdELvBP4i5TSdcAb1LQYMqwZgGJ+4zbgf9Y+diY15xLcLV2Q\nOFOvRsRlAMXnQx2uZ5GIKFMN7XtTSt8sNmdfN0BK6SiwnWqrYVVEzF61Kadj5Ebgtoh4EfgG1XbJ\nF8m3XgBSSi8Xnw9R7bteT97HxQHgQErpyeL+A1SDPOeaZ90CPJVSerW4f1Y15xLc3XxB4r8Bfr+4\n/ftUe8jZiIgA/hLYk1L6s3kPZVt3RKyNiFXF7WVUe/J7qAb47xZPy6bmlNJnUkrrU0obqB67f5dS\n+hiZ1gsQESsiYnD2NtX+624yPi5SSgeB/RFxVbHpfcCzZFzzPL/HqTYJnG3NnW7Yz2vc3wo8R7WX\n+SedrqdBjfcBvwQmqf72/0OqvcxHgH3AD4CLO11nTc03Uf0z7P8Bu4qPW3OuG/gnwE+KmncD/77Y\nfiXwY+BnVP/k7Ot0rXVqfw/wndzrLWr7h+Ljmdn/czkfF0V9m4AdxbHxv4DVXVDzCuAIcNG8bWdV\ns++clKQuk0urRJLUIoNbkrqMwS1JXcbglqQuY3BLUpcxuCWpyxjcktRlDG5J6jL/H94xibUVg9AX\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 7.7934 - mean_squared_error: 7.7934 - val_loss: 2.5252 - val_mean_squared_error: 2.5252\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.52523, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.4179 - mean_squared_error: 0.4179 - val_loss: 0.0065 - val_mean_squared_error: 0.0065\n",
            "\n",
            "Epoch 00002: val_loss improved from 2.52523 to 0.00651, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0055 - mean_squared_error: 0.0055 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00651 to 0.00233, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0040 - mean_squared_error: 0.0040 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00233 to 0.00174, saving model to weights.best_mlp.hdf5\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00174 to 0.00151, saving model to weights.best_mlp.hdf5\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00151 to 0.00145, saving model to weights.best_mlp.hdf5\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00145\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00145\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00145\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00145\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00145\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00145\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00145\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00145\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00145\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00145\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00145\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00145\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.8975e-04 - mean_squared_error: 9.8975e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00145\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.7603e-04 - mean_squared_error: 9.7603e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00145\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.5986e-04 - mean_squared_error: 9.5986e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00145\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.3963e-04 - mean_squared_error: 9.3963e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00145\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.1457e-04 - mean_squared_error: 9.1457e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00145\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.8561e-04 - mean_squared_error: 8.8561e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00145\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.5526e-04 - mean_squared_error: 8.5526e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00145\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.2566e-04 - mean_squared_error: 8.2566e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00145\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.9993e-04 - mean_squared_error: 7.9993e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00145\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.7933e-04 - mean_squared_error: 7.7933e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00145\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.6462e-04 - mean_squared_error: 7.6462e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00145\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.5242e-04 - mean_squared_error: 7.5242e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00145\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.4689e-04 - mean_squared_error: 7.4689e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00145\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.4041e-04 - mean_squared_error: 7.4041e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00145\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.3122e-04 - mean_squared_error: 7.3122e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00145\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.1835e-04 - mean_squared_error: 7.1835e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00145\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.0639e-04 - mean_squared_error: 7.0639e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00145\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.9290e-04 - mean_squared_error: 6.9290e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00145\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.7320e-04 - mean_squared_error: 6.7320e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00145\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.5096e-04 - mean_squared_error: 6.5096e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00145\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.2866e-04 - mean_squared_error: 6.2866e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00145\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.0500e-04 - mean_squared_error: 6.0500e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.00145 to 0.00143, saving model to weights.best_mlp.hdf5\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.8923e-04 - mean_squared_error: 5.8923e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.00143 to 0.00131, saving model to weights.best_mlp.hdf5\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.6557e-04 - mean_squared_error: 5.6557e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.00131 to 0.00120, saving model to weights.best_mlp.hdf5\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.3711e-04 - mean_squared_error: 5.3711e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.00120 to 0.00112, saving model to weights.best_mlp.hdf5\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.1539e-04 - mean_squared_error: 5.1539e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.00112 to 0.00103, saving model to weights.best_mlp.hdf5\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.9929e-04 - mean_squared_error: 4.9929e-04 - val_loss: 9.4730e-04 - val_mean_squared_error: 9.4730e-04\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.00103 to 0.00095, saving model to weights.best_mlp.hdf5\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.7463e-04 - mean_squared_error: 4.7463e-04 - val_loss: 8.7974e-04 - val_mean_squared_error: 8.7974e-04\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00095 to 0.00088, saving model to weights.best_mlp.hdf5\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.6502e-04 - mean_squared_error: 4.6502e-04 - val_loss: 8.1480e-04 - val_mean_squared_error: 8.1480e-04\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.00088 to 0.00081, saving model to weights.best_mlp.hdf5\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5146e-04 - mean_squared_error: 4.5146e-04 - val_loss: 7.5075e-04 - val_mean_squared_error: 7.5075e-04\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.00081 to 0.00075, saving model to weights.best_mlp.hdf5\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4376e-04 - mean_squared_error: 4.4376e-04 - val_loss: 7.0792e-04 - val_mean_squared_error: 7.0792e-04\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.00075 to 0.00071, saving model to weights.best_mlp.hdf5\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3165e-04 - mean_squared_error: 4.3165e-04 - val_loss: 6.5918e-04 - val_mean_squared_error: 6.5918e-04\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.00071 to 0.00066, saving model to weights.best_mlp.hdf5\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2509e-04 - mean_squared_error: 4.2509e-04 - val_loss: 6.2131e-04 - val_mean_squared_error: 6.2131e-04\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.00066 to 0.00062, saving model to weights.best_mlp.hdf5\n",
            "Epoch 52/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1691e-04 - mean_squared_error: 4.1691e-04 - val_loss: 5.8830e-04 - val_mean_squared_error: 5.8830e-04\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.00062 to 0.00059, saving model to weights.best_mlp.hdf5\n",
            "Epoch 53/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1404e-04 - mean_squared_error: 4.1404e-04 - val_loss: 5.6311e-04 - val_mean_squared_error: 5.6311e-04\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.00059 to 0.00056, saving model to weights.best_mlp.hdf5\n",
            "Epoch 54/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1170e-04 - mean_squared_error: 4.1170e-04 - val_loss: 5.4147e-04 - val_mean_squared_error: 5.4147e-04\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.00056 to 0.00054, saving model to weights.best_mlp.hdf5\n",
            "Epoch 55/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0747e-04 - mean_squared_error: 4.0747e-04 - val_loss: 5.2438e-04 - val_mean_squared_error: 5.2438e-04\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.00054 to 0.00052, saving model to weights.best_mlp.hdf5\n",
            "Epoch 56/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0674e-04 - mean_squared_error: 4.0674e-04 - val_loss: 5.1069e-04 - val_mean_squared_error: 5.1069e-04\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.00052 to 0.00051, saving model to weights.best_mlp.hdf5\n",
            "Epoch 57/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0176e-04 - mean_squared_error: 4.0176e-04 - val_loss: 4.9278e-04 - val_mean_squared_error: 4.9278e-04\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.00051 to 0.00049, saving model to weights.best_mlp.hdf5\n",
            "Epoch 58/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0285e-04 - mean_squared_error: 4.0285e-04 - val_loss: 4.7637e-04 - val_mean_squared_error: 4.7637e-04\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.00049 to 0.00048, saving model to weights.best_mlp.hdf5\n",
            "Epoch 59/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9802e-04 - mean_squared_error: 3.9802e-04 - val_loss: 4.7368e-04 - val_mean_squared_error: 4.7368e-04\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.00048 to 0.00047, saving model to weights.best_mlp.hdf5\n",
            "Epoch 60/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9939e-04 - mean_squared_error: 3.9939e-04 - val_loss: 4.6822e-04 - val_mean_squared_error: 4.6822e-04\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.00047 to 0.00047, saving model to weights.best_mlp.hdf5\n",
            "Epoch 61/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0174e-04 - mean_squared_error: 4.0174e-04 - val_loss: 4.6325e-04 - val_mean_squared_error: 4.6325e-04\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.00047 to 0.00046, saving model to weights.best_mlp.hdf5\n",
            "Epoch 00061: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 50\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 2\n",
            "MLP: Train: 0.010056, Validation: 0.000463\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntyR1mP8PJuAfT9DPGigAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 8.9393 - mean_squared_error: 8.9393 - val_loss: 4.2928 - val_mean_squared_error: 4.2928\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 4.29279, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 1.0130 - mean_squared_error: 1.0130 - val_loss: 0.0407 - val_mean_squared_error: 0.0407\n",
            "\n",
            "Epoch 00002: val_loss improved from 4.29279 to 0.04069, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0058 - mean_squared_error: 0.0058 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.04069 to 0.00247, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00247 to 0.00194, saving model to weights.best_mlp.hdf5\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00194 to 0.00170, saving model to weights.best_mlp.hdf5\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00170 to 0.00160, saving model to weights.best_mlp.hdf5\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00160 to 0.00157, saving model to weights.best_mlp.hdf5\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00157\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00157\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00157\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00157\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00157\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00157\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00157\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00157\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00157\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00157\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00157\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00157\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00157\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.9423e-04 - mean_squared_error: 9.9423e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00157\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.7765e-04 - mean_squared_error: 9.7765e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00157\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.5785e-04 - mean_squared_error: 9.5785e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00157\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.3548e-04 - mean_squared_error: 9.3548e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00157\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.1110e-04 - mean_squared_error: 9.1110e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00157\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.8630e-04 - mean_squared_error: 8.8630e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00157\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.6341e-04 - mean_squared_error: 8.6341e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00157\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.4240e-04 - mean_squared_error: 8.4240e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00157\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.2511e-04 - mean_squared_error: 8.2511e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00157\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.1199e-04 - mean_squared_error: 8.1199e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00157\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.0142e-04 - mean_squared_error: 8.0142e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00157\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.9248e-04 - mean_squared_error: 7.9248e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00157\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.8455e-04 - mean_squared_error: 7.8455e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00157\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.7646e-04 - mean_squared_error: 7.7646e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00157\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.6663e-04 - mean_squared_error: 7.6663e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00157\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.5441e-04 - mean_squared_error: 7.5441e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00157\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.4286e-04 - mean_squared_error: 7.4286e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00157\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.2338e-04 - mean_squared_error: 7.2338e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00157\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.0735e-04 - mean_squared_error: 7.0735e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00157\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.9331e-04 - mean_squared_error: 6.9331e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00157\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.7658e-04 - mean_squared_error: 6.7658e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00157\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.6217e-04 - mean_squared_error: 6.6217e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00157\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.4222e-04 - mean_squared_error: 6.4222e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00157\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.2495e-04 - mean_squared_error: 6.2495e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.00157 to 0.00154, saving model to weights.best_mlp.hdf5\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.0150e-04 - mean_squared_error: 6.0150e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.00154 to 0.00145, saving model to weights.best_mlp.hdf5\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.8607e-04 - mean_squared_error: 5.8607e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00145 to 0.00137, saving model to weights.best_mlp.hdf5\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.6864e-04 - mean_squared_error: 5.6864e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.00137 to 0.00129, saving model to weights.best_mlp.hdf5\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.4023e-04 - mean_squared_error: 5.4023e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.00129 to 0.00123, saving model to weights.best_mlp.hdf5\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.2967e-04 - mean_squared_error: 5.2967e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.00123 to 0.00116, saving model to weights.best_mlp.hdf5\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.0752e-04 - mean_squared_error: 5.0752e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.00116 to 0.00111, saving model to weights.best_mlp.hdf5\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.9258e-04 - mean_squared_error: 4.9258e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.00111 to 0.00105, saving model to weights.best_mlp.hdf5\n",
            "Epoch 52/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.7139e-04 - mean_squared_error: 4.7139e-04 - val_loss: 9.9306e-04 - val_mean_squared_error: 9.9306e-04\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.00105 to 0.00099, saving model to weights.best_mlp.hdf5\n",
            "Epoch 53/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.6741e-04 - mean_squared_error: 4.6741e-04 - val_loss: 9.6626e-04 - val_mean_squared_error: 9.6626e-04\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.00099 to 0.00097, saving model to weights.best_mlp.hdf5\n",
            "Epoch 54/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.6189e-04 - mean_squared_error: 4.6189e-04 - val_loss: 9.2183e-04 - val_mean_squared_error: 9.2183e-04\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.00097 to 0.00092, saving model to weights.best_mlp.hdf5\n",
            "Epoch 55/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4622e-04 - mean_squared_error: 4.4622e-04 - val_loss: 8.7847e-04 - val_mean_squared_error: 8.7847e-04\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.00092 to 0.00088, saving model to weights.best_mlp.hdf5\n",
            "Epoch 56/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4293e-04 - mean_squared_error: 4.4293e-04 - val_loss: 8.5890e-04 - val_mean_squared_error: 8.5890e-04\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.00088 to 0.00086, saving model to weights.best_mlp.hdf5\n",
            "Epoch 57/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3121e-04 - mean_squared_error: 4.3121e-04 - val_loss: 8.1496e-04 - val_mean_squared_error: 8.1496e-04\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.00086 to 0.00081, saving model to weights.best_mlp.hdf5\n",
            "Epoch 58/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3389e-04 - mean_squared_error: 4.3389e-04 - val_loss: 8.1337e-04 - val_mean_squared_error: 8.1337e-04\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.00081 to 0.00081, saving model to weights.best_mlp.hdf5\n",
            "Epoch 59/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2512e-04 - mean_squared_error: 4.2512e-04 - val_loss: 7.6949e-04 - val_mean_squared_error: 7.6949e-04\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.00081 to 0.00077, saving model to weights.best_mlp.hdf5\n",
            "Epoch 60/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2443e-04 - mean_squared_error: 4.2443e-04 - val_loss: 7.6294e-04 - val_mean_squared_error: 7.6294e-04\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.00077 to 0.00076, saving model to weights.best_mlp.hdf5\n",
            "Epoch 61/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2243e-04 - mean_squared_error: 4.2243e-04 - val_loss: 7.4119e-04 - val_mean_squared_error: 7.4119e-04\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.00076 to 0.00074, saving model to weights.best_mlp.hdf5\n",
            "Epoch 62/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1963e-04 - mean_squared_error: 4.1963e-04 - val_loss: 7.2456e-04 - val_mean_squared_error: 7.2456e-04\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.00074 to 0.00072, saving model to weights.best_mlp.hdf5\n",
            "Epoch 63/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1788e-04 - mean_squared_error: 4.1788e-04 - val_loss: 7.1487e-04 - val_mean_squared_error: 7.1487e-04\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.00072 to 0.00071, saving model to weights.best_mlp.hdf5\n",
            "Epoch 64/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1484e-04 - mean_squared_error: 4.1484e-04 - val_loss: 6.9960e-04 - val_mean_squared_error: 6.9960e-04\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.00071 to 0.00070, saving model to weights.best_mlp.hdf5\n",
            "Epoch 65/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2244e-04 - mean_squared_error: 4.2244e-04 - val_loss: 7.0341e-04 - val_mean_squared_error: 7.0341e-04\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.00070\n",
            "Epoch 66/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1377e-04 - mean_squared_error: 4.1377e-04 - val_loss: 6.8630e-04 - val_mean_squared_error: 6.8630e-04\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.00070 to 0.00069, saving model to weights.best_mlp.hdf5\n",
            "Epoch 67/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1750e-04 - mean_squared_error: 4.1750e-04 - val_loss: 6.8610e-04 - val_mean_squared_error: 6.8610e-04\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.00069 to 0.00069, saving model to weights.best_mlp.hdf5\n",
            "Epoch 68/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2015e-04 - mean_squared_error: 4.2015e-04 - val_loss: 6.7524e-04 - val_mean_squared_error: 6.7524e-04\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.00069 to 0.00068, saving model to weights.best_mlp.hdf5\n",
            "Epoch 69/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2884e-04 - mean_squared_error: 4.2884e-04 - val_loss: 6.8112e-04 - val_mean_squared_error: 6.8112e-04\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.00068\n",
            "Epoch 00069: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 50\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 3\n",
            "MLP: Train: 0.009366, Validation: 0.000681\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nY3BLUpcxuCWpyxjcktRlDG5J6jL/Ds5EmYeOtJceAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 8.5622 - mean_squared_error: 8.5622 - val_loss: 3.8003 - val_mean_squared_error: 3.8003\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.80033, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.7978 - mean_squared_error: 0.7978 - val_loss: 0.0140 - val_mean_squared_error: 0.0140\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.80033 to 0.01398, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0040 - mean_squared_error: 0.0040 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.01398 to 0.00244, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00244 to 0.00179, saving model to weights.best_mlp.hdf5\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00179 to 0.00145, saving model to weights.best_mlp.hdf5\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00145 to 0.00131, saving model to weights.best_mlp.hdf5\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00131 to 0.00128, saving model to weights.best_mlp.hdf5\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00128\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00128\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00128\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00128\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00128\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00128\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00128\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00128\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00128\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00128\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00128\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.9191e-04 - mean_squared_error: 9.9191e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00128\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.7591e-04 - mean_squared_error: 9.7591e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00128\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.6215e-04 - mean_squared_error: 9.6215e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00128\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.4784e-04 - mean_squared_error: 9.4784e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00128\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.3037e-04 - mean_squared_error: 9.3037e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00128\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.0867e-04 - mean_squared_error: 9.0867e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00128\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.8276e-04 - mean_squared_error: 8.8276e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00128\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.5423e-04 - mean_squared_error: 8.5423e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00128\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.2511e-04 - mean_squared_error: 8.2511e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00128\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.9864e-04 - mean_squared_error: 7.9864e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00128\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.7592e-04 - mean_squared_error: 7.7592e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00128\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.6009e-04 - mean_squared_error: 7.6009e-04 - val_loss: 0.0034 - val_mean_squared_error: 0.0034\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00128\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.4626e-04 - mean_squared_error: 7.4626e-04 - val_loss: 0.0034 - val_mean_squared_error: 0.0034\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00128\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.3798e-04 - mean_squared_error: 7.3798e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00128\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.3016e-04 - mean_squared_error: 7.3016e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00128\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.2369e-04 - mean_squared_error: 7.2369e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00128\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.1719e-04 - mean_squared_error: 7.1719e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00128\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.0613e-04 - mean_squared_error: 7.0613e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00128\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.9414e-04 - mean_squared_error: 6.9414e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00128\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.8178e-04 - mean_squared_error: 6.8178e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00128\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.6662e-04 - mean_squared_error: 6.6662e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00128\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.5250e-04 - mean_squared_error: 6.5250e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00128\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.3872e-04 - mean_squared_error: 6.3872e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00128\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.2878e-04 - mean_squared_error: 6.2878e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00128\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.1262e-04 - mean_squared_error: 6.1262e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00128\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.9007e-04 - mean_squared_error: 5.9007e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00128\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.7196e-04 - mean_squared_error: 5.7196e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00128\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.4848e-04 - mean_squared_error: 5.4848e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.00128\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.3486e-04 - mean_squared_error: 5.3486e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00128\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.1479e-04 - mean_squared_error: 5.1479e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.00128 to 0.00127, saving model to weights.best_mlp.hdf5\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.9264e-04 - mean_squared_error: 4.9264e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.00127 to 0.00120, saving model to weights.best_mlp.hdf5\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.8040e-04 - mean_squared_error: 4.8040e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.00120 to 0.00112, saving model to weights.best_mlp.hdf5\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.6415e-04 - mean_squared_error: 4.6415e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.00112 to 0.00108, saving model to weights.best_mlp.hdf5\n",
            "Epoch 52/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5171e-04 - mean_squared_error: 4.5171e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.00108 to 0.00101, saving model to weights.best_mlp.hdf5\n",
            "Epoch 53/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4762e-04 - mean_squared_error: 4.4762e-04 - val_loss: 9.6744e-04 - val_mean_squared_error: 9.6744e-04\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.00101 to 0.00097, saving model to weights.best_mlp.hdf5\n",
            "Epoch 54/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3188e-04 - mean_squared_error: 4.3188e-04 - val_loss: 9.2403e-04 - val_mean_squared_error: 9.2403e-04\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.00097 to 0.00092, saving model to weights.best_mlp.hdf5\n",
            "Epoch 55/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2933e-04 - mean_squared_error: 4.2933e-04 - val_loss: 8.6953e-04 - val_mean_squared_error: 8.6953e-04\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.00092 to 0.00087, saving model to weights.best_mlp.hdf5\n",
            "Epoch 56/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1585e-04 - mean_squared_error: 4.1585e-04 - val_loss: 8.3609e-04 - val_mean_squared_error: 8.3609e-04\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.00087 to 0.00084, saving model to weights.best_mlp.hdf5\n",
            "Epoch 57/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1736e-04 - mean_squared_error: 4.1736e-04 - val_loss: 8.0673e-04 - val_mean_squared_error: 8.0673e-04\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.00084 to 0.00081, saving model to weights.best_mlp.hdf5\n",
            "Epoch 58/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1117e-04 - mean_squared_error: 4.1117e-04 - val_loss: 7.8685e-04 - val_mean_squared_error: 7.8685e-04\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.00081 to 0.00079, saving model to weights.best_mlp.hdf5\n",
            "Epoch 59/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2061e-04 - mean_squared_error: 4.2061e-04 - val_loss: 7.5548e-04 - val_mean_squared_error: 7.5548e-04\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.00079 to 0.00076, saving model to weights.best_mlp.hdf5\n",
            "Epoch 60/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0222e-04 - mean_squared_error: 4.0222e-04 - val_loss: 7.4973e-04 - val_mean_squared_error: 7.4973e-04\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.00076 to 0.00075, saving model to weights.best_mlp.hdf5\n",
            "Epoch 61/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1862e-04 - mean_squared_error: 4.1862e-04 - val_loss: 7.2061e-04 - val_mean_squared_error: 7.2061e-04\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.00075 to 0.00072, saving model to weights.best_mlp.hdf5\n",
            "Epoch 62/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9518e-04 - mean_squared_error: 3.9518e-04 - val_loss: 7.1212e-04 - val_mean_squared_error: 7.1212e-04\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.00072 to 0.00071, saving model to weights.best_mlp.hdf5\n",
            "Epoch 63/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9691e-04 - mean_squared_error: 3.9691e-04 - val_loss: 6.9714e-04 - val_mean_squared_error: 6.9714e-04\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.00071 to 0.00070, saving model to weights.best_mlp.hdf5\n",
            "Epoch 64/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9115e-04 - mean_squared_error: 3.9115e-04 - val_loss: 6.7956e-04 - val_mean_squared_error: 6.7956e-04\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.00070 to 0.00068, saving model to weights.best_mlp.hdf5\n",
            "Epoch 65/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9920e-04 - mean_squared_error: 3.9920e-04 - val_loss: 6.7561e-04 - val_mean_squared_error: 6.7561e-04\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.00068 to 0.00068, saving model to weights.best_mlp.hdf5\n",
            "Epoch 66/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9229e-04 - mean_squared_error: 3.9229e-04 - val_loss: 6.7052e-04 - val_mean_squared_error: 6.7052e-04\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.00068 to 0.00067, saving model to weights.best_mlp.hdf5\n",
            "Epoch 67/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9216e-04 - mean_squared_error: 3.9216e-04 - val_loss: 6.6041e-04 - val_mean_squared_error: 6.6041e-04\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.00067 to 0.00066, saving model to weights.best_mlp.hdf5\n",
            "Epoch 68/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8803e-04 - mean_squared_error: 3.8803e-04 - val_loss: 6.6744e-04 - val_mean_squared_error: 6.6744e-04\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.00066\n",
            "Epoch 69/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9591e-04 - mean_squared_error: 3.9591e-04 - val_loss: 6.5306e-04 - val_mean_squared_error: 6.5306e-04\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.00066 to 0.00065, saving model to weights.best_mlp.hdf5\n",
            "Epoch 70/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8957e-04 - mean_squared_error: 3.8957e-04 - val_loss: 6.5315e-04 - val_mean_squared_error: 6.5315e-04\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.00065\n",
            "Epoch 71/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0164e-04 - mean_squared_error: 4.0164e-04 - val_loss: 6.5028e-04 - val_mean_squared_error: 6.5028e-04\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.00065 to 0.00065, saving model to weights.best_mlp.hdf5\n",
            "Epoch 72/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8309e-04 - mean_squared_error: 3.8309e-04 - val_loss: 6.4891e-04 - val_mean_squared_error: 6.4891e-04\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.00065 to 0.00065, saving model to weights.best_mlp.hdf5\n",
            "Epoch 73/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0356e-04 - mean_squared_error: 4.0356e-04 - val_loss: 6.6150e-04 - val_mean_squared_error: 6.6150e-04\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.00065\n",
            "Epoch 74/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9157e-04 - mean_squared_error: 3.9157e-04 - val_loss: 6.5090e-04 - val_mean_squared_error: 6.5090e-04\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.00065\n",
            "Epoch 75/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9889e-04 - mean_squared_error: 3.9889e-04 - val_loss: 6.4361e-04 - val_mean_squared_error: 6.4361e-04\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.00065 to 0.00064, saving model to weights.best_mlp.hdf5\n",
            "Epoch 76/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8158e-04 - mean_squared_error: 3.8158e-04 - val_loss: 6.6142e-04 - val_mean_squared_error: 6.6142e-04\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.00064\n",
            "Epoch 77/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9605e-04 - mean_squared_error: 3.9605e-04 - val_loss: 6.4679e-04 - val_mean_squared_error: 6.4679e-04\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.00064\n",
            "Epoch 78/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8934e-04 - mean_squared_error: 3.8934e-04 - val_loss: 6.5670e-04 - val_mean_squared_error: 6.5670e-04\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.00064\n",
            "Epoch 79/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9110e-04 - mean_squared_error: 3.9110e-04 - val_loss: 6.5362e-04 - val_mean_squared_error: 6.5362e-04\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.00064\n",
            "Epoch 80/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8987e-04 - mean_squared_error: 3.8987e-04 - val_loss: 6.6643e-04 - val_mean_squared_error: 6.6643e-04\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.00064\n",
            "Epoch 00080: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 50\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 4\n",
            "MLP: Train: 0.009372, Validation: 0.000666\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAFohJREFUeJzt3WuQ3NWZ3/Hv0zeNeiSQkMTFGhwp\niQM2OAhbJnKgtrx4kwBeg6u8tnDZW5vUxnpDwqXWtSWXq2K74hdO1dZm7ap4XdjLJpVgHAIm3hC8\nrO0VuDbGbCQQRiAZmTVYAwaNFAvQZTTTmpMX/Z9h1NPd07q0+gz6fqpGmr5MzzPTPb858/zPOf9I\nKSFJWjhKgy5AknRiDG5JWmAMbklaYAxuSVpgDG5JWmAMbklaYAxuSVpgDG5JWmAMbklaYCr9eNCV\nK1emNWvW9OOhJektadu2bftSSqt6uW9fgnvNmjVs3bq1Hw8tSW9JEfFir/e1VSJJC4zBLUkLjMEt\nSQtMX3rcknSiJicnGR0dZXx8fNCl9NXQ0BAjIyNUq9WTfgyDW1IWRkdHWbp0KWvWrCEiBl1OX6SU\n2L9/P6Ojo6xdu/akH8dWiaQsjI+Ps2LFirdsaANEBCtWrDjlvyoMbknZeCuH9rTT8TVmFdxf/eFu\nHn1ubNBlSFLWsgrurz/6PH+z2+CWdOYdOHCAr33tayf8cTfccAMHDhzoQ0WdZRXclVIwecyTF0s6\n8zoFd6PR6PpxDz30EMuWLetXWW1lNaukVikxeWxq0GVIOgtt3ryZ559/nnXr1lGtVhkaGmL58uXs\n2rWL5557jo985CPs2bOH8fFxbrvtNjZt2gS8ucXHwYMHuf7667nmmmv48Y9/zOrVq/nud7/L4sWL\nT3utWQV3pWRwS4Iv/q9nePbl10/rY77rbefw+Q9f1vH2L3/5y+zYsYPt27fzyCOP8KEPfYgdO3bM\nTNu76667OO+88zhy5Ajve9/7+OhHP8qKFSuOe4zdu3dzzz338I1vfIOPf/zj3H///XzqU586rV8H\nZBbc1UrQsFUiKQNXXXXVcXOtv/rVr/LAAw8AsGfPHnbv3j0nuNeuXcu6desAeO9738sLL7zQl9ry\nCu5SiQlH3NJZr9vI+EwZHh6eef+RRx7hBz/4AY899hj1ep0PfOADbediL1q0aOb9crnMkSNH+lJb\nVgcnq+WSI25JA7F06VLeeOONtre99tprLF++nHq9zq5du/jJT35yhqs7XlYj7ko57HFLGogVK1Zw\n9dVXc/nll7N48WIuuOCCmduuu+46vv71r/POd76TSy65hA0bNgyw0syCu1ouMTnliFvSYHzrW99q\ne/2iRYv43ve+1/a26T72ypUr2bFjx8z1n/nMZ057fdMya5UEkw1H3JLUTU/BHRF3RMQzEbEjIu6J\niKF+FFMtl2hMGdyS1M28wR0Rq4FbgfUppcuBMnBzP4qplEtMeHBSkrrqtVVSARZHRAWoAy/3o5ha\nOWh4cFKSupo3uFNKLwF/BPwS+BXwWkrpr1rvFxGbImJrRGwdGzu5jaJcOSlJ8+ulVbIcuAlYC7wN\nGI6IOWs4U0p3ppTWp5TWr1q16qSKqVacxy1J8+mlVfJbwC9SSmMppUngO8A/7Ucx1VK4clLSgrBk\nyZKBfe5egvuXwIaIqEfz1A0fBHb2oxhXTkrS/OZdgJNSejwi7gOeABrAk8CdfSnGlZOSBmTz5s1c\nfPHF3HLLLQB84QtfoFKpsGXLFn79618zOTnJl770JW666aYBV9rjysmU0ueBz/e5lubKSYNb0vc2\nwytPn97HvPDdcP2XO968ceNGbr/99pngvvfee3n44Ye59dZbOeecc9i3bx8bNmzgxhtvHPi5MTNb\n8u4ZcCQNxpVXXsnevXt5+eWXGRsbY/ny5Vx44YXccccd/OhHP6JUKvHSSy/x6quvcuGFFw601syC\n25WTkug6Mu6nj33sY9x333288sorbNy4kbvvvpuxsTG2bdtGtVplzZo1bbdzPdOyCu5KucTksURK\naeB/ikg6+2zcuJFPf/rT7Nu3j0cffZR7772X888/n2q1ypYtW3jxxRcHXSKQWXDXys2wbkwlqmWD\nW9KZddlll/HGG2+wevVqLrroIj75yU/y4Q9/mHe/+92sX7+eSy+9dNAlApkFd6XcnJ04eWyKajmr\njQslnSWefvrNg6IrV67ksccea3u/gwcPnqmS5sgqHaszwe0BSknqJLPgbrZHnBIoSZ1lFtzNclw9\nKZ2dUnrr/+yfjq8xq+CulBxxS2eroaEh9u/f/5YO75QS+/fvZ2jo1M5Fk9XByVrlzYOTks4uIyMj\njI6OcrLbQi8UQ0NDjIyMnNJjZBXclZIHJ6WzVbVaZe3atYMuY0HIqlXiwUlJml9mwW2rRJLmk2Vw\nN6ZslUhSJ1kFd2W6VdJwxC1JnWQV3DOtEkfcktRRZsHtiFuS5pNZcE/3uA1uSeoks+BujrgnnMct\nSR1lFtzTe5U44pakTrIK7orzuCVpXlkF95srJ22VSFIneQV3yRG3JM0nr+CuuB+3JM0nq+Ce3o97\nwhG3JHWUVXB7BhxJml9WwV0uBaWwxy1J3WQV3NAcdU+6clKSOsozuBu2SiSpkwyDO9yrRJK6yC64\nK+WSPW5J6iK74K6VS66clKQusgvuSjkccUtSF9kFd7Vcch63JHWRXXBXSuHKSUnqIrvgrlVK7sct\nSV1kF9yVUnhwUpK66Cm4I2JZRNwXEbsiYmdEvL9fBVWdDihJXVV6vN9XgL9MKf1ORNSAer8KqpZL\nHJ5o9OvhJWnBmze4I+Jc4DeAfwmQUpoAJvpVUHPlpK0SSeqkl1bJWmAM+POIeDIivhkRw613iohN\nEbE1IraOjY2ddEGVcomJhq0SSeqkl+CuAO8B/jSldCVwCNjceqeU0p0ppfUppfWrVq066YJq5ZIj\nbknqopfgHgVGU0qPF5fvoxnkfeHKSUnqbt7gTim9AuyJiEuKqz4IPNuvglw5KUnd9Tqr5N8Cdxcz\nSv4O+Ff9KqhaduWkJHXTU3CnlLYD6/tcCzA94ja4JamTDFdOuq2rJHWTXXBXKx6clKRu8gvukkve\nJamb/IK7XGIqwTHncktSW9kFd6UcAI66JamD7IK7Vm6W5OpJSWovu+CeGXG7X4kktZVdcFeLEbet\nEklqL8PgLkbctkokqa0Mg7sYcdsqkaS2sgvuyszBSYNbktrJLrhrRatkomGrRJLayS64KyVH3JLU\nTXbBXa04q0SSuskvuEvTKydtlUhSO/kFtyNuSeoqu+CuFCNuT18mSe1lF9zT87g9fZkktZdtcDvi\nlqT2Mgxut3WVpG4yDG4PTkpSNxkHt60SSWonu+Ce3o/blZOS1F52wT0zq8TdASWprQyDe3rEbatE\nktrJMLjdj1uSuskuuKdXTnoGHElqL7vgjgiq5XA6oCR1kF1wQ3NP7obBLUltZRnczRG3rRJJaifT\n4C7ZKpGkDgxuSVpgsgzuSjncHVCSOsgyuGvlkvtxS1IHWQa3I25J6izL4LbHLUmdZRnclXLJlZOS\n1EHPwR0R5Yh4MiIe7GdBALVyuFeJJHVwIiPu24Cd/Spktkqp5H7cktRBT8EdESPAh4Bv9recpmql\nxIQHJyWprV5H3H8C/CFwRobB1VK4V4kkdTBvcEfEbwN7U0rb5rnfpojYGhFbx8bGTqkoZ5VIUme9\njLivBm6MiBeAbwPXRsR/a71TSunOlNL6lNL6VatWnVJRzuOWpM7mDe6U0mdTSiMppTXAzcBfp5Q+\n1c+iXDkpSZ3lNY/74c/Brv/tiFuSuqicyJ1TSo8Aj/SlEoBt/wVSolr+hD1uSeogrxF3rQ4TBz04\nKUld5BXc1TpMHvYMOJLURV7BXVsCE4eplF05KUmdZBbcdZg8VLRKEik56pakVnkFd7UOE4eolgKA\nhjsEStIceQV3bRgmDlOtNMvyAKUkzZVfcE8eolKMuD1AKUlz5RXcRauk5ohbkjrKK7iLVkml1CzL\n1ZOSNFd+wT15mGpRlSNuSZorr+Cu1oHEEBOAwS1J7eQV3LVhAIbSOODBSUlqJ6/grtYBWMQRwBG3\nJLWTV3BPj7injgIGtyS1k2Vw19JhwJWTktROXsFdtEpqU0WPu+GIW5Ja5RXc0yPuqaLH7YhbkubI\nNLgdcUtSJ3kFd9EqqTSaI2735JakufIK7lozuKtFq2TCedySNEdewV1ttkoqx4oRt9MBJWmOvIK7\nUoNSlUqjOR3QedySNFdewQ1Qq1OeCW5bJZLUKsPgXkL5mEveJamT/IK7Wqc0eQhwP25Jaie/4K7V\nKTWmZ5U44pakVvkFd3WY0mSxV4kjbkmaI7/grg0Tk4cphT1uSWonw+BunjC4Ui4x6cpJSZojv+Cu\nNs87WSuXmGzYKpGkVvkFd224GHGHe5VIUhsZBnezVVItl+xxS1Ib+QV3dRiOHWVRTLlyUpLayC+4\nix0Cl1YmHXFLUhsZBndzh8AlcdR53JLURn7BXWzturQ04cpJSWojv+AuWiVLSkfdj1uS2pg3uCPi\n4ojYEhHPRsQzEXFbXyua1Srx4KQkzVXp4T4N4A9SSk9ExFJgW0R8P6X0bF8qKlolw6WjHpyUpDbm\nHXGnlH6VUnqieP8NYCewum8VFa2S4ZgwuCWpjRPqcUfEGuBK4PF+FAPMnOm9zjiNKVslktSq5+CO\niCXA/cDtKaXX29y+KSK2RsTWsbGxk6+otgSAekww0XDELUmtegruiKjSDO27U0rfaXeflNKdKaX1\nKaX1q1atOvmKatMj7iOOuCWpjV5mlQTwZ8DOlNIf972iolWyGA9OSlI7vYy4rwZ+F7g2IrYXbzf0\nr6IyVBYzlFw5KUntzDsdMKX0N0CcgVreVKuzmCOunJSkNvJbOQlQHWbR1LgrJyWpjTyDu1ZnUXLl\npCS1k2lwDzOUjnhwUpLayDO4q3VqUwa3JLWTZ3DXhqlNjTOV4JhzuSXpONkGd3VqHMBRtyS1yDO4\nq3Vqxw4DuHpSklrkGdy1YSpTRwCYdL8SSTpOnsFdrVM9Ng4kJqcMbkmaLc/grg1TSg1qNJzLLUkt\nsg1uaG405epJSTpensE9czIFdwiUpFZ5Bncx4q7HuK0SSWqRd3A74pakOfIM7uNaJY64JWm2PIN7\n+uBkjDvilqQWWQf3MJ4FR5Ja5Rnc060SR9ySNEeewT1rHrfBLUnHyzq4PTgpSXPlGdyVIRJBPcZp\nuFeJJB0nz+COIFXr1DnKhLsDStJx8gxumAnug0cbgy5FkrKSbXCXFi1heXWCZ15+fdClSFJWsg3u\nqA1zwdAUPx09MOhSJCkr2QY31TorFjXYvfeg7RJJmiXf4K4Ns6w8QUrw9Ohrg65GkrKRdXAPxwQA\nT9kukaQZ+QZ3tU7l2GHefl6dp/YY3JI0Ld/grtVh4jBXXLzM4JakWTIO7iUwcYgrRs7l5dfG2fvG\n+KArkqQs5Bvc1TpMHuaKkXMB+OkeD1BKEuQc3LU6kLjs/CrlUniAUpIKGQf3EgDq6Sj/6IKlbLfP\nLUlAzsFdnEyByUOsu/hcntpzgJTc4lWS8g3uWhHcE4e5YmQZr483eGH/4cHWJEkZyDe4q82TKTDZ\nnBIIOC1QkugxuCPiuoj4WUT8PCI297soYOYsOEwc5B3nL2FxtWyfW5LoIbgjogz8J+B64F3AJyLi\nXf0ubHarpFIucfnqc9wpUJLobcR9FfDzlNLfpZQmgG8DN/W3LN5slRx8FRoTXDGyjB0vv+7JgyWd\n9So93Gc1sGfW5VHgn/SnnFkWN/vaPHg7PHg7m8uL+dflIfb9+xIQpJm3aQFAKv5n9vWtVxWP0M38\nt+ehtc65X33r19F6//Yf37y++2OfDt0+Q+tzOfcrafNcd7395B9rvo+fc3t0+/gT+052+zrm3rd/\nTqSOXpzso/V7btmpfJWHyufyzs/9+LTV0kkvwd2TiNgEbAJ4+9vffuoPuOR8+N0HYP/zMH6AdPD/\n8cov9jB17Bik1DZYSC3XpZl/5j7ZqeWHq/Wh5t59jtP9Qj5Z89UxNwDjuEudQiu1+aV3er/mWb92\n2z1Bx1XU/XKr7re3PFY60V9Qp/C5T3BK64lGfL/M9/1WU6O69Ix8nl6C+yXg4lmXR4rrjpNSuhO4\nE2D9+vWn51n+B9c234AqsO60PKgkLWy99Lj/L/COiFgbETXgZuAv+luWJKmTeUfcKaVGRPwb4GGg\nDNyVUnqm75VJktrqqcedUnoIeKjPtUiSepDvyklJUlsGtyQtMAa3JC0wBrckLTAGtyQtMNGPkxNE\nxBjw4kl++Epg32ks53TJtS7It7Zc64J8a8u1Lsi3tlzrghOr7e+llFb1cse+BPepiIitKaX1g66j\nVa51Qb615VoX5FtbrnVBvrXlWhf0rzZbJZK0wBjckrTA5Bjcdw66gA5yrQvyrS3XuiDf2nKtC/Kt\nLde6oE+1ZdfjliR1l+OIW5LURTbBPZATEneu5a6I2BsRO2Zdd15EfD8idhf/Lx9AXRdHxJaIeDYi\nnomI2zKqbSgi/jYinipq+2Jx/dqIeLx4Xv97sTXwGRcR5Yh4MiIezKyuFyLi6YjYHhFbi+tyeD6X\nRcR9EbErInZGxPszqeuS4ns1/fZ6RNyeSW13FK/9HRFxT/Ez0ZfXWRbBPbATEnf2n4HrWq7bDPww\npfQO4IfF5TOtAfxBSuldwAbgluL7lENtR4FrU0pX0DznxXURsQH4D8B/TCn9Q+DXwO8PoDaA24Cd\nsy7nUhfAb6aU1s2aNpbD8/kV4C9TSpcCV9D83g28rpTSz4rv1TrgvcBh4IFB1xYRq4FbgfUppctp\nboF9M/16naWUBv4GvB94eNblzwKfHXBNa4Adsy7/DLioeP8i4GcZfN++C/yz3GoD6sATNM9Nug+o\ntHuez2A9IzR/mK8FHqR5RrCB11V87heAlS3XDfT5BM4FfkFxDCyXutrU+c+B/5NDbbx5bt7zaG6X\n/SDwL/r1OstixE37ExKvHlAtnVyQUvpV8f4rwAWDLCYi1gBXAo+TSW1FO2I7sBf4PvA8cCCl1Cju\nMqjn9U+APwSmissrMqkLmieK/KuI2FactxUG/3yuBcaAPy/aS9+MiOEM6mp1M3BP8f5Aa0spvQT8\nEfBL4FfAa8A2+vQ6yyW4F5TU/PU5sOk4EbEEuB+4PaX0+uzbBllbSulYav4JOwJcBVw6iDpmi4jf\nBvamlLYNupYOrkkpvYdmm/CWiPiN2TcO6PmsAO8B/jSldCVwiJbWQwY/AzXgRuB/tN42iNqKnvpN\nNH/pvQ0YZm679bTJJbh7OiHxgL0aERcBFP/vHUQREVGlGdp3p5S+k1Nt01JKB4AtNP80XBYR02da\nGsTzejVwY0S8AHybZrvkKxnUBcyM1Egp7aXZq72KwT+fo8BoSunx4vJ9NIN80HXNdj3wRErp1eLy\noGv7LeAXKaWxlNIk8B2ar72+vM5yCe6FcELivwB+r3j/92j2l8+oiAjgz4CdKaU/zqy2VRGxrHh/\nMc3e+06aAf47g6otpfTZlNJISmkNzdfVX6eUPjnougAiYjgilk6/T7Nnu4MBP58ppVeAPRFxSXHV\nB4FnB11Xi0/wZpsEBl/bL4ENEVEvfk6nv2f9eZ0N8uBCS3P/BuA5mn3Rzw24lnto9qkmaY4+fp9m\nX/SHwG7gB8B5A6jrGpp/Av4U2F683ZBJbf8YeLKobQfw74rr/z7wt8DPaf5Zu2iAz+sHgAdzqauo\n4ani7Znp130mz+c6YGvxfP5PYHkOdRW1DQP7gXNnXTfw2oAvAruK1/9/BRb163XmyklJWmByaZVI\nknpkcEvSAmNwS9ICY3BL0gJjcEvSAmNwS9ICY3BL0gJjcEvSAvP/AeF3exJnlT6DAAAAAElFTkSu\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 1s 4ms/step - loss: 8.0778 - mean_squared_error: 8.0778 - val_loss: 2.7767 - val_mean_squared_error: 2.7767\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.77673, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.4781 - mean_squared_error: 0.4781 - val_loss: 0.0118 - val_mean_squared_error: 0.0118\n",
            "\n",
            "Epoch 00002: val_loss improved from 2.77673 to 0.01182, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0066 - mean_squared_error: 0.0066 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.01182 to 0.00296, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0041 - mean_squared_error: 0.0041 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00296 to 0.00219, saving model to weights.best_mlp.hdf5\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00219 to 0.00192, saving model to weights.best_mlp.hdf5\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00192 to 0.00181, saving model to weights.best_mlp.hdf5\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00181 to 0.00177, saving model to weights.best_mlp.hdf5\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00177 to 0.00176, saving model to weights.best_mlp.hdf5\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.00176 to 0.00175, saving model to weights.best_mlp.hdf5\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00175\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00175\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00175\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00175\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00175\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00175\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00175\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.9307e-04 - mean_squared_error: 9.9307e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00175\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.7603e-04 - mean_squared_error: 9.7603e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00175\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.6076e-04 - mean_squared_error: 9.6076e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00175\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.4412e-04 - mean_squared_error: 9.4412e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00175\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.2369e-04 - mean_squared_error: 9.2369e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00175\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.9921e-04 - mean_squared_error: 8.9921e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00175\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.7114e-04 - mean_squared_error: 8.7114e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00175\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.4205e-04 - mean_squared_error: 8.4205e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00175\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.1344e-04 - mean_squared_error: 8.1344e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00175\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.8975e-04 - mean_squared_error: 7.8975e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00175\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.6937e-04 - mean_squared_error: 7.6937e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00175\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.5288e-04 - mean_squared_error: 7.5288e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00175\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.4107e-04 - mean_squared_error: 7.4107e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00175\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.2974e-04 - mean_squared_error: 7.2974e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00175\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.1936e-04 - mean_squared_error: 7.1936e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00175\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.0736e-04 - mean_squared_error: 7.0736e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00175\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.9269e-04 - mean_squared_error: 6.9269e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00175\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.7835e-04 - mean_squared_error: 6.7835e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00175\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.6178e-04 - mean_squared_error: 6.6178e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00175\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.4050e-04 - mean_squared_error: 6.4050e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00175\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.2017e-04 - mean_squared_error: 6.2017e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00175\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.9983e-04 - mean_squared_error: 5.9983e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.00175 to 0.00163, saving model to weights.best_mlp.hdf5\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.7979e-04 - mean_squared_error: 5.7979e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.00163 to 0.00148, saving model to weights.best_mlp.hdf5\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.6575e-04 - mean_squared_error: 5.6575e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.00148 to 0.00136, saving model to weights.best_mlp.hdf5\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.4451e-04 - mean_squared_error: 5.4451e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.00136 to 0.00124, saving model to weights.best_mlp.hdf5\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.2668e-04 - mean_squared_error: 5.2668e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.00124 to 0.00113, saving model to weights.best_mlp.hdf5\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.0840e-04 - mean_squared_error: 5.0840e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.00113 to 0.00104, saving model to weights.best_mlp.hdf5\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.9211e-04 - mean_squared_error: 4.9211e-04 - val_loss: 9.5704e-04 - val_mean_squared_error: 9.5704e-04\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.00104 to 0.00096, saving model to weights.best_mlp.hdf5\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.8484e-04 - mean_squared_error: 4.8484e-04 - val_loss: 8.8357e-04 - val_mean_squared_error: 8.8357e-04\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.00096 to 0.00088, saving model to weights.best_mlp.hdf5\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.7013e-04 - mean_squared_error: 4.7013e-04 - val_loss: 8.2414e-04 - val_mean_squared_error: 8.2414e-04\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00088 to 0.00082, saving model to weights.best_mlp.hdf5\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.6602e-04 - mean_squared_error: 4.6602e-04 - val_loss: 7.6551e-04 - val_mean_squared_error: 7.6551e-04\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.00082 to 0.00077, saving model to weights.best_mlp.hdf5\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5625e-04 - mean_squared_error: 4.5625e-04 - val_loss: 7.1915e-04 - val_mean_squared_error: 7.1915e-04\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.00077 to 0.00072, saving model to weights.best_mlp.hdf5\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5346e-04 - mean_squared_error: 4.5346e-04 - val_loss: 6.7978e-04 - val_mean_squared_error: 6.7978e-04\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.00072 to 0.00068, saving model to weights.best_mlp.hdf5\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5269e-04 - mean_squared_error: 4.5269e-04 - val_loss: 6.4809e-04 - val_mean_squared_error: 6.4809e-04\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.00068 to 0.00065, saving model to weights.best_mlp.hdf5\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4338e-04 - mean_squared_error: 4.4338e-04 - val_loss: 6.1998e-04 - val_mean_squared_error: 6.1998e-04\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.00065 to 0.00062, saving model to weights.best_mlp.hdf5\n",
            "Epoch 52/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4188e-04 - mean_squared_error: 4.4188e-04 - val_loss: 5.9606e-04 - val_mean_squared_error: 5.9606e-04\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.00062 to 0.00060, saving model to weights.best_mlp.hdf5\n",
            "Epoch 53/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3781e-04 - mean_squared_error: 4.3781e-04 - val_loss: 5.7957e-04 - val_mean_squared_error: 5.7957e-04\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.00060 to 0.00058, saving model to weights.best_mlp.hdf5\n",
            "Epoch 54/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3509e-04 - mean_squared_error: 4.3509e-04 - val_loss: 5.6276e-04 - val_mean_squared_error: 5.6276e-04\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.00058 to 0.00056, saving model to weights.best_mlp.hdf5\n",
            "Epoch 55/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3559e-04 - mean_squared_error: 4.3559e-04 - val_loss: 5.4974e-04 - val_mean_squared_error: 5.4974e-04\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.00056 to 0.00055, saving model to weights.best_mlp.hdf5\n",
            "Epoch 56/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2956e-04 - mean_squared_error: 4.2956e-04 - val_loss: 5.4100e-04 - val_mean_squared_error: 5.4100e-04\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.00055 to 0.00054, saving model to weights.best_mlp.hdf5\n",
            "Epoch 57/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3467e-04 - mean_squared_error: 4.3467e-04 - val_loss: 5.3047e-04 - val_mean_squared_error: 5.3047e-04\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.00054 to 0.00053, saving model to weights.best_mlp.hdf5\n",
            "Epoch 58/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2629e-04 - mean_squared_error: 4.2629e-04 - val_loss: 5.2430e-04 - val_mean_squared_error: 5.2430e-04\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.00053 to 0.00052, saving model to weights.best_mlp.hdf5\n",
            "Epoch 59/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3423e-04 - mean_squared_error: 4.3423e-04 - val_loss: 5.1874e-04 - val_mean_squared_error: 5.1874e-04\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.00052 to 0.00052, saving model to weights.best_mlp.hdf5\n",
            "Epoch 60/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2218e-04 - mean_squared_error: 4.2218e-04 - val_loss: 5.1147e-04 - val_mean_squared_error: 5.1147e-04\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.00052 to 0.00051, saving model to weights.best_mlp.hdf5\n",
            "Epoch 61/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2626e-04 - mean_squared_error: 4.2626e-04 - val_loss: 5.0769e-04 - val_mean_squared_error: 5.0769e-04\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.00051 to 0.00051, saving model to weights.best_mlp.hdf5\n",
            "Epoch 62/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2288e-04 - mean_squared_error: 4.2288e-04 - val_loss: 5.0526e-04 - val_mean_squared_error: 5.0526e-04\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.00051 to 0.00051, saving model to weights.best_mlp.hdf5\n",
            "Epoch 00062: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 50\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 2\n",
            "MLP: Train: 0.009896, Validation: 0.000505\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJanHGNyS1GP+Pw4GFr45yDr/AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 1s 5ms/step - loss: 7.6062 - mean_squared_error: 7.6062 - val_loss: 2.2408 - val_mean_squared_error: 2.2408\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.24080, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.3401 - mean_squared_error: 0.3401 - val_loss: 0.0080 - val_mean_squared_error: 0.0080\n",
            "\n",
            "Epoch 00002: val_loss improved from 2.24080 to 0.00798, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0074 - mean_squared_error: 0.0074 - val_loss: 0.0038 - val_mean_squared_error: 0.0038\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00798 to 0.00377, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0051 - mean_squared_error: 0.0051 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00377 to 0.00230, saving model to weights.best_mlp.hdf5\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00230 to 0.00165, saving model to weights.best_mlp.hdf5\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00165 to 0.00143, saving model to weights.best_mlp.hdf5\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00143 to 0.00143, saving model to weights.best_mlp.hdf5\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00143\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00143\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00143\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00143\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00143\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00143\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00143\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00143\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00143\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00143\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00143\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.9793e-04 - mean_squared_error: 9.9793e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00143\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.8209e-04 - mean_squared_error: 9.8209e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00143\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.6295e-04 - mean_squared_error: 9.6295e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00143\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.3900e-04 - mean_squared_error: 9.3900e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00143\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.1085e-04 - mean_squared_error: 9.1085e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00143\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.7941e-04 - mean_squared_error: 8.7941e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00143\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.4937e-04 - mean_squared_error: 8.4937e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00143\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.2185e-04 - mean_squared_error: 8.2185e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00143\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.9914e-04 - mean_squared_error: 7.9914e-04 - val_loss: 0.0034 - val_mean_squared_error: 0.0034\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00143\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.8259e-04 - mean_squared_error: 7.8259e-04 - val_loss: 0.0034 - val_mean_squared_error: 0.0034\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00143\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.7107e-04 - mean_squared_error: 7.7107e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00143\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.6303e-04 - mean_squared_error: 7.6303e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00143\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.5347e-04 - mean_squared_error: 7.5347e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00143\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.4353e-04 - mean_squared_error: 7.4353e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00143\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.3149e-04 - mean_squared_error: 7.3149e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00143\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.1263e-04 - mean_squared_error: 7.1263e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00143\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.9095e-04 - mean_squared_error: 6.9095e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00143\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.7414e-04 - mean_squared_error: 6.7414e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00143\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.5653e-04 - mean_squared_error: 6.5653e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00143\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.2862e-04 - mean_squared_error: 6.2862e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00143\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.0645e-04 - mean_squared_error: 6.0645e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00143\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.7500e-04 - mean_squared_error: 5.7500e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.00143 to 0.00133, saving model to weights.best_mlp.hdf5\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.5996e-04 - mean_squared_error: 5.5996e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.00133 to 0.00116, saving model to weights.best_mlp.hdf5\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.3192e-04 - mean_squared_error: 5.3192e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.00116 to 0.00106, saving model to weights.best_mlp.hdf5\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.1479e-04 - mean_squared_error: 5.1479e-04 - val_loss: 9.5374e-04 - val_mean_squared_error: 9.5374e-04\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.00106 to 0.00095, saving model to weights.best_mlp.hdf5\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.9811e-04 - mean_squared_error: 4.9811e-04 - val_loss: 8.6530e-04 - val_mean_squared_error: 8.6530e-04\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.00095 to 0.00087, saving model to weights.best_mlp.hdf5\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.8277e-04 - mean_squared_error: 4.8277e-04 - val_loss: 7.8870e-04 - val_mean_squared_error: 7.8870e-04\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.00087 to 0.00079, saving model to weights.best_mlp.hdf5\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.6605e-04 - mean_squared_error: 4.6605e-04 - val_loss: 7.2521e-04 - val_mean_squared_error: 7.2521e-04\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00079 to 0.00073, saving model to weights.best_mlp.hdf5\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5891e-04 - mean_squared_error: 4.5891e-04 - val_loss: 6.6786e-04 - val_mean_squared_error: 6.6786e-04\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.00073 to 0.00067, saving model to weights.best_mlp.hdf5\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4439e-04 - mean_squared_error: 4.4439e-04 - val_loss: 6.1875e-04 - val_mean_squared_error: 6.1875e-04\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.00067 to 0.00062, saving model to weights.best_mlp.hdf5\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3329e-04 - mean_squared_error: 4.3329e-04 - val_loss: 5.7453e-04 - val_mean_squared_error: 5.7453e-04\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.00062 to 0.00057, saving model to weights.best_mlp.hdf5\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2728e-04 - mean_squared_error: 4.2728e-04 - val_loss: 5.4061e-04 - val_mean_squared_error: 5.4061e-04\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.00057 to 0.00054, saving model to weights.best_mlp.hdf5\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2021e-04 - mean_squared_error: 4.2021e-04 - val_loss: 5.1156e-04 - val_mean_squared_error: 5.1156e-04\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.00054 to 0.00051, saving model to weights.best_mlp.hdf5\n",
            "Epoch 52/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2040e-04 - mean_squared_error: 4.2040e-04 - val_loss: 4.8917e-04 - val_mean_squared_error: 4.8917e-04\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.00051 to 0.00049, saving model to weights.best_mlp.hdf5\n",
            "Epoch 53/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1241e-04 - mean_squared_error: 4.1241e-04 - val_loss: 4.7387e-04 - val_mean_squared_error: 4.7387e-04\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.00049 to 0.00047, saving model to weights.best_mlp.hdf5\n",
            "Epoch 54/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1429e-04 - mean_squared_error: 4.1429e-04 - val_loss: 4.6498e-04 - val_mean_squared_error: 4.6498e-04\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.00047 to 0.00046, saving model to weights.best_mlp.hdf5\n",
            "Epoch 55/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1204e-04 - mean_squared_error: 4.1204e-04 - val_loss: 4.5224e-04 - val_mean_squared_error: 4.5224e-04\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.00046 to 0.00045, saving model to weights.best_mlp.hdf5\n",
            "Epoch 56/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1327e-04 - mean_squared_error: 4.1327e-04 - val_loss: 4.4981e-04 - val_mean_squared_error: 4.4981e-04\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.00045 to 0.00045, saving model to weights.best_mlp.hdf5\n",
            "Epoch 57/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1042e-04 - mean_squared_error: 4.1042e-04 - val_loss: 4.4554e-04 - val_mean_squared_error: 4.4554e-04\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.00045 to 0.00045, saving model to weights.best_mlp.hdf5\n",
            "Epoch 58/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0943e-04 - mean_squared_error: 4.0943e-04 - val_loss: 4.3224e-04 - val_mean_squared_error: 4.3224e-04\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.00045 to 0.00043, saving model to weights.best_mlp.hdf5\n",
            "Epoch 59/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0894e-04 - mean_squared_error: 4.0894e-04 - val_loss: 4.3732e-04 - val_mean_squared_error: 4.3732e-04\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.00043\n",
            "Epoch 60/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2151e-04 - mean_squared_error: 4.2151e-04 - val_loss: 4.3583e-04 - val_mean_squared_error: 4.3583e-04\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.00043\n",
            "Epoch 61/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0482e-04 - mean_squared_error: 4.0482e-04 - val_loss: 4.3443e-04 - val_mean_squared_error: 4.3443e-04\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.00043\n",
            "Epoch 62/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3919e-04 - mean_squared_error: 4.3919e-04 - val_loss: 4.5044e-04 - val_mean_squared_error: 4.5044e-04\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.00043\n",
            "Epoch 63/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0938e-04 - mean_squared_error: 4.0938e-04 - val_loss: 4.3802e-04 - val_mean_squared_error: 4.3802e-04\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.00043\n",
            "Epoch 64/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2978e-04 - mean_squared_error: 4.2978e-04 - val_loss: 4.4868e-04 - val_mean_squared_error: 4.4868e-04\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.00043\n",
            "Epoch 00064: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 50\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 3\n",
            "MLP: Train: 0.010114, Validation: 0.000449\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD9CAYAAACcJ53WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nS1KPMbglqcf8f8nsG9xMO71QAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 1s 5ms/step - loss: 7.7595 - mean_squared_error: 7.7595 - val_loss: 2.5035 - val_mean_squared_error: 2.5035\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.50352, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.4101 - mean_squared_error: 0.4101 - val_loss: 0.0076 - val_mean_squared_error: 0.0076\n",
            "\n",
            "Epoch 00002: val_loss improved from 2.50352 to 0.00763, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0049 - mean_squared_error: 0.0049 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00763 to 0.00275, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0040 - mean_squared_error: 0.0040 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00275 to 0.00222, saving model to weights.best_mlp.hdf5\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00222 to 0.00197, saving model to weights.best_mlp.hdf5\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00197 to 0.00186, saving model to weights.best_mlp.hdf5\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00186 to 0.00183, saving model to weights.best_mlp.hdf5\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00183\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00183\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00183\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00183\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00183\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00183\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00183\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00183\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00183\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.8146e-04 - mean_squared_error: 9.8146e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.00183 to 0.00181, saving model to weights.best_mlp.hdf5\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.5311e-04 - mean_squared_error: 9.5311e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.00181 to 0.00180, saving model to weights.best_mlp.hdf5\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.3189e-04 - mean_squared_error: 9.3189e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00180\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.1380e-04 - mean_squared_error: 9.1380e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00180\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.9557e-04 - mean_squared_error: 8.9557e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00180\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.7433e-04 - mean_squared_error: 8.7433e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00180\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.4954e-04 - mean_squared_error: 8.4954e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00180\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.2189e-04 - mean_squared_error: 8.2189e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00180\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.9359e-04 - mean_squared_error: 7.9359e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00180\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.6681e-04 - mean_squared_error: 7.6681e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00180\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.4244e-04 - mean_squared_error: 7.4244e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00180\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.2370e-04 - mean_squared_error: 7.2370e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00180\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.1210e-04 - mean_squared_error: 7.1210e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00180\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.0386e-04 - mean_squared_error: 7.0386e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00180\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.9485e-04 - mean_squared_error: 6.9485e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00180\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.8811e-04 - mean_squared_error: 6.8811e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00180\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.8041e-04 - mean_squared_error: 6.8041e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00180\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.7181e-04 - mean_squared_error: 6.7181e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00180\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.5612e-04 - mean_squared_error: 6.5612e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00180\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.3720e-04 - mean_squared_error: 6.3720e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00180\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.2277e-04 - mean_squared_error: 6.2277e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00180\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.9911e-04 - mean_squared_error: 5.9911e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.00180 to 0.00175, saving model to weights.best_mlp.hdf5\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.7918e-04 - mean_squared_error: 5.7918e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.00175 to 0.00159, saving model to weights.best_mlp.hdf5\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.5502e-04 - mean_squared_error: 5.5502e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.00159 to 0.00145, saving model to weights.best_mlp.hdf5\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.3307e-04 - mean_squared_error: 5.3307e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.00145 to 0.00132, saving model to weights.best_mlp.hdf5\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.0795e-04 - mean_squared_error: 5.0795e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.00132 to 0.00120, saving model to weights.best_mlp.hdf5\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.8778e-04 - mean_squared_error: 4.8778e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.00120 to 0.00110, saving model to weights.best_mlp.hdf5\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.7003e-04 - mean_squared_error: 4.7003e-04 - val_loss: 9.9436e-04 - val_mean_squared_error: 9.9436e-04\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.00110 to 0.00099, saving model to weights.best_mlp.hdf5\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5307e-04 - mean_squared_error: 4.5307e-04 - val_loss: 9.2138e-04 - val_mean_squared_error: 9.2138e-04\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.00099 to 0.00092, saving model to weights.best_mlp.hdf5\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3817e-04 - mean_squared_error: 4.3817e-04 - val_loss: 8.3939e-04 - val_mean_squared_error: 8.3939e-04\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00092 to 0.00084, saving model to weights.best_mlp.hdf5\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2676e-04 - mean_squared_error: 4.2676e-04 - val_loss: 7.7214e-04 - val_mean_squared_error: 7.7214e-04\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.00084 to 0.00077, saving model to weights.best_mlp.hdf5\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1800e-04 - mean_squared_error: 4.1800e-04 - val_loss: 7.2314e-04 - val_mean_squared_error: 7.2314e-04\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.00077 to 0.00072, saving model to weights.best_mlp.hdf5\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1333e-04 - mean_squared_error: 4.1333e-04 - val_loss: 6.7603e-04 - val_mean_squared_error: 6.7603e-04\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.00072 to 0.00068, saving model to weights.best_mlp.hdf5\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1020e-04 - mean_squared_error: 4.1020e-04 - val_loss: 6.3700e-04 - val_mean_squared_error: 6.3700e-04\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.00068 to 0.00064, saving model to weights.best_mlp.hdf5\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0731e-04 - mean_squared_error: 4.0731e-04 - val_loss: 6.0658e-04 - val_mean_squared_error: 6.0658e-04\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.00064 to 0.00061, saving model to weights.best_mlp.hdf5\n",
            "Epoch 52/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0478e-04 - mean_squared_error: 4.0478e-04 - val_loss: 5.7851e-04 - val_mean_squared_error: 5.7851e-04\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.00061 to 0.00058, saving model to weights.best_mlp.hdf5\n",
            "Epoch 53/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0411e-04 - mean_squared_error: 4.0411e-04 - val_loss: 5.5974e-04 - val_mean_squared_error: 5.5974e-04\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.00058 to 0.00056, saving model to weights.best_mlp.hdf5\n",
            "Epoch 54/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0368e-04 - mean_squared_error: 4.0368e-04 - val_loss: 5.4224e-04 - val_mean_squared_error: 5.4224e-04\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.00056 to 0.00054, saving model to weights.best_mlp.hdf5\n",
            "Epoch 55/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0222e-04 - mean_squared_error: 4.0222e-04 - val_loss: 5.3090e-04 - val_mean_squared_error: 5.3090e-04\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.00054 to 0.00053, saving model to weights.best_mlp.hdf5\n",
            "Epoch 56/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0197e-04 - mean_squared_error: 4.0197e-04 - val_loss: 5.2030e-04 - val_mean_squared_error: 5.2030e-04\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.00053 to 0.00052, saving model to weights.best_mlp.hdf5\n",
            "Epoch 57/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0550e-04 - mean_squared_error: 4.0550e-04 - val_loss: 5.1371e-04 - val_mean_squared_error: 5.1371e-04\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.00052 to 0.00051, saving model to weights.best_mlp.hdf5\n",
            "Epoch 58/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0327e-04 - mean_squared_error: 4.0327e-04 - val_loss: 5.0376e-04 - val_mean_squared_error: 5.0376e-04\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.00051 to 0.00050, saving model to weights.best_mlp.hdf5\n",
            "Epoch 59/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0534e-04 - mean_squared_error: 4.0534e-04 - val_loss: 5.0378e-04 - val_mean_squared_error: 5.0378e-04\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.00050\n",
            "Epoch 60/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0374e-04 - mean_squared_error: 4.0374e-04 - val_loss: 4.9447e-04 - val_mean_squared_error: 4.9447e-04\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.00050 to 0.00049, saving model to weights.best_mlp.hdf5\n",
            "Epoch 00060: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 50\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 4\n",
            "MLP: Train: 0.009926, Validation: 0.000494\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 1s 6ms/step - loss: 8.7817 - mean_squared_error: 8.7817 - val_loss: 4.1899 - val_mean_squared_error: 4.1899\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 4.18986, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 1.0799 - mean_squared_error: 1.0799 - val_loss: 0.0781 - val_mean_squared_error: 0.0781\n",
            "\n",
            "Epoch 00002: val_loss improved from 4.18986 to 0.07814, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0122 - mean_squared_error: 0.0122 - val_loss: 0.0034 - val_mean_squared_error: 0.0034\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.07814 to 0.00341, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0040 - mean_squared_error: 0.0040 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00341 to 0.00214, saving model to weights.best_mlp.hdf5\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00214 to 0.00177, saving model to weights.best_mlp.hdf5\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00177 to 0.00164, saving model to weights.best_mlp.hdf5\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00164 to 0.00161, saving model to weights.best_mlp.hdf5\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00161\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00161\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00161\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00161\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00161\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00161\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00161\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00161\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00161\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00161\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00161\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00161\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00161\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00161\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.9217e-04 - mean_squared_error: 9.9217e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00161\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.6758e-04 - mean_squared_error: 9.6758e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00161\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.4024e-04 - mean_squared_error: 9.4024e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00161\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.1053e-04 - mean_squared_error: 9.1053e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00161\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.8065e-04 - mean_squared_error: 8.8065e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00161\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.5294e-04 - mean_squared_error: 8.5294e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00161\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.2876e-04 - mean_squared_error: 8.2876e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00161\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.0765e-04 - mean_squared_error: 8.0765e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00161\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.9245e-04 - mean_squared_error: 7.9245e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00161\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.8191e-04 - mean_squared_error: 7.8191e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00161\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.7062e-04 - mean_squared_error: 7.7062e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00161\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.6437e-04 - mean_squared_error: 7.6437e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00161\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.5511e-04 - mean_squared_error: 7.5511e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00161\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.4869e-04 - mean_squared_error: 7.4869e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00161\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.3911e-04 - mean_squared_error: 7.3911e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00161\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.3009e-04 - mean_squared_error: 7.3009e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00161\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.1954e-04 - mean_squared_error: 7.1954e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00161\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.0654e-04 - mean_squared_error: 7.0654e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00161\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.8870e-04 - mean_squared_error: 6.8870e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00161\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.7389e-04 - mean_squared_error: 6.7389e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00161\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.5371e-04 - mean_squared_error: 6.5371e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00161\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.3708e-04 - mean_squared_error: 6.3708e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00161\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.1775e-04 - mean_squared_error: 6.1775e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00161\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.9780e-04 - mean_squared_error: 5.9780e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.00161 to 0.00160, saving model to weights.best_mlp.hdf5\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.7833e-04 - mean_squared_error: 5.7833e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00160 to 0.00152, saving model to weights.best_mlp.hdf5\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.5777e-04 - mean_squared_error: 5.5777e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.00152 to 0.00143, saving model to weights.best_mlp.hdf5\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.4121e-04 - mean_squared_error: 5.4121e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.00143 to 0.00134, saving model to weights.best_mlp.hdf5\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.2267e-04 - mean_squared_error: 5.2267e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.00134 to 0.00127, saving model to weights.best_mlp.hdf5\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.0481e-04 - mean_squared_error: 5.0481e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.00127 to 0.00119, saving model to weights.best_mlp.hdf5\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.9781e-04 - mean_squared_error: 4.9781e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.00119 to 0.00114, saving model to weights.best_mlp.hdf5\n",
            "Epoch 52/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.7695e-04 - mean_squared_error: 4.7695e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.00114 to 0.00108, saving model to weights.best_mlp.hdf5\n",
            "Epoch 53/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.6435e-04 - mean_squared_error: 4.6435e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.00108 to 0.00104, saving model to weights.best_mlp.hdf5\n",
            "Epoch 54/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5677e-04 - mean_squared_error: 4.5677e-04 - val_loss: 9.8370e-04 - val_mean_squared_error: 9.8370e-04\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.00104 to 0.00098, saving model to weights.best_mlp.hdf5\n",
            "Epoch 55/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4706e-04 - mean_squared_error: 4.4706e-04 - val_loss: 9.4971e-04 - val_mean_squared_error: 9.4971e-04\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.00098 to 0.00095, saving model to weights.best_mlp.hdf5\n",
            "Epoch 56/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4581e-04 - mean_squared_error: 4.4581e-04 - val_loss: 9.1735e-04 - val_mean_squared_error: 9.1735e-04\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.00095 to 0.00092, saving model to weights.best_mlp.hdf5\n",
            "Epoch 57/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3384e-04 - mean_squared_error: 4.3384e-04 - val_loss: 8.8651e-04 - val_mean_squared_error: 8.8651e-04\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.00092 to 0.00089, saving model to weights.best_mlp.hdf5\n",
            "Epoch 58/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2605e-04 - mean_squared_error: 4.2605e-04 - val_loss: 8.5563e-04 - val_mean_squared_error: 8.5563e-04\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.00089 to 0.00086, saving model to weights.best_mlp.hdf5\n",
            "Epoch 59/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2068e-04 - mean_squared_error: 4.2068e-04 - val_loss: 8.2804e-04 - val_mean_squared_error: 8.2804e-04\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.00086 to 0.00083, saving model to weights.best_mlp.hdf5\n",
            "Epoch 60/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1696e-04 - mean_squared_error: 4.1696e-04 - val_loss: 8.1096e-04 - val_mean_squared_error: 8.1096e-04\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.00083 to 0.00081, saving model to weights.best_mlp.hdf5\n",
            "Epoch 61/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1163e-04 - mean_squared_error: 4.1163e-04 - val_loss: 7.8323e-04 - val_mean_squared_error: 7.8323e-04\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.00081 to 0.00078, saving model to weights.best_mlp.hdf5\n",
            "Epoch 62/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0269e-04 - mean_squared_error: 4.0269e-04 - val_loss: 7.6378e-04 - val_mean_squared_error: 7.6378e-04\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.00078 to 0.00076, saving model to weights.best_mlp.hdf5\n",
            "Epoch 63/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0583e-04 - mean_squared_error: 4.0583e-04 - val_loss: 7.4695e-04 - val_mean_squared_error: 7.4695e-04\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.00076 to 0.00075, saving model to weights.best_mlp.hdf5\n",
            "Epoch 64/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0369e-04 - mean_squared_error: 4.0369e-04 - val_loss: 7.3526e-04 - val_mean_squared_error: 7.3526e-04\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.00075 to 0.00074, saving model to weights.best_mlp.hdf5\n",
            "Epoch 00064: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 50\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 2\n",
            "MLP: Train: 0.008976, Validation: 0.000735\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDG5J6jAGtyR1GINbkjrM/wcmu5cAaLA7ZQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 1s 6ms/step - loss: 8.5163 - mean_squared_error: 8.5163 - val_loss: 3.7543 - val_mean_squared_error: 3.7543\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.75426, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.8347 - mean_squared_error: 0.8347 - val_loss: 0.0247 - val_mean_squared_error: 0.0247\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.75426 to 0.02472, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0059 - mean_squared_error: 0.0059 - val_loss: 0.0036 - val_mean_squared_error: 0.0036\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.02472 to 0.00362, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0052 - mean_squared_error: 0.0052 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00362 to 0.00265, saving model to weights.best_mlp.hdf5\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00265 to 0.00208, saving model to weights.best_mlp.hdf5\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00208 to 0.00174, saving model to weights.best_mlp.hdf5\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00174 to 0.00156, saving model to weights.best_mlp.hdf5\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00156 to 0.00149, saving model to weights.best_mlp.hdf5\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.00149 to 0.00148, saving model to weights.best_mlp.hdf5\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00148\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00148\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00148\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00148\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00148\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00148\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00148\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00148\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00148\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.8891e-04 - mean_squared_error: 9.8891e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00148\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.6401e-04 - mean_squared_error: 9.6401e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00148\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.4514e-04 - mean_squared_error: 9.4514e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00148\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.3059e-04 - mean_squared_error: 9.3059e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00148\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.1480e-04 - mean_squared_error: 9.1480e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00148\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.9599e-04 - mean_squared_error: 8.9599e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00148\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.7490e-04 - mean_squared_error: 8.7490e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00148\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.4843e-04 - mean_squared_error: 8.4843e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00148\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.2078e-04 - mean_squared_error: 8.2078e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00148\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.9313e-04 - mean_squared_error: 7.9313e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00148\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.6859e-04 - mean_squared_error: 7.6859e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00148\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.4709e-04 - mean_squared_error: 7.4709e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00148\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.3057e-04 - mean_squared_error: 7.3057e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00148\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.1806e-04 - mean_squared_error: 7.1806e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00148\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.1002e-04 - mean_squared_error: 7.1002e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00148\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.0379e-04 - mean_squared_error: 7.0379e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00148\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.9731e-04 - mean_squared_error: 6.9731e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00148\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.9318e-04 - mean_squared_error: 6.9318e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00148\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.8391e-04 - mean_squared_error: 6.8391e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00148\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.7421e-04 - mean_squared_error: 6.7421e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00148\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.5373e-04 - mean_squared_error: 6.5373e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00148\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.5043e-04 - mean_squared_error: 6.5043e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00148\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.2452e-04 - mean_squared_error: 6.2452e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00148\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.1023e-04 - mean_squared_error: 6.1023e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00148\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.8867e-04 - mean_squared_error: 5.8867e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00148\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.7159e-04 - mean_squared_error: 5.7159e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00148\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.4701e-04 - mean_squared_error: 5.4701e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00148\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.2193e-04 - mean_squared_error: 5.2193e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00148 to 0.00143, saving model to weights.best_mlp.hdf5\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.0776e-04 - mean_squared_error: 5.0776e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.00143 to 0.00132, saving model to weights.best_mlp.hdf5\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.8476e-04 - mean_squared_error: 4.8476e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.00132 to 0.00125, saving model to weights.best_mlp.hdf5\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.7834e-04 - mean_squared_error: 4.7834e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.00125 to 0.00116, saving model to weights.best_mlp.hdf5\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5918e-04 - mean_squared_error: 4.5918e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.00116 to 0.00108, saving model to weights.best_mlp.hdf5\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4353e-04 - mean_squared_error: 4.4353e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.00108 to 0.00100, saving model to weights.best_mlp.hdf5\n",
            "Epoch 52/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3377e-04 - mean_squared_error: 4.3377e-04 - val_loss: 9.5231e-04 - val_mean_squared_error: 9.5231e-04\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.00100 to 0.00095, saving model to weights.best_mlp.hdf5\n",
            "Epoch 53/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2826e-04 - mean_squared_error: 4.2826e-04 - val_loss: 9.0788e-04 - val_mean_squared_error: 9.0788e-04\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.00095 to 0.00091, saving model to weights.best_mlp.hdf5\n",
            "Epoch 54/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1655e-04 - mean_squared_error: 4.1655e-04 - val_loss: 8.6445e-04 - val_mean_squared_error: 8.6445e-04\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.00091 to 0.00086, saving model to weights.best_mlp.hdf5\n",
            "Epoch 55/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1365e-04 - mean_squared_error: 4.1365e-04 - val_loss: 8.2961e-04 - val_mean_squared_error: 8.2961e-04\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.00086 to 0.00083, saving model to weights.best_mlp.hdf5\n",
            "Epoch 56/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0770e-04 - mean_squared_error: 4.0770e-04 - val_loss: 7.8578e-04 - val_mean_squared_error: 7.8578e-04\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.00083 to 0.00079, saving model to weights.best_mlp.hdf5\n",
            "Epoch 57/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1390e-04 - mean_squared_error: 4.1390e-04 - val_loss: 7.5987e-04 - val_mean_squared_error: 7.5987e-04\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.00079 to 0.00076, saving model to weights.best_mlp.hdf5\n",
            "Epoch 58/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9092e-04 - mean_squared_error: 3.9092e-04 - val_loss: 7.3371e-04 - val_mean_squared_error: 7.3371e-04\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.00076 to 0.00073, saving model to weights.best_mlp.hdf5\n",
            "Epoch 59/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9737e-04 - mean_squared_error: 3.9737e-04 - val_loss: 7.0122e-04 - val_mean_squared_error: 7.0122e-04\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.00073 to 0.00070, saving model to weights.best_mlp.hdf5\n",
            "Epoch 60/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0132e-04 - mean_squared_error: 4.0132e-04 - val_loss: 6.8695e-04 - val_mean_squared_error: 6.8695e-04\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.00070 to 0.00069, saving model to weights.best_mlp.hdf5\n",
            "Epoch 61/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0158e-04 - mean_squared_error: 4.0158e-04 - val_loss: 6.6169e-04 - val_mean_squared_error: 6.6169e-04\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.00069 to 0.00066, saving model to weights.best_mlp.hdf5\n",
            "Epoch 00061: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 50\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 3\n",
            "MLP: Train: 0.009255, Validation: 0.000662\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUpcxuCWpyxjcktRlDG5J6jL/H5EOlQXhQyXdAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 1s 6ms/step - loss: 8.7855 - mean_squared_error: 8.7855 - val_loss: 4.3816 - val_mean_squared_error: 4.3816\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 4.38160, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 1.1117 - mean_squared_error: 1.1117 - val_loss: 0.0507 - val_mean_squared_error: 0.0507\n",
            "\n",
            "Epoch 00002: val_loss improved from 4.38160 to 0.05068, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0051 - mean_squared_error: 0.0051 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.05068 to 0.00319, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00319 to 0.00232, saving model to weights.best_mlp.hdf5\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00232 to 0.00183, saving model to weights.best_mlp.hdf5\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00183 to 0.00151, saving model to weights.best_mlp.hdf5\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00151 to 0.00134, saving model to weights.best_mlp.hdf5\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00134 to 0.00128, saving model to weights.best_mlp.hdf5\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00128\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00128\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00128\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00128\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00128\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00128\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00128\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00128\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00128\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00128\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00128\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00128\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00128\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00128\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00128\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00128\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.8615e-04 - mean_squared_error: 9.8615e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00128\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.6271e-04 - mean_squared_error: 9.6271e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00128\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.3518e-04 - mean_squared_error: 9.3518e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00128\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.0676e-04 - mean_squared_error: 9.0676e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00128\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.7809e-04 - mean_squared_error: 8.7809e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00128\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.4861e-04 - mean_squared_error: 8.4861e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00128\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.2485e-04 - mean_squared_error: 8.2485e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00128\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.0573e-04 - mean_squared_error: 8.0573e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00128\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.9359e-04 - mean_squared_error: 7.9359e-04 - val_loss: 0.0034 - val_mean_squared_error: 0.0034\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00128\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.8173e-04 - mean_squared_error: 7.8173e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00128\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.7521e-04 - mean_squared_error: 7.7521e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00128\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.7188e-04 - mean_squared_error: 7.7188e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00128\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.6712e-04 - mean_squared_error: 7.6712e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00128\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.5708e-04 - mean_squared_error: 7.5708e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00128\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.5030e-04 - mean_squared_error: 7.5030e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00128\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.4314e-04 - mean_squared_error: 7.4314e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00128\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.2902e-04 - mean_squared_error: 7.2902e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00128\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.1149e-04 - mean_squared_error: 7.1149e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00128\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.1010e-04 - mean_squared_error: 7.1010e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00128\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.8326e-04 - mean_squared_error: 6.8326e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00128\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.8446e-04 - mean_squared_error: 6.8446e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00128\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.5014e-04 - mean_squared_error: 6.5014e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.00128\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.6272e-04 - mean_squared_error: 6.6272e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00128\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.2447e-04 - mean_squared_error: 6.2447e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.00128\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.3301e-04 - mean_squared_error: 6.3301e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.00128\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.0889e-04 - mean_squared_error: 6.0889e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.00128\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.8782e-04 - mean_squared_error: 5.8782e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.00128\n",
            "Epoch 52/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.8312e-04 - mean_squared_error: 5.8312e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.00128\n",
            "Epoch 53/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.7593e-04 - mean_squared_error: 5.7593e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.00128\n",
            "Epoch 54/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.6816e-04 - mean_squared_error: 5.6816e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.00128\n",
            "Epoch 55/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.6022e-04 - mean_squared_error: 5.6022e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.00128\n",
            "Epoch 56/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.4389e-04 - mean_squared_error: 5.4389e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.00128\n",
            "Epoch 57/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.1826e-04 - mean_squared_error: 5.1826e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.00128\n",
            "Epoch 58/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.2059e-04 - mean_squared_error: 5.2059e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.00128\n",
            "Epoch 59/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.1638e-04 - mean_squared_error: 5.1638e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.00128 to 0.00126, saving model to weights.best_mlp.hdf5\n",
            "Epoch 60/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.0887e-04 - mean_squared_error: 5.0887e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.00126 to 0.00122, saving model to weights.best_mlp.hdf5\n",
            "Epoch 61/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.0016e-04 - mean_squared_error: 5.0016e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.00122 to 0.00122, saving model to weights.best_mlp.hdf5\n",
            "Epoch 62/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.9942e-04 - mean_squared_error: 4.9942e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.00122 to 0.00119, saving model to weights.best_mlp.hdf5\n",
            "Epoch 63/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.8014e-04 - mean_squared_error: 4.8014e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.00119 to 0.00119, saving model to weights.best_mlp.hdf5\n",
            "Epoch 64/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.8768e-04 - mean_squared_error: 4.8768e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.00119 to 0.00116, saving model to weights.best_mlp.hdf5\n",
            "Epoch 65/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.8405e-04 - mean_squared_error: 4.8405e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.00116 to 0.00116, saving model to weights.best_mlp.hdf5\n",
            "Epoch 66/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.7635e-04 - mean_squared_error: 4.7635e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.00116 to 0.00115, saving model to weights.best_mlp.hdf5\n",
            "Epoch 67/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.6371e-04 - mean_squared_error: 4.6371e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.00115 to 0.00113, saving model to weights.best_mlp.hdf5\n",
            "Epoch 68/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5852e-04 - mean_squared_error: 4.5852e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.00113 to 0.00112, saving model to weights.best_mlp.hdf5\n",
            "Epoch 69/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5853e-04 - mean_squared_error: 4.5853e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.00112 to 0.00111, saving model to weights.best_mlp.hdf5\n",
            "Epoch 70/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4991e-04 - mean_squared_error: 4.4991e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.00111\n",
            "Epoch 71/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4867e-04 - mean_squared_error: 4.4867e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.00111 to 0.00107, saving model to weights.best_mlp.hdf5\n",
            "Epoch 72/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3481e-04 - mean_squared_error: 4.3481e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.00107\n",
            "Epoch 73/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4955e-04 - mean_squared_error: 4.4955e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.00107 to 0.00107, saving model to weights.best_mlp.hdf5\n",
            "Epoch 74/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3960e-04 - mean_squared_error: 4.3960e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.00107 to 0.00106, saving model to weights.best_mlp.hdf5\n",
            "Epoch 75/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2564e-04 - mean_squared_error: 4.2564e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.00106\n",
            "Epoch 76/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3649e-04 - mean_squared_error: 4.3649e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.00106\n",
            "Epoch 77/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2500e-04 - mean_squared_error: 4.2500e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.00106 to 0.00103, saving model to weights.best_mlp.hdf5\n",
            "Epoch 78/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1313e-04 - mean_squared_error: 4.1313e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.00103 to 0.00102, saving model to weights.best_mlp.hdf5\n",
            "Epoch 79/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2704e-04 - mean_squared_error: 4.2704e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.00102\n",
            "Epoch 80/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2035e-04 - mean_squared_error: 4.2035e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.00102 to 0.00101, saving model to weights.best_mlp.hdf5\n",
            "Epoch 81/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0882e-04 - mean_squared_error: 4.0882e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.00101\n",
            "Epoch 82/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1058e-04 - mean_squared_error: 4.1058e-04 - val_loss: 9.7802e-04 - val_mean_squared_error: 9.7802e-04\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.00101 to 0.00098, saving model to weights.best_mlp.hdf5\n",
            "Epoch 83/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0386e-04 - mean_squared_error: 4.0386e-04 - val_loss: 9.8613e-04 - val_mean_squared_error: 9.8613e-04\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.00098\n",
            "Epoch 84/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1522e-04 - mean_squared_error: 4.1522e-04 - val_loss: 9.8320e-04 - val_mean_squared_error: 9.8320e-04\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.00098\n",
            "Epoch 85/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0757e-04 - mean_squared_error: 4.0757e-04 - val_loss: 9.6931e-04 - val_mean_squared_error: 9.6931e-04\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.00098 to 0.00097, saving model to weights.best_mlp.hdf5\n",
            "Epoch 86/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0254e-04 - mean_squared_error: 4.0254e-04 - val_loss: 9.6117e-04 - val_mean_squared_error: 9.6117e-04\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.00097 to 0.00096, saving model to weights.best_mlp.hdf5\n",
            "Epoch 87/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0174e-04 - mean_squared_error: 4.0174e-04 - val_loss: 9.5134e-04 - val_mean_squared_error: 9.5134e-04\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.00096 to 0.00095, saving model to weights.best_mlp.hdf5\n",
            "Epoch 88/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8862e-04 - mean_squared_error: 3.8862e-04 - val_loss: 9.5014e-04 - val_mean_squared_error: 9.5014e-04\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.00095 to 0.00095, saving model to weights.best_mlp.hdf5\n",
            "Epoch 89/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0413e-04 - mean_squared_error: 4.0413e-04 - val_loss: 9.2742e-04 - val_mean_squared_error: 9.2742e-04\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.00095 to 0.00093, saving model to weights.best_mlp.hdf5\n",
            "Epoch 90/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9091e-04 - mean_squared_error: 3.9091e-04 - val_loss: 9.1770e-04 - val_mean_squared_error: 9.1770e-04\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.00093 to 0.00092, saving model to weights.best_mlp.hdf5\n",
            "Epoch 91/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0795e-04 - mean_squared_error: 4.0795e-04 - val_loss: 9.2384e-04 - val_mean_squared_error: 9.2384e-04\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.00092\n",
            "Epoch 92/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9305e-04 - mean_squared_error: 3.9305e-04 - val_loss: 9.2457e-04 - val_mean_squared_error: 9.2457e-04\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.00092\n",
            "Epoch 00092: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 50\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 4\n",
            "MLP: Train: 0.008459, Validation: 0.000925\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngA7z/0Ck3Iuj5Ni9AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 1s 7ms/step - loss: 8.2681 - mean_squared_error: 8.2681 - val_loss: 3.4897 - val_mean_squared_error: 3.4897\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.48966, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.7726 - mean_squared_error: 0.7726 - val_loss: 0.0310 - val_mean_squared_error: 0.0310\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.48966 to 0.03101, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0056 - mean_squared_error: 0.0056 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.03101 to 0.00301, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00301 to 0.00218, saving model to weights.best_mlp.hdf5\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00218 to 0.00180, saving model to weights.best_mlp.hdf5\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00180 to 0.00161, saving model to weights.best_mlp.hdf5\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00161 to 0.00153, saving model to weights.best_mlp.hdf5\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00153 to 0.00152, saving model to weights.best_mlp.hdf5\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00152\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00152\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00152\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00152\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00152\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00152\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00152\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00152\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00152\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00152\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00152\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00152\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00152\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00152\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00152\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.9380e-04 - mean_squared_error: 9.9380e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00152\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.6752e-04 - mean_squared_error: 9.6752e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00152\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.3590e-04 - mean_squared_error: 9.3590e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00152\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.0194e-04 - mean_squared_error: 9.0194e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00152\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.6753e-04 - mean_squared_error: 8.6753e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00152\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.3498e-04 - mean_squared_error: 8.3498e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00152\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.0690e-04 - mean_squared_error: 8.0690e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00152\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.8400e-04 - mean_squared_error: 7.8400e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00152\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.6441e-04 - mean_squared_error: 7.6441e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00152\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.4924e-04 - mean_squared_error: 7.4924e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00152\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.3628e-04 - mean_squared_error: 7.3628e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00152\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.2988e-04 - mean_squared_error: 7.2988e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00152\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.1605e-04 - mean_squared_error: 7.1605e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00152\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.0686e-04 - mean_squared_error: 7.0686e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00152\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.8619e-04 - mean_squared_error: 6.8619e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00152\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.8207e-04 - mean_squared_error: 6.8207e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00152\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.5871e-04 - mean_squared_error: 6.5871e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00152\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.3906e-04 - mean_squared_error: 6.3906e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00152\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.2260e-04 - mean_squared_error: 6.2260e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00152\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.1130e-04 - mean_squared_error: 6.1130e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00152\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.9307e-04 - mean_squared_error: 5.9307e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.00152 to 0.00149, saving model to weights.best_mlp.hdf5\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.7862e-04 - mean_squared_error: 5.7862e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.00149 to 0.00139, saving model to weights.best_mlp.hdf5\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.5434e-04 - mean_squared_error: 5.5434e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00139 to 0.00130, saving model to weights.best_mlp.hdf5\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.4061e-04 - mean_squared_error: 5.4061e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.00130 to 0.00123, saving model to weights.best_mlp.hdf5\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.1812e-04 - mean_squared_error: 5.1812e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.00123 to 0.00116, saving model to weights.best_mlp.hdf5\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.0997e-04 - mean_squared_error: 5.0997e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.00116 to 0.00110, saving model to weights.best_mlp.hdf5\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.9465e-04 - mean_squared_error: 4.9465e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.00110 to 0.00105, saving model to weights.best_mlp.hdf5\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.9944e-04 - mean_squared_error: 4.9944e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.00105 to 0.00101, saving model to weights.best_mlp.hdf5\n",
            "Epoch 52/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.8182e-04 - mean_squared_error: 4.8182e-04 - val_loss: 9.6640e-04 - val_mean_squared_error: 9.6640e-04\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.00101 to 0.00097, saving model to weights.best_mlp.hdf5\n",
            "Epoch 53/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.7111e-04 - mean_squared_error: 4.7111e-04 - val_loss: 9.2919e-04 - val_mean_squared_error: 9.2919e-04\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.00097 to 0.00093, saving model to weights.best_mlp.hdf5\n",
            "Epoch 54/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.7899e-04 - mean_squared_error: 4.7899e-04 - val_loss: 9.0639e-04 - val_mean_squared_error: 9.0639e-04\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.00093 to 0.00091, saving model to weights.best_mlp.hdf5\n",
            "Epoch 55/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.6704e-04 - mean_squared_error: 4.6704e-04 - val_loss: 8.7580e-04 - val_mean_squared_error: 8.7580e-04\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.00091 to 0.00088, saving model to weights.best_mlp.hdf5\n",
            "Epoch 56/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.6530e-04 - mean_squared_error: 4.6530e-04 - val_loss: 8.5830e-04 - val_mean_squared_error: 8.5830e-04\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.00088 to 0.00086, saving model to weights.best_mlp.hdf5\n",
            "Epoch 57/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5508e-04 - mean_squared_error: 4.5508e-04 - val_loss: 8.3484e-04 - val_mean_squared_error: 8.3484e-04\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.00086 to 0.00083, saving model to weights.best_mlp.hdf5\n",
            "Epoch 58/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.6101e-04 - mean_squared_error: 4.6101e-04 - val_loss: 8.1897e-04 - val_mean_squared_error: 8.1897e-04\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.00083 to 0.00082, saving model to weights.best_mlp.hdf5\n",
            "Epoch 59/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5669e-04 - mean_squared_error: 4.5669e-04 - val_loss: 8.0147e-04 - val_mean_squared_error: 8.0147e-04\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.00082 to 0.00080, saving model to weights.best_mlp.hdf5\n",
            "Epoch 00059: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 50\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 2\n",
            "MLP: Train: 0.008848, Validation: 0.000801\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 1s 7ms/step - loss: 7.7234 - mean_squared_error: 7.7234 - val_loss: 2.3783 - val_mean_squared_error: 2.3783\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.37826, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.3769 - mean_squared_error: 0.3769 - val_loss: 0.0060 - val_mean_squared_error: 0.0060\n",
            "\n",
            "Epoch 00002: val_loss improved from 2.37826 to 0.00598, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0050 - mean_squared_error: 0.0050 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00598 to 0.00268, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00268 to 0.00217, saving model to weights.best_mlp.hdf5\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00217 to 0.00194, saving model to weights.best_mlp.hdf5\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00194 to 0.00183, saving model to weights.best_mlp.hdf5\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00183 to 0.00180, saving model to weights.best_mlp.hdf5\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00180\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00180\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00180\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00180\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00180\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00180\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00180\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00180\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.9926e-04 - mean_squared_error: 9.9926e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.00180 to 0.00178, saving model to weights.best_mlp.hdf5\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.6510e-04 - mean_squared_error: 9.6510e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.00178 to 0.00177, saving model to weights.best_mlp.hdf5\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.4027e-04 - mean_squared_error: 9.4027e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00177\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.2175e-04 - mean_squared_error: 9.2175e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00177\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.0608e-04 - mean_squared_error: 9.0608e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00177\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.8867e-04 - mean_squared_error: 8.8867e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00177\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.6970e-04 - mean_squared_error: 8.6970e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00177\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.4420e-04 - mean_squared_error: 8.4420e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00177\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.1745e-04 - mean_squared_error: 8.1745e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00177\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.9282e-04 - mean_squared_error: 7.9282e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00177\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.6811e-04 - mean_squared_error: 7.6811e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00177\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.4853e-04 - mean_squared_error: 7.4853e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00177\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.3311e-04 - mean_squared_error: 7.3311e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00177\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.2273e-04 - mean_squared_error: 7.2273e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00177\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.1393e-04 - mean_squared_error: 7.1393e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00177\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.1094e-04 - mean_squared_error: 7.1094e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00177\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.0258e-04 - mean_squared_error: 7.0258e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00177\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.9258e-04 - mean_squared_error: 6.9258e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00177\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.7803e-04 - mean_squared_error: 6.7803e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00177\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.5968e-04 - mean_squared_error: 6.5968e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00177\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.3696e-04 - mean_squared_error: 6.3696e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00177\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.1255e-04 - mean_squared_error: 6.1255e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00177\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.9587e-04 - mean_squared_error: 5.9587e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.00177 to 0.00171, saving model to weights.best_mlp.hdf5\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.7407e-04 - mean_squared_error: 5.7407e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.00171 to 0.00155, saving model to weights.best_mlp.hdf5\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.4200e-04 - mean_squared_error: 5.4200e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.00155 to 0.00142, saving model to weights.best_mlp.hdf5\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.1801e-04 - mean_squared_error: 5.1801e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.00142 to 0.00129, saving model to weights.best_mlp.hdf5\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.9093e-04 - mean_squared_error: 4.9093e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.00129 to 0.00122, saving model to weights.best_mlp.hdf5\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.8017e-04 - mean_squared_error: 4.8017e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.00122 to 0.00109, saving model to weights.best_mlp.hdf5\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.6442e-04 - mean_squared_error: 4.6442e-04 - val_loss: 9.9548e-04 - val_mean_squared_error: 9.9548e-04\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.00109 to 0.00100, saving model to weights.best_mlp.hdf5\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5510e-04 - mean_squared_error: 4.5510e-04 - val_loss: 9.0006e-04 - val_mean_squared_error: 9.0006e-04\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.00100 to 0.00090, saving model to weights.best_mlp.hdf5\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4245e-04 - mean_squared_error: 4.4245e-04 - val_loss: 8.3380e-04 - val_mean_squared_error: 8.3380e-04\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00090 to 0.00083, saving model to weights.best_mlp.hdf5\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3835e-04 - mean_squared_error: 4.3835e-04 - val_loss: 7.6806e-04 - val_mean_squared_error: 7.6806e-04\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.00083 to 0.00077, saving model to weights.best_mlp.hdf5\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3500e-04 - mean_squared_error: 4.3500e-04 - val_loss: 6.9880e-04 - val_mean_squared_error: 6.9880e-04\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.00077 to 0.00070, saving model to weights.best_mlp.hdf5\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2869e-04 - mean_squared_error: 4.2869e-04 - val_loss: 6.7290e-04 - val_mean_squared_error: 6.7290e-04\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.00070 to 0.00067, saving model to weights.best_mlp.hdf5\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3585e-04 - mean_squared_error: 4.3585e-04 - val_loss: 6.3554e-04 - val_mean_squared_error: 6.3554e-04\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.00067 to 0.00064, saving model to weights.best_mlp.hdf5\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3541e-04 - mean_squared_error: 4.3541e-04 - val_loss: 6.0465e-04 - val_mean_squared_error: 6.0465e-04\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.00064 to 0.00060, saving model to weights.best_mlp.hdf5\n",
            "Epoch 52/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3706e-04 - mean_squared_error: 4.3706e-04 - val_loss: 5.6866e-04 - val_mean_squared_error: 5.6866e-04\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.00060 to 0.00057, saving model to weights.best_mlp.hdf5\n",
            "Epoch 00052: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 50\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 3\n",
            "MLP: Train: 0.009686, Validation: 0.000569\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxxjckjTHGNySNMf8f/Za04Jpwg3XAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 1s 7ms/step - loss: 8.2276 - mean_squared_error: 8.2276 - val_loss: 3.2870 - val_mean_squared_error: 3.2870\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.28699, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.6659 - mean_squared_error: 0.6659 - val_loss: 0.0174 - val_mean_squared_error: 0.0174\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.28699 to 0.01737, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0059 - mean_squared_error: 0.0059 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.01737 to 0.00312, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00312 to 0.00214, saving model to weights.best_mlp.hdf5\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00214 to 0.00171, saving model to weights.best_mlp.hdf5\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00171 to 0.00149, saving model to weights.best_mlp.hdf5\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00149 to 0.00140, saving model to weights.best_mlp.hdf5\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00140 to 0.00140, saving model to weights.best_mlp.hdf5\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00140\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00140\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00140\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00140\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00140\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00140\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00140\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00140\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00140\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00140\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.8989e-04 - mean_squared_error: 9.8989e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00140\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.7390e-04 - mean_squared_error: 9.7390e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00140\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.6102e-04 - mean_squared_error: 9.6102e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00140\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.4815e-04 - mean_squared_error: 9.4815e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00140\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.3223e-04 - mean_squared_error: 9.3223e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00140\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.1191e-04 - mean_squared_error: 9.1191e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00140\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.8801e-04 - mean_squared_error: 8.8801e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00140\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.6162e-04 - mean_squared_error: 8.6162e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00140\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.3461e-04 - mean_squared_error: 8.3461e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00140\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.0861e-04 - mean_squared_error: 8.0861e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00140\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.8625e-04 - mean_squared_error: 7.8625e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00140\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.6843e-04 - mean_squared_error: 7.6843e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00140\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.5405e-04 - mean_squared_error: 7.5405e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00140\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.4294e-04 - mean_squared_error: 7.4294e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00140\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.3590e-04 - mean_squared_error: 7.3590e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00140\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.2487e-04 - mean_squared_error: 7.2487e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00140\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.1365e-04 - mean_squared_error: 7.1365e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00140\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.0164e-04 - mean_squared_error: 7.0164e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00140\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.8732e-04 - mean_squared_error: 6.8732e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00140\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.6467e-04 - mean_squared_error: 6.6467e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00140\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.5061e-04 - mean_squared_error: 6.5061e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00140\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.2905e-04 - mean_squared_error: 6.2905e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00140\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.1247e-04 - mean_squared_error: 6.1247e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00140\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.8659e-04 - mean_squared_error: 5.8659e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00140\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.7314e-04 - mean_squared_error: 5.7314e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00140\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.6261e-04 - mean_squared_error: 5.6261e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00140\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.3526e-04 - mean_squared_error: 5.3526e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.00140 to 0.00129, saving model to weights.best_mlp.hdf5\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.1960e-04 - mean_squared_error: 5.1960e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00129 to 0.00118, saving model to weights.best_mlp.hdf5\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.9103e-04 - mean_squared_error: 4.9103e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.00118 to 0.00111, saving model to weights.best_mlp.hdf5\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.7767e-04 - mean_squared_error: 4.7767e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.00111 to 0.00102, saving model to weights.best_mlp.hdf5\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5685e-04 - mean_squared_error: 4.5685e-04 - val_loss: 9.8108e-04 - val_mean_squared_error: 9.8108e-04\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.00102 to 0.00098, saving model to weights.best_mlp.hdf5\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.6310e-04 - mean_squared_error: 4.6310e-04 - val_loss: 9.0916e-04 - val_mean_squared_error: 9.0916e-04\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.00098 to 0.00091, saving model to weights.best_mlp.hdf5\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3487e-04 - mean_squared_error: 4.3487e-04 - val_loss: 8.6464e-04 - val_mean_squared_error: 8.6464e-04\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.00091 to 0.00086, saving model to weights.best_mlp.hdf5\n",
            "Epoch 52/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1618e-04 - mean_squared_error: 4.1618e-04 - val_loss: 8.1383e-04 - val_mean_squared_error: 8.1383e-04\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.00086 to 0.00081, saving model to weights.best_mlp.hdf5\n",
            "Epoch 53/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0486e-04 - mean_squared_error: 4.0486e-04 - val_loss: 7.6619e-04 - val_mean_squared_error: 7.6619e-04\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.00081 to 0.00077, saving model to weights.best_mlp.hdf5\n",
            "Epoch 54/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0909e-04 - mean_squared_error: 4.0909e-04 - val_loss: 7.2196e-04 - val_mean_squared_error: 7.2196e-04\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.00077 to 0.00072, saving model to weights.best_mlp.hdf5\n",
            "Epoch 55/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0348e-04 - mean_squared_error: 4.0348e-04 - val_loss: 7.0923e-04 - val_mean_squared_error: 7.0923e-04\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.00072 to 0.00071, saving model to weights.best_mlp.hdf5\n",
            "Epoch 56/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8978e-04 - mean_squared_error: 3.8978e-04 - val_loss: 6.9296e-04 - val_mean_squared_error: 6.9296e-04\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.00071 to 0.00069, saving model to weights.best_mlp.hdf5\n",
            "Epoch 57/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8210e-04 - mean_squared_error: 3.8210e-04 - val_loss: 6.5211e-04 - val_mean_squared_error: 6.5211e-04\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.00069 to 0.00065, saving model to weights.best_mlp.hdf5\n",
            "Epoch 58/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9702e-04 - mean_squared_error: 3.9702e-04 - val_loss: 6.2504e-04 - val_mean_squared_error: 6.2504e-04\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.00065 to 0.00063, saving model to weights.best_mlp.hdf5\n",
            "Epoch 59/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.7701e-04 - mean_squared_error: 3.7701e-04 - val_loss: 6.0237e-04 - val_mean_squared_error: 6.0237e-04\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.00063 to 0.00060, saving model to weights.best_mlp.hdf5\n",
            "Epoch 60/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1768e-04 - mean_squared_error: 4.1768e-04 - val_loss: 6.0071e-04 - val_mean_squared_error: 6.0071e-04\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.00060 to 0.00060, saving model to weights.best_mlp.hdf5\n",
            "Epoch 61/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.7184e-04 - mean_squared_error: 3.7184e-04 - val_loss: 5.8973e-04 - val_mean_squared_error: 5.8973e-04\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.00060 to 0.00059, saving model to weights.best_mlp.hdf5\n",
            "Epoch 62/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8873e-04 - mean_squared_error: 3.8873e-04 - val_loss: 5.8090e-04 - val_mean_squared_error: 5.8090e-04\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.00059 to 0.00058, saving model to weights.best_mlp.hdf5\n",
            "Epoch 63/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.7776e-04 - mean_squared_error: 3.7776e-04 - val_loss: 5.6100e-04 - val_mean_squared_error: 5.6100e-04\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.00058 to 0.00056, saving model to weights.best_mlp.hdf5\n",
            "Epoch 64/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.6758e-04 - mean_squared_error: 3.6758e-04 - val_loss: 5.6263e-04 - val_mean_squared_error: 5.6263e-04\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.00056\n",
            "Epoch 65/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.7349e-04 - mean_squared_error: 3.7349e-04 - val_loss: 5.5127e-04 - val_mean_squared_error: 5.5127e-04\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.00056 to 0.00055, saving model to weights.best_mlp.hdf5\n",
            "Epoch 66/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.7213e-04 - mean_squared_error: 3.7213e-04 - val_loss: 5.5144e-04 - val_mean_squared_error: 5.5144e-04\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.00055\n",
            "Epoch 67/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9143e-04 - mean_squared_error: 3.9143e-04 - val_loss: 5.5683e-04 - val_mean_squared_error: 5.5683e-04\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.00055\n",
            "Epoch 68/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.7677e-04 - mean_squared_error: 3.7677e-04 - val_loss: 5.6206e-04 - val_mean_squared_error: 5.6206e-04\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.00055\n",
            "Epoch 00068: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 50\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 4\n",
            "MLP: Train: 0.009733, Validation: 0.000562\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAD8CAYAAAC4uSVNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 1s 7ms/step - loss: 6.9439 - mean_squared_error: 6.9439 - val_loss: 0.9388 - val_mean_squared_error: 0.9388\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.93877, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0844 - mean_squared_error: 0.0844 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.93877 to 0.00251, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00251 to 0.00164, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00164 to 0.00146, saving model to weights.best_mlp.hdf5\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00146\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00146\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00146\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00146\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00146\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00146\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00146\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00146\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00146\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.9982e-04 - mean_squared_error: 9.9982e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00146\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.8638e-04 - mean_squared_error: 9.8638e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00146\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.7264e-04 - mean_squared_error: 9.7264e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00146\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.5466e-04 - mean_squared_error: 9.5466e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00146\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.3064e-04 - mean_squared_error: 9.3064e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00146\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.0112e-04 - mean_squared_error: 9.0112e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00146\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.6816e-04 - mean_squared_error: 8.6816e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00146\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.3625e-04 - mean_squared_error: 8.3625e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00146\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.0988e-04 - mean_squared_error: 8.0988e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00146\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.8753e-04 - mean_squared_error: 7.8753e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00146\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.7471e-04 - mean_squared_error: 7.7471e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00146\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.6499e-04 - mean_squared_error: 7.6499e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00146\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.5869e-04 - mean_squared_error: 7.5869e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00146\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.5023e-04 - mean_squared_error: 7.5023e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00146\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.3939e-04 - mean_squared_error: 7.3939e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00146\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.2277e-04 - mean_squared_error: 7.2277e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00146\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.0549e-04 - mean_squared_error: 7.0549e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00146\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.7858e-04 - mean_squared_error: 6.7858e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00146\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.5342e-04 - mean_squared_error: 6.5342e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00146\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.2602e-04 - mean_squared_error: 6.2602e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00146 to 0.00134, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.9719e-04 - mean_squared_error: 5.9719e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00134 to 0.00117, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.7501e-04 - mean_squared_error: 5.7501e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00117 to 0.00102, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.4698e-04 - mean_squared_error: 5.4698e-04 - val_loss: 8.7889e-04 - val_mean_squared_error: 8.7889e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00102 to 0.00088, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.2280e-04 - mean_squared_error: 5.2280e-04 - val_loss: 7.7594e-04 - val_mean_squared_error: 7.7594e-04\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.00088 to 0.00078, saving model to weights.best_mlp.hdf5\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.0706e-04 - mean_squared_error: 5.0706e-04 - val_loss: 6.8569e-04 - val_mean_squared_error: 6.8569e-04\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.00078 to 0.00069, saving model to weights.best_mlp.hdf5\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.8333e-04 - mean_squared_error: 4.8333e-04 - val_loss: 5.9819e-04 - val_mean_squared_error: 5.9819e-04\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.00069 to 0.00060, saving model to weights.best_mlp.hdf5\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.7911e-04 - mean_squared_error: 4.7911e-04 - val_loss: 5.3611e-04 - val_mean_squared_error: 5.3611e-04\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.00060 to 0.00054, saving model to weights.best_mlp.hdf5\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.7047e-04 - mean_squared_error: 4.7047e-04 - val_loss: 4.8880e-04 - val_mean_squared_error: 4.8880e-04\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.00054 to 0.00049, saving model to weights.best_mlp.hdf5\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.6018e-04 - mean_squared_error: 4.6018e-04 - val_loss: 4.4075e-04 - val_mean_squared_error: 4.4075e-04\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.00049 to 0.00044, saving model to weights.best_mlp.hdf5\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5769e-04 - mean_squared_error: 4.5769e-04 - val_loss: 4.0812e-04 - val_mean_squared_error: 4.0812e-04\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.00044 to 0.00041, saving model to weights.best_mlp.hdf5\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5304e-04 - mean_squared_error: 4.5304e-04 - val_loss: 3.7990e-04 - val_mean_squared_error: 3.7990e-04\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.00041 to 0.00038, saving model to weights.best_mlp.hdf5\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4678e-04 - mean_squared_error: 4.4678e-04 - val_loss: 3.5960e-04 - val_mean_squared_error: 3.5960e-04\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.00038 to 0.00036, saving model to weights.best_mlp.hdf5\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4890e-04 - mean_squared_error: 4.4890e-04 - val_loss: 3.3922e-04 - val_mean_squared_error: 3.3922e-04\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00036 to 0.00034, saving model to weights.best_mlp.hdf5\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4706e-04 - mean_squared_error: 4.4706e-04 - val_loss: 3.2022e-04 - val_mean_squared_error: 3.2022e-04\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.00034 to 0.00032, saving model to weights.best_mlp.hdf5\n",
            "Epoch 00047: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 100\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 2\n",
            "MLP: Train: 0.010762, Validation: 0.000320\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 1s 8ms/step - loss: 6.7453 - mean_squared_error: 6.7453 - val_loss: 0.8351 - val_mean_squared_error: 0.8351\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.83514, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0778 - mean_squared_error: 0.0778 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.83514 to 0.00255, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00255 to 0.00169, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00169 to 0.00160, saving model to weights.best_mlp.hdf5\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00160\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00160\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00160\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00160\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00160\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00160\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00160\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00160\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.8396e-04 - mean_squared_error: 9.8396e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00160\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.6506e-04 - mean_squared_error: 9.6506e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00160\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.5108e-04 - mean_squared_error: 9.5108e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00160\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.3698e-04 - mean_squared_error: 9.3698e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00160\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.1933e-04 - mean_squared_error: 9.1933e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00160\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.9614e-04 - mean_squared_error: 8.9614e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00160\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.6804e-04 - mean_squared_error: 8.6804e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00160\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.3760e-04 - mean_squared_error: 8.3760e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00160\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.0855e-04 - mean_squared_error: 8.0855e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00160\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.8394e-04 - mean_squared_error: 7.8394e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00160\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.6669e-04 - mean_squared_error: 7.6669e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00160\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.5256e-04 - mean_squared_error: 7.5256e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00160\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.4465e-04 - mean_squared_error: 7.4465e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00160\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.3651e-04 - mean_squared_error: 7.3651e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00160\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.2652e-04 - mean_squared_error: 7.2652e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00160\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.1233e-04 - mean_squared_error: 7.1233e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00160\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.9491e-04 - mean_squared_error: 6.9491e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00160\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.7262e-04 - mean_squared_error: 6.7262e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00160\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.4747e-04 - mean_squared_error: 6.4747e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00160\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.1944e-04 - mean_squared_error: 6.1944e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.00160 to 0.00157, saving model to weights.best_mlp.hdf5\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.8572e-04 - mean_squared_error: 5.8572e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00157 to 0.00140, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.5743e-04 - mean_squared_error: 5.5743e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00140 to 0.00124, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.2728e-04 - mean_squared_error: 5.2728e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00124 to 0.00109, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.9969e-04 - mean_squared_error: 4.9969e-04 - val_loss: 9.6869e-04 - val_mean_squared_error: 9.6869e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00109 to 0.00097, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.7930e-04 - mean_squared_error: 4.7930e-04 - val_loss: 8.4488e-04 - val_mean_squared_error: 8.4488e-04\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.00097 to 0.00084, saving model to weights.best_mlp.hdf5\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5999e-04 - mean_squared_error: 4.5999e-04 - val_loss: 7.4759e-04 - val_mean_squared_error: 7.4759e-04\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.00084 to 0.00075, saving model to weights.best_mlp.hdf5\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5118e-04 - mean_squared_error: 4.5118e-04 - val_loss: 6.6187e-04 - val_mean_squared_error: 6.6187e-04\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.00075 to 0.00066, saving model to weights.best_mlp.hdf5\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4295e-04 - mean_squared_error: 4.4295e-04 - val_loss: 5.9282e-04 - val_mean_squared_error: 5.9282e-04\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.00066 to 0.00059, saving model to weights.best_mlp.hdf5\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3826e-04 - mean_squared_error: 4.3826e-04 - val_loss: 5.3910e-04 - val_mean_squared_error: 5.3910e-04\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.00059 to 0.00054, saving model to weights.best_mlp.hdf5\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3439e-04 - mean_squared_error: 4.3439e-04 - val_loss: 4.9499e-04 - val_mean_squared_error: 4.9499e-04\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.00054 to 0.00049, saving model to weights.best_mlp.hdf5\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3074e-04 - mean_squared_error: 4.3074e-04 - val_loss: 4.5975e-04 - val_mean_squared_error: 4.5975e-04\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.00049 to 0.00046, saving model to weights.best_mlp.hdf5\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3026e-04 - mean_squared_error: 4.3026e-04 - val_loss: 4.2685e-04 - val_mean_squared_error: 4.2685e-04\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.00046 to 0.00043, saving model to weights.best_mlp.hdf5\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2865e-04 - mean_squared_error: 4.2865e-04 - val_loss: 3.9734e-04 - val_mean_squared_error: 3.9734e-04\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.00043 to 0.00040, saving model to weights.best_mlp.hdf5\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2757e-04 - mean_squared_error: 4.2757e-04 - val_loss: 3.7400e-04 - val_mean_squared_error: 3.7400e-04\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00040 to 0.00037, saving model to weights.best_mlp.hdf5\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2392e-04 - mean_squared_error: 4.2392e-04 - val_loss: 3.4986e-04 - val_mean_squared_error: 3.4986e-04\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.00037 to 0.00035, saving model to weights.best_mlp.hdf5\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2251e-04 - mean_squared_error: 4.2251e-04 - val_loss: 3.2800e-04 - val_mean_squared_error: 3.2800e-04\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.00035 to 0.00033, saving model to weights.best_mlp.hdf5\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2182e-04 - mean_squared_error: 4.2182e-04 - val_loss: 3.1422e-04 - val_mean_squared_error: 3.1422e-04\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.00033 to 0.00031, saving model to weights.best_mlp.hdf5\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2195e-04 - mean_squared_error: 4.2195e-04 - val_loss: 2.9807e-04 - val_mean_squared_error: 2.9807e-04\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.00031 to 0.00030, saving model to weights.best_mlp.hdf5\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1390e-04 - mean_squared_error: 4.1390e-04 - val_loss: 2.8684e-04 - val_mean_squared_error: 2.8684e-04\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.00030 to 0.00029, saving model to weights.best_mlp.hdf5\n",
            "Epoch 52/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1615e-04 - mean_squared_error: 4.1615e-04 - val_loss: 2.7991e-04 - val_mean_squared_error: 2.7991e-04\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.00029 to 0.00028, saving model to weights.best_mlp.hdf5\n",
            "Epoch 53/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1448e-04 - mean_squared_error: 4.1448e-04 - val_loss: 2.7121e-04 - val_mean_squared_error: 2.7121e-04\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.00028 to 0.00027, saving model to weights.best_mlp.hdf5\n",
            "Epoch 54/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9976e-04 - mean_squared_error: 3.9976e-04 - val_loss: 2.6659e-04 - val_mean_squared_error: 2.6659e-04\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.00027 to 0.00027, saving model to weights.best_mlp.hdf5\n",
            "Epoch 55/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2432e-04 - mean_squared_error: 4.2432e-04 - val_loss: 2.6074e-04 - val_mean_squared_error: 2.6074e-04\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.00027 to 0.00026, saving model to weights.best_mlp.hdf5\n",
            "Epoch 56/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8485e-04 - mean_squared_error: 3.8485e-04 - val_loss: 2.6395e-04 - val_mean_squared_error: 2.6395e-04\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.00026\n",
            "Epoch 57/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.1140e-04 - mean_squared_error: 5.1140e-04 - val_loss: 2.6230e-04 - val_mean_squared_error: 2.6230e-04\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.00026\n",
            "Epoch 58/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0222e-04 - mean_squared_error: 4.0222e-04 - val_loss: 2.6390e-04 - val_mean_squared_error: 2.6390e-04\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.00026\n",
            "Epoch 59/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.7115e-04 - mean_squared_error: 4.7115e-04 - val_loss: 2.6372e-04 - val_mean_squared_error: 2.6372e-04\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.00026\n",
            "Epoch 00059: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 100\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 3\n",
            "MLP: Train: 0.011638, Validation: 0.000264\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAFbtJREFUeJzt3XuMnNV5x/HfM7Ozu76BzXqxza7T\ndZXIxECxYXEcgSICSWpDCkgJMRFUaRvJ/YM2JkoUGUVqiJQ/6B9NE6QmyCHkohpTYkKhCEKA2NA0\nxukanLC+BAdi5DWxd+zaxia+7OXpH/POenbnncteZue86+9HWnnu8xw8/Pb4ec+8x9xdAIDkSNW7\nAADA6BDcAJAwBDcAJAzBDQAJQ3ADQMIQ3ACQMAQ3ACQMwQ0ACVMxuM1ssZntKPh518zumYziAADF\nbDTfnDSztKQDkj7k7m+XetzcuXO9o6Nj/NUBwHli+/bth929tZrHNozytW+U9Ga50Jakjo4OdXV1\njfKlAeD8ZWZlc7XQaHvcd0jaOMrnAAAmUNXBbWaNkm6R9JMS968xsy4z68pmsxNVHwBghNHMuFdJ\netXdD8Xd6e7r3b3T3TtbW6tq0wAAxmA0Pe7PijYJgBrp6+tTT0+PTp8+Xe9Saqq5uVnt7e3KZDJj\nfo2qgtvMZkj6uKS/H/M7AUAZPT09mjVrljo6OmRm9S6nJtxdR44cUU9PjxYtWjTm16mqVeLu77l7\ni7sfH/M7AUAZp0+fVktLy5QNbUkyM7W0tIz7XxV8cxJAMKZyaOdNxBiDCu4HXtyrl95gRQoAlBNU\ncD/40pv6b4IbQB0cO3ZM3/nOd0b9vJtuuknHjh2rQUWlBRXcjQ0pnR0YrHcZAM5DpYK7v7+/7POe\neeYZzZ49u1ZlxRrtV95rqjGd0tl+ghvA5Fu3bp3efPNNLV26VJlMRs3NzZozZ4727NmjN954Q7fd\ndpv279+v06dPa+3atVqzZo2kc6f4OHnypFatWqXrrrtOv/rVr9TW1qYnn3xS06ZNm/BagwruDMEN\nQNLX/2undr3z7oS+5pJLLtDX/uqykvfff//96u7u1o4dO7RlyxbdfPPN6u7uHlq29/DDD+uiiy7S\nqVOndM011+hTn/qUWlpahr3G3r17tXHjRn3ve9/TZz7zGT3++OO66667JnQcUmDB3USrBEAgli9f\nPmyt9QMPPKAnnnhCkrR//37t3bu3KLgXLVqkpUuXSpKuvvpq7du3rya1BRXcjQ3MuAGo7Mx4ssyY\nMWPo8pYtW/TCCy9o69atmj59uq6//vrYtdhNTU1Dl9PptE6dOlWT2jg4CQCSZs2apRMnTsTed/z4\ncc2ZM0fTp0/Xnj179Morr0xydcOFNeOmxw2gTlpaWnTttdfq8ssv17Rp0zRv3ryh+1auXKkHH3xQ\nH/zgB7V48WKtWLGijpWGFty0SgDU0SOPPBJ7e1NTk5599tnY+/J97Llz56q7u3vo9i9/+csTXl8e\nrRIASJiwgptWCQBUFFRwZ2iVAEBFQQV3U5pWCQBUElRwc3ASACoLL7iZcQNAWWEFNwcnASTEzJkz\n6/beYQU3rRIAqCi4L+D0D7oGB12p1NTfwghAONatW6eFCxfq7rvvliTdd999amho0ObNm3X06FH1\n9fXpG9/4hm699dY6V1r9Lu+zJT0k6XJJLunv3H3rRBfT2JD7B8DZgUE1p9IT/fIAkuLZddLB1yf2\nNedfIa26v+Tdq1ev1j333DMU3I899piee+45feELX9AFF1ygw4cPa8WKFbrlllvqvjdmtTPub0v6\nmbt/2swaJU2vRTGN6Vxwn+kfVHOG4AYweZYtW6be3l698847ymazmjNnjubPn68vfvGLevnll5VK\npXTgwAEdOnRI8+fPr2utFYPbzC6U9BFJfyNJ7n5W0tlaFJOfcfexsgQ4v5WZGdfS7bffrk2bNung\nwYNavXq1NmzYoGw2q+3btyuTyaijoyP2dK6TrZqDk4skZSX9wMxeM7OHzGxGpSeNRX7GzQFKAPWw\nevVqPfroo9q0aZNuv/12HT9+XBdffLEymYw2b96st99+u94lSqouuBskXSXpu+6+TNJ7ktaNfJCZ\nrTGzLjPrymbHtlP7UI+b4AZQB5dddplOnDihtrY2LViwQHfeeae6urp0xRVX6Mc//rEuvfTSepco\nqboed4+kHnffFl3fpJjgdvf1ktZLUmdnp4+lmMKDkwBQD6+/fu6g6Ny5c7V1a/w6jJMnT05WSUUq\nzrjd/aCk/Wa2OLrpRkm7alEMrRIAqKzaVSX/KGlDtKLkLUl/W4ti8jPuMwQ3AJRUVXC7+w5JnTWu\nhR43cJ5z97qvka419zF1kocJ6ivvTfS4gfNWc3Ozjhw5MiHBFip315EjR9Tc3Dyu1wnqK++ZqMfd\nx4wbOO+0t7erp6dHY12VlhTNzc1qb28f12sEFdysKgHOX5lMRosWLap3GYkQVKuEVSUAUFlYwc3B\nSQCoKMjgPkOrBABKCiq4m9K5MwIy4waA0oIKblolAFAZwQ0ACRNUcKdTppRxPm4AKCeo4JaiDYMJ\nbgAoKbzgTrPTOwCUE15wN6Q5OyAAlBFccDc1MOMGgHKCC2563ABQXnjBnU7pbP9AvcsAgGCFF9y0\nSgCgrOCCO5M29Q1M3ROpA8B4BRfczLgBoLwAgzvN2QEBoIzwgpsv4ABAWVVtXWZm+ySdkDQgqd/d\na7bje24dN6tKAKCU0ew5+VF3P1yzSiKs4waA8miVAEDCVBvcLunnZrbdzNbEPcDM1phZl5l1ZbPZ\nMRfEqhIAKK/a4L7O3a+StErS3Wb2kZEPcPf17t7p7p2tra1jLiiTTrGOGwDKqCq43f1A9GevpCck\nLa9VQcy4AaC8isFtZjPMbFb+sqRPSOquVUH5g5PuzLoBIE41q0rmSXrCzPKPf8Tdf1argpry+04O\nDKqpIV2rtwGAxKoY3O7+lqQrJ6EWSblVJVJuw2CCGwCKhbcckJ3eAaCscIObL+EAQKzwgjvNjBsA\nygkuuDPRjLuPGTcAxAouuPMzbnZ6B4B4wQV3EwcnAaCs4IKbVSUAUF64wU2PGwBihRfcrCoBgLLC\nC25aJQBQVrjBTasEAGKFF9y0SgCgrPCCmxk3AJQVXnAz4waAssILbg5OAkBZBDcAJExwwd2QMpnR\n4waAUoILbjNTY5oNgwGglOCCW8q1Szg7IADECzK4mxpSnI8bAEqoOrjNLG1mr5nZ07UsSJIytEoA\noKTRzLjXStpdq0IKNTakODgJACVUFdxm1i7pZkkP1bacHA5OAkBp1c64vyXpK5ImJU0bGwhuACil\nYnCb2Scl9br79gqPW2NmXWbWlc1mx1UUrRIAKK2aGfe1km4xs32SHpV0g5n9+8gHuft6d+90987W\n1tZxFdWYZjkgAJRSMbjd/V53b3f3Dkl3SPqFu99Vy6JolQBAaazjBoCEaRjNg919i6QtNamkAOu4\nAaC0IGfcHJwEgNLCDG5m3ABQUpjBzcFJACiJ4AaAhAk2uM/Q4waAWEEGd1PU43b3epcCAMEJMrjz\n+072DxLcADBSkMGdSbNhMACUEmRws9M7AJQWdnBzgBIAioQZ3LRKAKCkMIM7mnFzalcAKBZkcDfR\n4waAkoIMbnrcAFBamMGdTksS5+QGgBhBBncmbZJolQBAnCCDm3XcAFBa0MHNqhIAKBZkcDdxcBIA\nSgoyuPMHJ2mVAECxisFtZs1m9msz+42Z7TSzr9e6KHrcAFBaNbu8n5F0g7ufNLOMpF+a2bPu/kqt\nijoX3AO1egsASKyKwe253QxORlcz0U9NT5SdD+6+Ac7HDQAjVdXjNrO0me2Q1CvpeXffVsuihtZx\nc3ASAIpUFdzuPuDuSyW1S1puZpePfIyZrTGzLjPrymaz4yoqf3ZAlgMCQLFRrSpx92OSNktaGXPf\nenfvdPfO1tbWcRVlZmpMs9M7AMSpZlVJq5nNji5Pk/RxSXtqXVhjA8ENAHGqWVWyQNKPzCytXNA/\n5u5P17asKLgHWFUCACNVs6rkt5KWTUItw9AqAYB4QX5zUqJVAgClBB3crOMGgGLBBncmnWI5IADE\nCDa4cwcnCW4AGCnY4G5KpzhXCQDECDa4OTgJAPHCDm5aJQBQJNzgZh03AMQKN7hplQBArKCDm3Xc\nAFAs6OBmHTcAFAs3uFkOCACxwg1uVpUAQKxwg5tVJQAQK9zgbkhp0KV+Zt0AMEzQwS2xYTAAjBRu\ncEcbBvf1syQQAAqFG9zRjPsM25cBwDDBBzcHKAFguHCDO01wA0CccIObg5MAEKticJvZQjPbbGa7\nzGynma2djMKYcQNAvIYqHtMv6Uvu/qqZzZK03cyed/ddtSyMHjcAxKs443b3P7r7q9HlE5J2S2qr\ndWEENwDEG1WP28w6JC2TtC3mvjVm1mVmXdlsdtyF0eMGgHhVB7eZzZT0uKR73P3dkfe7+3p373T3\nztbW1nEXRo8bAOJVFdxmllEutDe4+09rW1JOEzNuAIhVzaoSk/R9Sbvd/Zu1Lyknw4wbAGJVM+O+\nVtJfS7rBzHZEPzfVuC4OTgJACRWXA7r7LyXZJNQyDAcnASBe+N+cZMYNAMOEG9xRj5sNgwFguOCD\nu49WCQAME2xwp1KmTNpolQDACMEGt8SGwQAQJ+jgzjSkWFUCACMEHdzMuAGgWNjB3UBwA8BIwQf3\nGVolADBM2MFNqwQAigQd3E0NKdZxA8AIQQc3PW4AKEZwA0DCBB3cmTTruAFgpKCDm4OTAFAs7OCm\nVQIARYIPbk7rCgDDBR3cTZyrBACKBB3cjWnWcQPASGEHNz1uAChSMbjN7GEz6zWz7skoqBDBDQDF\nqplx/1DSyhrXESuTTql/0DU46PV4ewAIUsXgdveXJf3fJNRSZGind/rcADAk7B43O70DQJEJC24z\nW2NmXWbWlc1mJ+Q1m/IzboIbAIZMWHC7+3p373T3ztbW1gl5TVolAFAs7FZJFNx9zLgBYEg1ywE3\nStoqabGZ9ZjZ52tSyUC/9NZL0qFdQzc1ptOSmHEDQKFqVpV81t0XuHvG3dvd/fs1q2bjHdL2Hw5d\nbaTHDQBFwmmVpBukS66SDnQN3ZQPblaVAMA54QS3JLVfLR18Xeo/I0nKpE0SM24AKBRWcLd1SgNn\nc+GtguWA9LgBYEhYwd3emfuzJ9cuGTo4yYwbAIaEFdwXXCLNumSoz83BSQAoFlZwS7k+d8/w4Oac\n3ABwTnjB3dYpHf2D9N4RZtwAECO84M73uQ90nTvJFDNuABgSXnAvWCpZSurpYsYNADHCC+6mmdLF\nS4bNuAluADgnvOCWpLarpQPb1ZhbDUhwA0CBMIO7vVM6fVzpo28pnTKdHRiod0UAEIwwg7vt3Bdx\nGtNsGAwAhcIM7tbFUuPMXJ+7IaW+ATYLBoC8MIM7lZYuWTa0soSzAwLAOWEGtyS1XyMd6tbMVB+t\nEgAoEHBwd0qD/bo8tY+zAwJAgXCDOzpAeZnv1dl+VpUAQF64wT1rnnThQi0Z3EurBAAKhBvcktR2\ntS7t30OrBAAKVBXcZrbSzH5nZr83s3W1LmpIe6cuHuzVtDNHJu0tASB0FYPbzNKS/k3SKklLJH3W\nzJbUujBJQ33uzMFXdd9TO/Vm9uSkvC0AhKyaGfdySb9397fc/aykRyXdWtuyIguulFtat7Ue1IZt\nb+vGf3lJdz20Tc/tPKh+2icAzlMNVTymTdL+gus9kj5Um3JGaJwum3eZ/vLwJu2Z/bxO9bne6xlQ\n/2NSr0wyq/gSpR5h8vh7K79k3eVqr+a2Ql5wuxfdH/f84c+J/vTafIvVY/4uh7+TFdxuZe8f+bjC\nx3qZT0Sp55R6zXLvPd7nl/qvHPffKf69E/BBrpF6jv1P6Qu15Kv/U/P3qSa4q2JmayStkaT3ve99\nE/Wy0se+Ju16Smm5ZrprurveOfon9Z44pXyG5D/kJs9dHnF7nHx0Dc+h6kIphC/gF304vfL/1B4T\nfqoQVMPut+LnlH5e9WJ/aXjxL5pSz6n2l45K/NKxglivtqai1y7z/mVfs+Iv3PKPjVOLX67xtWOk\n/sZZk/I+1QT3AUkLC663R7cN4+7rJa2XpM7Ozon7W37/x3I/kVRUQPuEvQEAJEs1Pe7/lfQBM1tk\nZo2S7pD0VG3LAgCUUnHG7e79ZvYPkp6TlJb0sLvvrHllAIBYVfW43f0ZSc/UuBYAQBXC/uYkAKAI\nwQ0ACUNwA0DCENwAkDAENwAkjHktvmVllpX09hifPlfS4Qksp96m2nikqTemqTYeaeqNaaqNRyoe\n05+5e2s1T6xJcI+HmXW5e2e965goU2080tQb01QbjzT1xjTVxiONb0y0SgAgYQhuAEiYEIN7fb0L\nmGBTbTzS1BvTVBuPNPXGNNXGI41jTMH1uAEA5YU44wYAlBFMcNdtQ+IJZGYPm1mvmXUX3HaRmT1v\nZnujP+fUs8bRMLOFZrbZzHaZ2U4zWxvdnuQxNZvZr83sN9GYvh7dvsjMtkWfv/+ITmGcGGaWNrPX\nzOzp6HrSx7PPzF43sx1m1hXdluTP3Wwz22Rme8xst5l9eDzjCSK467oh8cT6oaSVI25bJ+lFd/+A\npBej60nRL+lL7r5E0gpJd0d/L0ke0xlJN7j7lZKWSlppZisk/bOkf3X390s6KunzdaxxLNZK2l1w\nPenjkaSPuvvSgiVzSf7cfVvSz9z9UklXKvd3NfbxuHvdfyR9WNJzBdfvlXRvvesa41g6JHUXXP+d\npAXR5QWSflfvGscxticlfXyqjEnSdEmvKreH6mFJDdHtwz6Pof8otyHUi5JukPS0crufJXY8Uc37\nJM0dcVsiP3eSLpT0B0XHFCdiPEHMuBW/IXFbnWqZaPPc/Y/R5YOS5tWzmLEysw5JyyRtU8LHFLUV\ndkjqlfS8pDclHXP3/ughSfv8fUvSVyQNRtdblOzxSLkNNn9uZtuj/Wyl5H7uFknKSvpB1M56yMxm\naBzjCSW4zwue+9WauGU8ZjZT0uOS7nH3dwvvS+KY3H3A3ZcqN1NdLunSOpc0Zmb2SUm97r693rVM\nsOvc/Srl2qd3m9lHCu9M2OeuQdJVkr7r7sskvacRbZHRjieU4K5qQ+KEOmRmCyQp+rO3zvWMipll\nlAvtDe7+0+jmRI8pz92PSdqsXCthtpnld4RK0ufvWkm3mNk+SY8q1y75tpI7HkmSux+I/uyV9IRy\nv2CT+rnrkdTj7tui65uUC/IxjyeU4J7KGxI/Jelz0eXPKdcnTgQzM0nfl7Tb3b9ZcFeSx9RqZrOj\ny9OU69nvVi7APx09LDFjcvd73b3d3TuU+//mF+5+pxI6HkkysxlmNit/WdInJHUroZ87dz8oab+Z\nLY5uulHSLo1nPPVu3Bc06m+S9IZy/cav1rueMY5ho6Q/SupT7rfs55XrN74oaa+kFyRdVO86RzGe\n65T759tvJe2Ifm5K+Jj+QtJr0Zi6Jf1TdPufS/q1pN9L+omkpnrXOoaxXS/p6aSPJ6r9N9HPznwe\nJPxzt1RSV/S5+09Jc8YzHr45CQAJE0qrBABQJYIbABKG4AaAhCG4ASBhCG4ASBiCGwAShuAGgIQh\nuAEgYf4fYDqweryB5yoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 1s 8ms/step - loss: 6.9636 - mean_squared_error: 6.9636 - val_loss: 0.8797 - val_mean_squared_error: 0.8797\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.87965, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0719 - mean_squared_error: 0.0719 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.87965 to 0.00204, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00204 to 0.00166, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00166 to 0.00161, saving model to weights.best_mlp.hdf5\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00161\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00161\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00161\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00161\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00161\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00161\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00161\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00161\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.9965e-04 - mean_squared_error: 9.9965e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00161\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.8309e-04 - mean_squared_error: 9.8309e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00161\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.7005e-04 - mean_squared_error: 9.7005e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00161\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.5548e-04 - mean_squared_error: 9.5548e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00161\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.3613e-04 - mean_squared_error: 9.3613e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00161\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.1084e-04 - mean_squared_error: 9.1084e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00161\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.8077e-04 - mean_squared_error: 8.8077e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00161\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.4883e-04 - mean_squared_error: 8.4883e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00161\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.1934e-04 - mean_squared_error: 8.1934e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00161\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.9458e-04 - mean_squared_error: 7.9458e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00161\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.7654e-04 - mean_squared_error: 7.7654e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00161\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.6415e-04 - mean_squared_error: 7.6415e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00161\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.5500e-04 - mean_squared_error: 7.5500e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00161\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.4602e-04 - mean_squared_error: 7.4602e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00161\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.3675e-04 - mean_squared_error: 7.3675e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00161\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.1994e-04 - mean_squared_error: 7.1994e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00161\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.9757e-04 - mean_squared_error: 6.9757e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00161\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.7345e-04 - mean_squared_error: 6.7345e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00161\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.4636e-04 - mean_squared_error: 6.4636e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00161\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.1806e-04 - mean_squared_error: 6.1806e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.00161 to 0.00144, saving model to weights.best_mlp.hdf5\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.8725e-04 - mean_squared_error: 5.8725e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00144 to 0.00127, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.5862e-04 - mean_squared_error: 5.5862e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00127 to 0.00111, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.2885e-04 - mean_squared_error: 5.2885e-04 - val_loss: 9.7043e-04 - val_mean_squared_error: 9.7043e-04\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00111 to 0.00097, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.0377e-04 - mean_squared_error: 5.0377e-04 - val_loss: 8.4691e-04 - val_mean_squared_error: 8.4691e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00097 to 0.00085, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.8362e-04 - mean_squared_error: 4.8362e-04 - val_loss: 7.4507e-04 - val_mean_squared_error: 7.4507e-04\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.00085 to 0.00075, saving model to weights.best_mlp.hdf5\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.6863e-04 - mean_squared_error: 4.6863e-04 - val_loss: 6.5566e-04 - val_mean_squared_error: 6.5566e-04\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.00075 to 0.00066, saving model to weights.best_mlp.hdf5\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5643e-04 - mean_squared_error: 4.5643e-04 - val_loss: 5.8075e-04 - val_mean_squared_error: 5.8075e-04\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.00066 to 0.00058, saving model to weights.best_mlp.hdf5\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4761e-04 - mean_squared_error: 4.4761e-04 - val_loss: 5.2193e-04 - val_mean_squared_error: 5.2193e-04\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.00058 to 0.00052, saving model to weights.best_mlp.hdf5\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4117e-04 - mean_squared_error: 4.4117e-04 - val_loss: 4.7401e-04 - val_mean_squared_error: 4.7401e-04\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.00052 to 0.00047, saving model to weights.best_mlp.hdf5\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3584e-04 - mean_squared_error: 4.3584e-04 - val_loss: 4.3723e-04 - val_mean_squared_error: 4.3723e-04\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.00047 to 0.00044, saving model to weights.best_mlp.hdf5\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3163e-04 - mean_squared_error: 4.3163e-04 - val_loss: 4.0399e-04 - val_mean_squared_error: 4.0399e-04\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.00044 to 0.00040, saving model to weights.best_mlp.hdf5\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2960e-04 - mean_squared_error: 4.2960e-04 - val_loss: 3.7568e-04 - val_mean_squared_error: 3.7568e-04\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.00040 to 0.00038, saving model to weights.best_mlp.hdf5\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2503e-04 - mean_squared_error: 4.2503e-04 - val_loss: 3.4903e-04 - val_mean_squared_error: 3.4903e-04\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.00038 to 0.00035, saving model to weights.best_mlp.hdf5\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2142e-04 - mean_squared_error: 4.2142e-04 - val_loss: 3.2591e-04 - val_mean_squared_error: 3.2591e-04\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00035 to 0.00033, saving model to weights.best_mlp.hdf5\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1773e-04 - mean_squared_error: 4.1773e-04 - val_loss: 3.0558e-04 - val_mean_squared_error: 3.0558e-04\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.00033 to 0.00031, saving model to weights.best_mlp.hdf5\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1672e-04 - mean_squared_error: 4.1672e-04 - val_loss: 2.8889e-04 - val_mean_squared_error: 2.8889e-04\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.00031 to 0.00029, saving model to weights.best_mlp.hdf5\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1483e-04 - mean_squared_error: 4.1483e-04 - val_loss: 2.7596e-04 - val_mean_squared_error: 2.7596e-04\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.00029 to 0.00028, saving model to weights.best_mlp.hdf5\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1543e-04 - mean_squared_error: 4.1543e-04 - val_loss: 2.6525e-04 - val_mean_squared_error: 2.6525e-04\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.00028 to 0.00027, saving model to weights.best_mlp.hdf5\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.1186e-04 - mean_squared_error: 4.1186e-04 - val_loss: 2.5790e-04 - val_mean_squared_error: 2.5790e-04\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.00027 to 0.00026, saving model to weights.best_mlp.hdf5\n",
            "Epoch 52/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1678e-04 - mean_squared_error: 4.1678e-04 - val_loss: 2.5193e-04 - val_mean_squared_error: 2.5193e-04\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.00026 to 0.00025, saving model to weights.best_mlp.hdf5\n",
            "Epoch 53/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1198e-04 - mean_squared_error: 4.1198e-04 - val_loss: 2.4678e-04 - val_mean_squared_error: 2.4678e-04\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.00025 to 0.00025, saving model to weights.best_mlp.hdf5\n",
            "Epoch 54/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0873e-04 - mean_squared_error: 4.0873e-04 - val_loss: 2.4076e-04 - val_mean_squared_error: 2.4076e-04\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.00025 to 0.00024, saving model to weights.best_mlp.hdf5\n",
            "Epoch 55/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0697e-04 - mean_squared_error: 4.0697e-04 - val_loss: 2.4006e-04 - val_mean_squared_error: 2.4006e-04\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.00024 to 0.00024, saving model to weights.best_mlp.hdf5\n",
            "Epoch 56/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9803e-04 - mean_squared_error: 3.9803e-04 - val_loss: 2.3862e-04 - val_mean_squared_error: 2.3862e-04\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.00024 to 0.00024, saving model to weights.best_mlp.hdf5\n",
            "Epoch 57/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1624e-04 - mean_squared_error: 4.1624e-04 - val_loss: 2.3939e-04 - val_mean_squared_error: 2.3939e-04\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.00024\n",
            "Epoch 58/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0315e-04 - mean_squared_error: 4.0315e-04 - val_loss: 2.3570e-04 - val_mean_squared_error: 2.3570e-04\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.00024 to 0.00024, saving model to weights.best_mlp.hdf5\n",
            "Epoch 59/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.7518e-04 - mean_squared_error: 4.7518e-04 - val_loss: 2.4310e-04 - val_mean_squared_error: 2.4310e-04\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.00024\n",
            "Epoch 60/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.7340e-04 - mean_squared_error: 4.7340e-04 - val_loss: 2.3936e-04 - val_mean_squared_error: 2.3936e-04\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.00024\n",
            "Epoch 00060: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 100\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 4\n",
            "MLP: Train: 0.011661, Validation: 0.000239\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDQAZQ3ADQMYQ3ACQMQQ3AGQMwQ0AGfP/iQaG2vwBgvAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 2s 9ms/step - loss: 6.5520 - mean_squared_error: 6.5520 - val_loss: 0.6475 - val_mean_squared_error: 0.6475\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.64753, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0467 - mean_squared_error: 0.0467 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.64753 to 0.00228, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00228 to 0.00169, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00169 to 0.00157, saving model to weights.best_mlp.hdf5\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00157\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00157\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00157\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00157\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00157\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00157\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00157\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.9747e-04 - mean_squared_error: 9.9747e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00157\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.7404e-04 - mean_squared_error: 9.7404e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00157\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.5735e-04 - mean_squared_error: 9.5735e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00157\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.4267e-04 - mean_squared_error: 9.4267e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00157\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.2583e-04 - mean_squared_error: 9.2583e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00157\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.0394e-04 - mean_squared_error: 9.0394e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00157\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.7637e-04 - mean_squared_error: 8.7637e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00157\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.4511e-04 - mean_squared_error: 8.4511e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00157\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.1388e-04 - mean_squared_error: 8.1388e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00157\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.8609e-04 - mean_squared_error: 7.8609e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00157\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.6408e-04 - mean_squared_error: 7.6408e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00157\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.4898e-04 - mean_squared_error: 7.4898e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00157\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.3695e-04 - mean_squared_error: 7.3695e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00157\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.2929e-04 - mean_squared_error: 7.2929e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00157\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.2274e-04 - mean_squared_error: 7.2274e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00157\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.1576e-04 - mean_squared_error: 7.1576e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00157\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.0025e-04 - mean_squared_error: 7.0025e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00157\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.8257e-04 - mean_squared_error: 6.8257e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00157\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.5660e-04 - mean_squared_error: 6.5660e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00157\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.2600e-04 - mean_squared_error: 6.2600e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00157\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.9721e-04 - mean_squared_error: 5.9721e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.00157 to 0.00141, saving model to weights.best_mlp.hdf5\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.6957e-04 - mean_squared_error: 5.6957e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00141 to 0.00122, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.3704e-04 - mean_squared_error: 5.3704e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00122 to 0.00105, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.0552e-04 - mean_squared_error: 5.0552e-04 - val_loss: 9.1444e-04 - val_mean_squared_error: 9.1444e-04\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00105 to 0.00091, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.8403e-04 - mean_squared_error: 4.8403e-04 - val_loss: 8.0136e-04 - val_mean_squared_error: 8.0136e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00091 to 0.00080, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.6871e-04 - mean_squared_error: 4.6871e-04 - val_loss: 6.9479e-04 - val_mean_squared_error: 6.9479e-04\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.00080 to 0.00069, saving model to weights.best_mlp.hdf5\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.5348e-04 - mean_squared_error: 4.5348e-04 - val_loss: 6.0690e-04 - val_mean_squared_error: 6.0690e-04\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.00069 to 0.00061, saving model to weights.best_mlp.hdf5\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4421e-04 - mean_squared_error: 4.4421e-04 - val_loss: 5.3838e-04 - val_mean_squared_error: 5.3838e-04\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.00061 to 0.00054, saving model to weights.best_mlp.hdf5\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3457e-04 - mean_squared_error: 4.3457e-04 - val_loss: 4.8359e-04 - val_mean_squared_error: 4.8359e-04\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.00054 to 0.00048, saving model to weights.best_mlp.hdf5\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2845e-04 - mean_squared_error: 4.2845e-04 - val_loss: 4.3904e-04 - val_mean_squared_error: 4.3904e-04\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.00048 to 0.00044, saving model to weights.best_mlp.hdf5\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2226e-04 - mean_squared_error: 4.2226e-04 - val_loss: 4.0667e-04 - val_mean_squared_error: 4.0667e-04\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.00044 to 0.00041, saving model to weights.best_mlp.hdf5\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2213e-04 - mean_squared_error: 4.2213e-04 - val_loss: 3.7716e-04 - val_mean_squared_error: 3.7716e-04\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.00041 to 0.00038, saving model to weights.best_mlp.hdf5\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1693e-04 - mean_squared_error: 4.1693e-04 - val_loss: 3.4807e-04 - val_mean_squared_error: 3.4807e-04\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.00038 to 0.00035, saving model to weights.best_mlp.hdf5\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2049e-04 - mean_squared_error: 4.2049e-04 - val_loss: 3.2847e-04 - val_mean_squared_error: 3.2847e-04\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.00035 to 0.00033, saving model to weights.best_mlp.hdf5\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1723e-04 - mean_squared_error: 4.1723e-04 - val_loss: 3.0710e-04 - val_mean_squared_error: 3.0710e-04\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00033 to 0.00031, saving model to weights.best_mlp.hdf5\n",
            "Epoch 00046: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 100\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 2\n",
            "MLP: Train: 0.011133, Validation: 0.000307\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 2s 9ms/step - loss: 6.3472 - mean_squared_error: 6.3472 - val_loss: 0.4524 - val_mean_squared_error: 0.4524\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.45238, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0304 - mean_squared_error: 0.0304 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.45238 to 0.00166, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00166 to 0.00132, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00132\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00132\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00132\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00132\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00132\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00132\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00132\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00132\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00132\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00132\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00132\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.9377e-04 - mean_squared_error: 9.9377e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00132\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.7237e-04 - mean_squared_error: 9.7237e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00132\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.4482e-04 - mean_squared_error: 9.4482e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00132\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.1208e-04 - mean_squared_error: 9.1208e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00132\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.7779e-04 - mean_squared_error: 8.7779e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00132\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.4576e-04 - mean_squared_error: 8.4576e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00132\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.1970e-04 - mean_squared_error: 8.1970e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00132\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.0011e-04 - mean_squared_error: 8.0011e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00132\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.8653e-04 - mean_squared_error: 7.8653e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00132\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.7644e-04 - mean_squared_error: 7.7644e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00132\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.6723e-04 - mean_squared_error: 7.6723e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00132\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.5385e-04 - mean_squared_error: 7.5385e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00132\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.3595e-04 - mean_squared_error: 7.3595e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00132\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.1267e-04 - mean_squared_error: 7.1267e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00132\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.8371e-04 - mean_squared_error: 6.8371e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00132\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.5191e-04 - mean_squared_error: 6.5191e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00132\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.1777e-04 - mean_squared_error: 6.1777e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00132\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.8226e-04 - mean_squared_error: 5.8226e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.00132 to 0.00114, saving model to weights.best_mlp.hdf5\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.4997e-04 - mean_squared_error: 5.4997e-04 - val_loss: 9.8612e-04 - val_mean_squared_error: 9.8612e-04\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00114 to 0.00099, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.2067e-04 - mean_squared_error: 5.2067e-04 - val_loss: 8.4453e-04 - val_mean_squared_error: 8.4453e-04\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00099 to 0.00084, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.9409e-04 - mean_squared_error: 4.9409e-04 - val_loss: 7.3288e-04 - val_mean_squared_error: 7.3288e-04\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00084 to 0.00073, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.7821e-04 - mean_squared_error: 4.7821e-04 - val_loss: 6.3200e-04 - val_mean_squared_error: 6.3200e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00073 to 0.00063, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.6439e-04 - mean_squared_error: 4.6439e-04 - val_loss: 5.4993e-04 - val_mean_squared_error: 5.4993e-04\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.00063 to 0.00055, saving model to weights.best_mlp.hdf5\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5588e-04 - mean_squared_error: 4.5588e-04 - val_loss: 4.9196e-04 - val_mean_squared_error: 4.9196e-04\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.00055 to 0.00049, saving model to weights.best_mlp.hdf5\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5143e-04 - mean_squared_error: 4.5143e-04 - val_loss: 4.5370e-04 - val_mean_squared_error: 4.5370e-04\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.00049 to 0.00045, saving model to weights.best_mlp.hdf5\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5008e-04 - mean_squared_error: 4.5008e-04 - val_loss: 4.1719e-04 - val_mean_squared_error: 4.1719e-04\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.00045 to 0.00042, saving model to weights.best_mlp.hdf5\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5259e-04 - mean_squared_error: 4.5259e-04 - val_loss: 3.9694e-04 - val_mean_squared_error: 3.9694e-04\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.00042 to 0.00040, saving model to weights.best_mlp.hdf5\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5020e-04 - mean_squared_error: 4.5020e-04 - val_loss: 3.7103e-04 - val_mean_squared_error: 3.7103e-04\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.00040 to 0.00037, saving model to weights.best_mlp.hdf5\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5214e-04 - mean_squared_error: 4.5214e-04 - val_loss: 3.4997e-04 - val_mean_squared_error: 3.4997e-04\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.00037 to 0.00035, saving model to weights.best_mlp.hdf5\n",
            "Epoch 00043: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 100\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 3\n",
            "MLP: Train: 0.011081, Validation: 0.000350\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nENwA0GL+H4lc6HB+x6VTAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 2s 9ms/step - loss: 6.0597 - mean_squared_error: 6.0597 - val_loss: 0.3880 - val_mean_squared_error: 0.3880\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.38796, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0298 - mean_squared_error: 0.0298 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.38796 to 0.00177, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00177 to 0.00144, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00144\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00144\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00144\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00144\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00144\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00144\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00144\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.8524e-04 - mean_squared_error: 9.8524e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00144\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.5149e-04 - mean_squared_error: 9.5149e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00144\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.2764e-04 - mean_squared_error: 9.2764e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00144\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.0803e-04 - mean_squared_error: 9.0803e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00144\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.8791e-04 - mean_squared_error: 8.8791e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00144\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.6411e-04 - mean_squared_error: 8.6411e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00144\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.3606e-04 - mean_squared_error: 8.3606e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00144\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.0606e-04 - mean_squared_error: 8.0606e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00144\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.7743e-04 - mean_squared_error: 7.7743e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00144\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.5295e-04 - mean_squared_error: 7.5295e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00144\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.3487e-04 - mean_squared_error: 7.3487e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00144\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.2327e-04 - mean_squared_error: 7.2327e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00144\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.1466e-04 - mean_squared_error: 7.1466e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00144\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.0693e-04 - mean_squared_error: 7.0693e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00144\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.9642e-04 - mean_squared_error: 6.9642e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00144\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.8161e-04 - mean_squared_error: 6.8161e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00144\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.6130e-04 - mean_squared_error: 6.6130e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00144\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.3513e-04 - mean_squared_error: 6.3513e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00144\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.0716e-04 - mean_squared_error: 6.0716e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00144\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.7973e-04 - mean_squared_error: 5.7973e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00144\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.5065e-04 - mean_squared_error: 5.5065e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.00144 to 0.00134, saving model to weights.best_mlp.hdf5\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.2161e-04 - mean_squared_error: 5.2161e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.00134 to 0.00115, saving model to weights.best_mlp.hdf5\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.9587e-04 - mean_squared_error: 4.9587e-04 - val_loss: 9.9309e-04 - val_mean_squared_error: 9.9309e-04\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00115 to 0.00099, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.7357e-04 - mean_squared_error: 4.7357e-04 - val_loss: 8.4654e-04 - val_mean_squared_error: 8.4654e-04\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00099 to 0.00085, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5468e-04 - mean_squared_error: 4.5468e-04 - val_loss: 7.2615e-04 - val_mean_squared_error: 7.2615e-04\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00085 to 0.00073, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4085e-04 - mean_squared_error: 4.4085e-04 - val_loss: 6.2492e-04 - val_mean_squared_error: 6.2492e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00073 to 0.00062, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2949e-04 - mean_squared_error: 4.2949e-04 - val_loss: 5.4316e-04 - val_mean_squared_error: 5.4316e-04\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.00062 to 0.00054, saving model to weights.best_mlp.hdf5\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2305e-04 - mean_squared_error: 4.2305e-04 - val_loss: 4.8079e-04 - val_mean_squared_error: 4.8079e-04\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.00054 to 0.00048, saving model to weights.best_mlp.hdf5\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1984e-04 - mean_squared_error: 4.1984e-04 - val_loss: 4.3325e-04 - val_mean_squared_error: 4.3325e-04\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.00048 to 0.00043, saving model to weights.best_mlp.hdf5\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1690e-04 - mean_squared_error: 4.1690e-04 - val_loss: 3.8975e-04 - val_mean_squared_error: 3.8975e-04\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.00043 to 0.00039, saving model to weights.best_mlp.hdf5\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1686e-04 - mean_squared_error: 4.1686e-04 - val_loss: 3.5286e-04 - val_mean_squared_error: 3.5286e-04\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.00039 to 0.00035, saving model to weights.best_mlp.hdf5\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1612e-04 - mean_squared_error: 4.1612e-04 - val_loss: 3.1966e-04 - val_mean_squared_error: 3.1966e-04\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.00035 to 0.00032, saving model to weights.best_mlp.hdf5\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1558e-04 - mean_squared_error: 4.1558e-04 - val_loss: 2.9446e-04 - val_mean_squared_error: 2.9446e-04\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.00032 to 0.00029, saving model to weights.best_mlp.hdf5\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1642e-04 - mean_squared_error: 4.1642e-04 - val_loss: 2.7623e-04 - val_mean_squared_error: 2.7623e-04\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.00029 to 0.00028, saving model to weights.best_mlp.hdf5\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0806e-04 - mean_squared_error: 4.0806e-04 - val_loss: 2.6262e-04 - val_mean_squared_error: 2.6262e-04\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.00028 to 0.00026, saving model to weights.best_mlp.hdf5\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0279e-04 - mean_squared_error: 4.0279e-04 - val_loss: 2.5303e-04 - val_mean_squared_error: 2.5303e-04\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00026 to 0.00025, saving model to weights.best_mlp.hdf5\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9787e-04 - mean_squared_error: 3.9787e-04 - val_loss: 2.4561e-04 - val_mean_squared_error: 2.4561e-04\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.00025 to 0.00025, saving model to weights.best_mlp.hdf5\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9361e-04 - mean_squared_error: 3.9361e-04 - val_loss: 2.3927e-04 - val_mean_squared_error: 2.3927e-04\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.00025 to 0.00024, saving model to weights.best_mlp.hdf5\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.7453e-04 - mean_squared_error: 3.7453e-04 - val_loss: 2.3602e-04 - val_mean_squared_error: 2.3602e-04\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.00024 to 0.00024, saving model to weights.best_mlp.hdf5\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.6470e-04 - mean_squared_error: 3.6470e-04 - val_loss: 2.3403e-04 - val_mean_squared_error: 2.3403e-04\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.00024 to 0.00023, saving model to weights.best_mlp.hdf5\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.5447e-04 - mean_squared_error: 3.5447e-04 - val_loss: 2.3253e-04 - val_mean_squared_error: 2.3253e-04\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.00023 to 0.00023, saving model to weights.best_mlp.hdf5\n",
            "Epoch 52/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.3917e-04 - mean_squared_error: 3.3917e-04 - val_loss: 2.3684e-04 - val_mean_squared_error: 2.3684e-04\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.00023\n",
            "Epoch 53/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.2369e-04 - mean_squared_error: 3.2369e-04 - val_loss: 2.4215e-04 - val_mean_squared_error: 2.4215e-04\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.00023\n",
            "Epoch 54/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.1761e-04 - mean_squared_error: 3.1761e-04 - val_loss: 2.5086e-04 - val_mean_squared_error: 2.5086e-04\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.00023\n",
            "Epoch 55/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.0278e-04 - mean_squared_error: 3.0278e-04 - val_loss: 2.5314e-04 - val_mean_squared_error: 2.5314e-04\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.00023\n",
            "Epoch 56/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.2412e-04 - mean_squared_error: 3.2412e-04 - val_loss: 2.7457e-04 - val_mean_squared_error: 2.7457e-04\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.00023\n",
            "Epoch 57/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.0135e-04 - mean_squared_error: 4.0135e-04 - val_loss: 2.5166e-04 - val_mean_squared_error: 2.5166e-04\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.00023\n",
            "Epoch 58/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.6645e-04 - mean_squared_error: 4.6645e-04 - val_loss: 2.7877e-04 - val_mean_squared_error: 2.7877e-04\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.00023\n",
            "Epoch 59/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.5618e-04 - mean_squared_error: 3.5618e-04 - val_loss: 2.3845e-04 - val_mean_squared_error: 2.3845e-04\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.00023\n",
            "Epoch 00059: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 100\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 4\n",
            "MLP: Train: 0.011577, Validation: 0.000238\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 2s 9ms/step - loss: 6.4253 - mean_squared_error: 6.4253 - val_loss: 0.5458 - val_mean_squared_error: 0.5458\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.54576, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0387 - mean_squared_error: 0.0387 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.54576 to 0.00164, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00164 to 0.00135, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00135\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00135\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00135\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00135\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00135\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00135\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00135\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00135\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.8495e-04 - mean_squared_error: 9.8495e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00135\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.6542e-04 - mean_squared_error: 9.6542e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00135\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.5149e-04 - mean_squared_error: 9.5149e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00135\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.3838e-04 - mean_squared_error: 9.3838e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00135\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.2187e-04 - mean_squared_error: 9.2187e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00135\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.9951e-04 - mean_squared_error: 8.9951e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00135\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.7139e-04 - mean_squared_error: 8.7139e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00135\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.3966e-04 - mean_squared_error: 8.3966e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00135\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.0830e-04 - mean_squared_error: 8.0830e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00135\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.8079e-04 - mean_squared_error: 7.8079e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00135\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.5920e-04 - mean_squared_error: 7.5920e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00135\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.4442e-04 - mean_squared_error: 7.4442e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00135\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.3314e-04 - mean_squared_error: 7.3314e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00135\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.2660e-04 - mean_squared_error: 7.2660e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00135\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.1677e-04 - mean_squared_error: 7.1677e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00135\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.0672e-04 - mean_squared_error: 7.0672e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00135\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.9124e-04 - mean_squared_error: 6.9124e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00135\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.6654e-04 - mean_squared_error: 6.6654e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00135\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.4068e-04 - mean_squared_error: 6.4068e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00135\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.1282e-04 - mean_squared_error: 6.1282e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00135\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.8394e-04 - mean_squared_error: 5.8394e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.00135 to 0.00133, saving model to weights.best_mlp.hdf5\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.5166e-04 - mean_squared_error: 5.5166e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00133 to 0.00116, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.2867e-04 - mean_squared_error: 5.2867e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00116 to 0.00101, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.0634e-04 - mean_squared_error: 5.0634e-04 - val_loss: 8.7623e-04 - val_mean_squared_error: 8.7623e-04\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00101 to 0.00088, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.8558e-04 - mean_squared_error: 4.8558e-04 - val_loss: 7.6156e-04 - val_mean_squared_error: 7.6156e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00088 to 0.00076, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.6818e-04 - mean_squared_error: 4.6818e-04 - val_loss: 6.6289e-04 - val_mean_squared_error: 6.6289e-04\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.00076 to 0.00066, saving model to weights.best_mlp.hdf5\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5122e-04 - mean_squared_error: 4.5122e-04 - val_loss: 5.8053e-04 - val_mean_squared_error: 5.8053e-04\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.00066 to 0.00058, saving model to weights.best_mlp.hdf5\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4186e-04 - mean_squared_error: 4.4186e-04 - val_loss: 5.1938e-04 - val_mean_squared_error: 5.1938e-04\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.00058 to 0.00052, saving model to weights.best_mlp.hdf5\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3114e-04 - mean_squared_error: 4.3114e-04 - val_loss: 4.7210e-04 - val_mean_squared_error: 4.7210e-04\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.00052 to 0.00047, saving model to weights.best_mlp.hdf5\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2466e-04 - mean_squared_error: 4.2466e-04 - val_loss: 4.3185e-04 - val_mean_squared_error: 4.3185e-04\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.00047 to 0.00043, saving model to weights.best_mlp.hdf5\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2082e-04 - mean_squared_error: 4.2082e-04 - val_loss: 3.9781e-04 - val_mean_squared_error: 3.9781e-04\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.00043 to 0.00040, saving model to weights.best_mlp.hdf5\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1756e-04 - mean_squared_error: 4.1756e-04 - val_loss: 3.6663e-04 - val_mean_squared_error: 3.6663e-04\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.00040 to 0.00037, saving model to weights.best_mlp.hdf5\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1604e-04 - mean_squared_error: 4.1604e-04 - val_loss: 3.3826e-04 - val_mean_squared_error: 3.3826e-04\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.00037 to 0.00034, saving model to weights.best_mlp.hdf5\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1572e-04 - mean_squared_error: 4.1572e-04 - val_loss: 3.1085e-04 - val_mean_squared_error: 3.1085e-04\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.00034 to 0.00031, saving model to weights.best_mlp.hdf5\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1548e-04 - mean_squared_error: 4.1548e-04 - val_loss: 2.9048e-04 - val_mean_squared_error: 2.9048e-04\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00031 to 0.00029, saving model to weights.best_mlp.hdf5\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1322e-04 - mean_squared_error: 4.1322e-04 - val_loss: 2.7558e-04 - val_mean_squared_error: 2.7558e-04\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.00029 to 0.00028, saving model to weights.best_mlp.hdf5\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1219e-04 - mean_squared_error: 4.1219e-04 - val_loss: 2.6165e-04 - val_mean_squared_error: 2.6165e-04\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.00028 to 0.00026, saving model to weights.best_mlp.hdf5\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1061e-04 - mean_squared_error: 4.1061e-04 - val_loss: 2.5460e-04 - val_mean_squared_error: 2.5460e-04\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.00026 to 0.00025, saving model to weights.best_mlp.hdf5\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0740e-04 - mean_squared_error: 4.0740e-04 - val_loss: 2.4364e-04 - val_mean_squared_error: 2.4364e-04\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.00025 to 0.00024, saving model to weights.best_mlp.hdf5\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0977e-04 - mean_squared_error: 4.0977e-04 - val_loss: 2.4360e-04 - val_mean_squared_error: 2.4360e-04\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.00024 to 0.00024, saving model to weights.best_mlp.hdf5\n",
            "Epoch 52/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9313e-04 - mean_squared_error: 3.9313e-04 - val_loss: 2.3176e-04 - val_mean_squared_error: 2.3176e-04\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.00024 to 0.00023, saving model to weights.best_mlp.hdf5\n",
            "Epoch 53/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9926e-04 - mean_squared_error: 3.9926e-04 - val_loss: 2.3282e-04 - val_mean_squared_error: 2.3282e-04\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.00023\n",
            "Epoch 54/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.7224e-04 - mean_squared_error: 3.7224e-04 - val_loss: 2.2741e-04 - val_mean_squared_error: 2.2741e-04\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.00023 to 0.00023, saving model to weights.best_mlp.hdf5\n",
            "Epoch 55/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8645e-04 - mean_squared_error: 3.8645e-04 - val_loss: 2.2766e-04 - val_mean_squared_error: 2.2766e-04\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.00023\n",
            "Epoch 56/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.6186e-04 - mean_squared_error: 3.6186e-04 - val_loss: 2.2455e-04 - val_mean_squared_error: 2.2455e-04\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.00023 to 0.00022, saving model to weights.best_mlp.hdf5\n",
            "Epoch 57/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.6230e-04 - mean_squared_error: 3.6230e-04 - val_loss: 2.2703e-04 - val_mean_squared_error: 2.2703e-04\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.00022\n",
            "Epoch 58/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.5115e-04 - mean_squared_error: 3.5115e-04 - val_loss: 2.2219e-04 - val_mean_squared_error: 2.2219e-04\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.00022 to 0.00022, saving model to weights.best_mlp.hdf5\n",
            "Epoch 59/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.6607e-04 - mean_squared_error: 3.6607e-04 - val_loss: 2.2132e-04 - val_mean_squared_error: 2.2132e-04\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.00022 to 0.00022, saving model to weights.best_mlp.hdf5\n",
            "Epoch 60/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.3319e-04 - mean_squared_error: 3.3319e-04 - val_loss: 2.2109e-04 - val_mean_squared_error: 2.2109e-04\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.00022 to 0.00022, saving model to weights.best_mlp.hdf5\n",
            "Epoch 61/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.6068e-04 - mean_squared_error: 3.6068e-04 - val_loss: 2.3247e-04 - val_mean_squared_error: 2.3247e-04\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.00022\n",
            "Epoch 62/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.3733e-04 - mean_squared_error: 3.3733e-04 - val_loss: 2.2347e-04 - val_mean_squared_error: 2.2347e-04\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.00022\n",
            "Epoch 00062: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 100\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 2\n",
            "MLP: Train: 0.011595, Validation: 0.000223\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 2s 10ms/step - loss: 6.2984 - mean_squared_error: 6.2984 - val_loss: 0.4100 - val_mean_squared_error: 0.4100\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.40998, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.40998 to 0.00165, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00165 to 0.00139, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00139\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00139\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00139\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00139\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00139\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00139\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00139\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.9094e-04 - mean_squared_error: 9.9094e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00139\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.6454e-04 - mean_squared_error: 9.6454e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00139\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.4638e-04 - mean_squared_error: 9.4638e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00139\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.3113e-04 - mean_squared_error: 9.3113e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00139\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.1417e-04 - mean_squared_error: 9.1417e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00139\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.9206e-04 - mean_squared_error: 8.9206e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00139\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.6479e-04 - mean_squared_error: 8.6479e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00139\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.3450e-04 - mean_squared_error: 8.3450e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00139\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.0429e-04 - mean_squared_error: 8.0429e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00139\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.7794e-04 - mean_squared_error: 7.7794e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00139\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.5793e-04 - mean_squared_error: 7.5793e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00139\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.4490e-04 - mean_squared_error: 7.4490e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00139\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.3586e-04 - mean_squared_error: 7.3586e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00139\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.3028e-04 - mean_squared_error: 7.3028e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00139\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.2221e-04 - mean_squared_error: 7.2221e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00139\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.0799e-04 - mean_squared_error: 7.0799e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00139\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.9276e-04 - mean_squared_error: 6.9276e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00139\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.6829e-04 - mean_squared_error: 6.6829e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00139\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.4170e-04 - mean_squared_error: 6.4170e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00139\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.1031e-04 - mean_squared_error: 6.1031e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00139\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.8170e-04 - mean_squared_error: 5.8170e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.00139 to 0.00129, saving model to weights.best_mlp.hdf5\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.5130e-04 - mean_squared_error: 5.5130e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.00129 to 0.00111, saving model to weights.best_mlp.hdf5\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.2157e-04 - mean_squared_error: 5.2157e-04 - val_loss: 9.5007e-04 - val_mean_squared_error: 9.5007e-04\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00111 to 0.00095, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.9499e-04 - mean_squared_error: 4.9499e-04 - val_loss: 8.1276e-04 - val_mean_squared_error: 8.1276e-04\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00095 to 0.00081, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.7421e-04 - mean_squared_error: 4.7421e-04 - val_loss: 6.9470e-04 - val_mean_squared_error: 6.9470e-04\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00081 to 0.00069, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.6020e-04 - mean_squared_error: 4.6020e-04 - val_loss: 5.8995e-04 - val_mean_squared_error: 5.8995e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00069 to 0.00059, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4881e-04 - mean_squared_error: 4.4881e-04 - val_loss: 5.1012e-04 - val_mean_squared_error: 5.1012e-04\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.00059 to 0.00051, saving model to weights.best_mlp.hdf5\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4259e-04 - mean_squared_error: 4.4259e-04 - val_loss: 4.4350e-04 - val_mean_squared_error: 4.4350e-04\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.00051 to 0.00044, saving model to weights.best_mlp.hdf5\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3816e-04 - mean_squared_error: 4.3816e-04 - val_loss: 4.0420e-04 - val_mean_squared_error: 4.0420e-04\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.00044 to 0.00040, saving model to weights.best_mlp.hdf5\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3073e-04 - mean_squared_error: 4.3073e-04 - val_loss: 3.6705e-04 - val_mean_squared_error: 3.6705e-04\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.00040 to 0.00037, saving model to weights.best_mlp.hdf5\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2842e-04 - mean_squared_error: 4.2842e-04 - val_loss: 3.3640e-04 - val_mean_squared_error: 3.3640e-04\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.00037 to 0.00034, saving model to weights.best_mlp.hdf5\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2640e-04 - mean_squared_error: 4.2640e-04 - val_loss: 3.0558e-04 - val_mean_squared_error: 3.0558e-04\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.00034 to 0.00031, saving model to weights.best_mlp.hdf5\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2771e-04 - mean_squared_error: 4.2771e-04 - val_loss: 2.8082e-04 - val_mean_squared_error: 2.8082e-04\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.00031 to 0.00028, saving model to weights.best_mlp.hdf5\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2461e-04 - mean_squared_error: 4.2461e-04 - val_loss: 2.6208e-04 - val_mean_squared_error: 2.6208e-04\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.00028 to 0.00026, saving model to weights.best_mlp.hdf5\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2307e-04 - mean_squared_error: 4.2307e-04 - val_loss: 2.4763e-04 - val_mean_squared_error: 2.4763e-04\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.00026 to 0.00025, saving model to weights.best_mlp.hdf5\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1839e-04 - mean_squared_error: 4.1839e-04 - val_loss: 2.3406e-04 - val_mean_squared_error: 2.3406e-04\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00025 to 0.00023, saving model to weights.best_mlp.hdf5\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1271e-04 - mean_squared_error: 4.1271e-04 - val_loss: 2.2762e-04 - val_mean_squared_error: 2.2762e-04\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.00023 to 0.00023, saving model to weights.best_mlp.hdf5\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0450e-04 - mean_squared_error: 4.0450e-04 - val_loss: 2.2174e-04 - val_mean_squared_error: 2.2174e-04\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.00023 to 0.00022, saving model to weights.best_mlp.hdf5\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9720e-04 - mean_squared_error: 3.9720e-04 - val_loss: 2.1907e-04 - val_mean_squared_error: 2.1907e-04\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.00022 to 0.00022, saving model to weights.best_mlp.hdf5\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8330e-04 - mean_squared_error: 3.8330e-04 - val_loss: 2.1178e-04 - val_mean_squared_error: 2.1178e-04\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.00022 to 0.00021, saving model to weights.best_mlp.hdf5\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8033e-04 - mean_squared_error: 3.8033e-04 - val_loss: 2.1490e-04 - val_mean_squared_error: 2.1490e-04\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.00021\n",
            "Epoch 52/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.6094e-04 - mean_squared_error: 3.6094e-04 - val_loss: 2.0950e-04 - val_mean_squared_error: 2.0950e-04\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.00021 to 0.00021, saving model to weights.best_mlp.hdf5\n",
            "Epoch 53/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.7574e-04 - mean_squared_error: 3.7574e-04 - val_loss: 2.1516e-04 - val_mean_squared_error: 2.1516e-04\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.00021\n",
            "Epoch 54/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9450e-04 - mean_squared_error: 3.9450e-04 - val_loss: 2.1121e-04 - val_mean_squared_error: 2.1121e-04\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.00021\n",
            "Epoch 55/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.6920e-04 - mean_squared_error: 3.6920e-04 - val_loss: 2.1812e-04 - val_mean_squared_error: 2.1812e-04\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.00021\n",
            "Epoch 00055: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 100\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 3\n",
            "MLP: Train: 0.011791, Validation: 0.000218\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAFQJJREFUeJzt3X2QnWV5x/HfdV43b5iwWZKYJW46\ndUBeNKlLjBOmg7Q6AeRlqhItdGy1ZtrSGhypE9s/1I7O0H+sMlOlEalODdgMmGIdEFETUkpANxAk\nQCSGCc0GQjaRYAJksy9X/zgve87uc142u8+e+9n9fmYy2fOcZ59cN3v2tzf3Xs+5zd0FAEiOVKsL\nAACMD8ENAAlDcANAwhDcAJAwBDcAJAzBDQAJQ3ADQMIQ3ACQMAQ3ACRMJo6LLly40Lu6uuK4NABM\nS7t27Trq7h3NnBtLcHd1damnpyeOSwPAtGRmLzZ7LkslAJAwBDcAJAzBDQAJE8saNwCM18DAgHp7\ne3Xq1KlWlxKrtrY2dXZ2KpvNnvE1CG4AQejt7dW8efPU1dUlM2t1ObFwdx07dky9vb1avnz5GV+H\npRIAQTh16pTa29unbWhLkpmpvb19wv9XQXADCMZ0Du2SyRhjUMF928/26eHn+1pdBgAELajgvv3h\n/XpkH8ENYOodP35c3/jGN8b9eVdeeaWOHz8eQ0W1BRXc+UxK/YPDrS4DwAxUK7gHBwfrft7999+v\n+fPnx1VWpKC6SnKZlE4T3ABaYOPGjdq/f79WrFihbDartrY2LViwQHv37tXzzz+v6667TgcPHtSp\nU6e0YcMGrV+/XtLIW3ycPHlSV1xxhS699FI9+uijWrp0qe677z7NmjVr0msluAEE50v//Yyefel3\nk3rNC956lr5w9YU1n7/11lu1Z88e7d69W9u3b9dVV12lPXv2lNv27rzzTp199tl68803dckll+hD\nH/qQ2tvbq66xb98+3X333frWt76l66+/Xvfee69uvPHGSR2HFFhw5zNplkoABGHVqlVVvda33Xab\ntm7dKkk6ePCg9u3bNya4ly9frhUrVkiS3v3ud+vAgQOx1BZUcOfSrHEDUN2Z8VSZM2dO+ePt27fr\npz/9qXbu3KnZs2frsssui+zFzufz5Y/T6bTefPPNWGpr6peTZjbfzO4xs71m9pyZvTeOYnKZlE4P\nEdwApt68efN04sSJyOdee+01LViwQLNnz9bevXv12GOPTXF11ZqdcX9d0o/d/cNmlpM0O45i8pmU\n+geG4rg0ANTV3t6uNWvW6KKLLtKsWbO0aNGi8nNr167V7bffrne84x0677zztHr16hZW2kRwm9lb\nJP2hpD+XJHc/Lel0HMXkMimd7K/fegMAcbnrrrsij+fzeT3wwAORz5XWsRcuXKg9e/aUj99yyy2T\nXl9JM0slyyX1Sfp3M3vSzO4wszmNPulM5OkqAYCGmgnujKQ/kPRNd18p6XVJG0efZGbrzazHzHr6\n+s7s7ke6SgCgsWaCu1dSr7s/Xnx8jwpBXsXdN7l7t7t3d3Q0td/lGPRxA0BjDYPb3Q9LOmhm5xUP\n/ZGkZ+MoJpcmuAGgkWa7Sv5O0uZiR8kLkv4ijmLy2ZT6B+kqAYB6mgpud98tqTvmWphxA0ATgnp3\nQG7AAZAUc+fObdm/HVRw5zNpDQy5hoe91aUAQLDCeq+STOHnyOmhYbWl0i2uBsBMsnHjRp177rm6\n6aabJElf/OIXlclktG3bNr366qsaGBjQl7/8ZV177bUtrjTQ4O4fHFZbluAGZqwHNkqHn57cay6+\nWLri1ppPr1u3TjfffHM5uLds2aIHH3xQn/70p3XWWWfp6NGjWr16ta655pqW740ZVHDny8E9JCnb\n2mIAzCgrV67UkSNH9NJLL6mvr08LFizQ4sWL9ZnPfEY7duxQKpXSoUOH9Morr2jx4sUtrTWo4C4v\nldBZAsxsdWbGcfrIRz6ie+65R4cPH9a6deu0efNm9fX1adeuXcpms+rq6op8O9epFlRw5wluAC20\nbt06fepTn9LRo0f18MMPa8uWLTrnnHOUzWa1bds2vfjii60uUVKgwc37lQBohQsvvFAnTpzQ0qVL\ntWTJEt1www26+uqrdfHFF6u7u1vnn39+q0uUFFhws1QCoNWefnrkl6ILFy7Uzp07I887efLkVJU0\nRlB93Ll0oZOEm3AAoLaggjufLS6VDBDcAFBLUMGdS5duwOGNpoCZyH363zU9GWMMK7hZ4wZmrLa2\nNh07dmxah7e769ixY2pra5vQdYL65SRdJcDM1dnZqd7eXp3pDlpJ0dbWps7OzgldI6jgzhHcwIyV\nzWa1fPnyVpeRCCyVAEDCBBXc+UyhHZAZNwDUFlhwM+MGgEaCCu5yOyDBDQA1BRXcqZQpmzY2DAaA\nOoIKbokNgwGgkfCCmw2DAaCupvq4zeyApBOShiQNunt3XAXlMineqwQA6hjPDTjvc/ejsVVSlM+k\nmXEDQB1hLpWwxg0ANTUb3C7pJ2a2y8zWx1lQLp2iqwQA6mh2qeRSdz9kZudIesjM9rr7jsoTioG+\nXpKWLVt2xgXlsynunASAOpqacbv7oeLfRyRtlbQq4pxN7t7t7t0dHR1nXBDtgABQX8PgNrM5Zjav\n9LGkD0jaE1dBuQwzbgCop5mlkkWStppZ6fy73P3HcRWUz6R1bPB0XJcHgMRrGNzu/oKkd01BLZIK\nbzRFOyAA1BZkOyBdJQBQW3DBnaePGwDqCi64uQEHAOoLL7jTdJUAQD3BBXc+y4wbAOoJLrhz6bQG\nh11Dw97qUgAgSOEFN/tOAkBdwQU3GwYDQH3BBXdpxt0/RC83AEQJN7jZBQcAIgUX3OWlEm57B4BI\n4QY3a9wAECm44C4vlRDcABApuODOZ9KSmHEDQC3BBTd93ABQX3jBnS4tldAOCABRggvufJYZNwDU\nE1xwl2bctAMCQLTwgpsbcACgruCCu9RV0s+MGwAiBRfcdJUAQH3BBXc+Q1cJANQTXHCXfznJjBsA\nIjUd3GaWNrMnzexHsRaUMmXTRnADQA3jmXFvkPRcXIVUYsNgAKitqeA2s05JV0m6I95yCvLZNDNu\nAKih2Rn31yR9TlLNNDWz9WbWY2Y9fX19Eyoql2andwCopWFwm9kHJR1x9131znP3Te7e7e7dHR0d\nEyoql0nRVQIANTQz414j6RozOyDp+5IuN7PvxVlUPpPilncAqKFhcLv759290927JH1U0s/d/cY4\ni8plWCoBgFqC6+OWSkslBDcARMmM52R33y5peyyVVMgT3ABQU6AzbtoBAaCWMIObG3AAoKYggzuf\nTek07YAAECnM4E7TDggAtQQZ3LQDAkBtQQY3XSUAUFuQwc2MGwBqI7gBIGGCDO58Jq3BYdfQsLe6\nFAAITpDBzYbBAFBbmMHNvpMAUFOQwZ3PstM7ANQSZHCXZty0BALAWGEGd2mNm7snAWCMIIM7Xwzu\n/gGCGwBGCzS405KYcQNAlCCDm3ZAAKgt6OCmqwQAxgoyuPPMuAGgpiCDm6USAKgtzOCmjxsAagoy\nuPPZYlcJwQ0AYzQMbjNrM7NfmNlTZvaMmX0p7qLKM27aAQFgjEwT5/RLutzdT5pZVtIjZvaAuz8W\nV1HlrpIBukoAYLSGwe3uLulk8WG2+CfWN8rOc8s7ANTU1Bq3maXNbLekI5IecvfH4yyKt3UFgNqa\nCm53H3L3FZI6Ja0ys4tGn2Nm682sx8x6+vr6JlZUypRNG10lABBhXF0l7n5c0jZJayOe2+Tu3e7e\n3dHRMeHC8pk0M24AiNBMV0mHmc0vfjxL0vsl7Y27MDYMBoBozXSVLJH0XTNLqxD0W9z9R/GWVVjn\n5r1KAGCsZrpKfiVp5RTUUiWfZcYNAFGCvHNSKsy4aQcEgLHCDe5Mih1wACBCsMGdzzDjBoAowQZ3\nLpOijxsAIgQc3GmCGwAiBBvcefq4ASBSsMFduAGHPm4AGC3Y4M6nWeMGgCjhBjc34ABApGCDmxtw\nACBauMHNDTgAECnY4M5n0sy4ASBCsMGdy6Q0NOwaJLwBoErQwS2x7yQAjBZscJc3DKazBACqBBvc\nOYIbACKFG9zFnd65CQcAqgUb3PlsWhLBDQCjBRvcpRk3SyUAUC3Y4C79cpINgwGgWvDBzYwbAKoF\nG9z0cQNAtIbBbWbnmtk2M3vWzJ4xsw1TUVgpuHm/EgColmninEFJn3X3J8xsnqRdZvaQuz8bZ2H5\nTKGrhBk3AFRrOON295fd/YnixyckPSdpadyFcQMOAEQb1xq3mXVJWinp8TiKqZSjqwQAIjUd3GY2\nV9K9km52999FPL/ezHrMrKevr2/ChdFVAgDRmgpuM8uqENqb3f0HUee4+yZ373b37o6OjgkXNjLj\nJrgBoFIzXSUm6duSnnP3r8ZfUgHvVQIA0ZqZca+R9GeSLjez3cU/V8ZcF0slAFBDw3ZAd39Ekk1B\nLVXMjA2DASBCsHdOSmwYDABRgg7ufCal00O0AwJApaCDO5dJscYNAKMEH9x0lQBAtaCDO8+MGwDG\nCDq4WSoBgLHCDu40SyUAMFrYwc2MGwDGCDq485m0+rkBBwCqBB3chRtw6OMGgErBBze3vANAtaCD\nm3ZAABgr+OCmqwQAqgUd3Lk0M24AGC3o4M5n0wQ3AIwSdHAXbsChqwQAKoUd3JmUhl0apLMEAMqC\nDu7y9mUENwCUBR3c5Z3e2QUHAMoSEdzMuAFgRNDBnc+kJbHTOwBUCjq4y0sldJYAQFnYwZ0uBTcz\nbgAoaRjcZnanmR0xsz1TUVClfLa4xk1wA0BZMzPu70haG3MdkfLMuAFgjIbB7e47JP12CmoZo9xV\nQnADQFnQa9x0lQDAWJMW3Ga23sx6zKynr69vUq450lVCcANAyaQFt7tvcvdud+/u6OiYlGuO3IBD\nOyAAlAS+VMIaNwCM1kw74N2Sdko6z8x6zeyT8ZdVwFIJAIyVaXSCu39sKgqJQlcJAIyViKUSZtwA\nMCLo4OaWdwAYK+jgNjM2DAaAUYIObqmwXEJwA8CI4IM7l2HDYAColIjgZsYNACOCD+58JsXWZQBQ\nIfjgzmVSbBYMABUSEdzMuAFgRPDBnc+kWeMGgArBB3cuTVcJAFQKP7jpKgGAKuEEd/8J6QfrpV9t\nqTqcz6S45R0AKoQT3Lm50stPSY/eJrmPHGbGDQBVwgluM2n1X0uHn5YOPFI+nGPGDQBVwgluSXrn\nOmnW2dJj3ywfymfStAMCQIWwgjs7S+r+hPTr+6Vj+yUV17gH6CoBgJKwgluSLvlLKZWRfrFJEjfg\nAMBo4QX3WUuki/5EevJ70qnXym/r6hW/sASAmSy84Jak1X8jnT4pPfEfyqVTGnZpcJjgBgAp1OB+\n6wrpbWukx/9N+VRhmYSWQAAoCDO4pUJr4Gv/p7e/ukMSwQ0AJU0Ft5mtNbNfm9lvzGxj3EVJks67\nUpr/Nl14cLMkNgwGgJKGwW1maUn/KukKSRdI+piZXRB3YUqlpff8lc559Um90/Yz4waAomZm3Ksk\n/cbdX3D305K+L+naeMsqWnmjBjJz9InMA7rjkRe05ZcH9ej+ozr42zc0SIsggBkq08Q5SyUdrHjc\nK+k98ZQzSttZev3CP9XVT31blzxxrfyJkacOS0qZyWVVn2LyMR9Xn1HN6j05RSprbu78KGPHPfrc\n8nFv8HydmsZbayOjv36NzvHiF6y6Cqs6L2p0Y58b/W/X+q9aeY1GNY89t9b5zYy76vxxvFDHe+3p\nIoRxv5F+iy74x/+N/d9pJribYmbrJa2XpGXLlk3WZTX/j/9eygxqyWC/3jg9pNf7B/RG/6BePz2k\ngeL7dFd9k5SyqfQ44pt35HGNEJrizsPGP15qfc7oY9FBFPVNH3Xu6DrGG3Lj1/iHw+ivmCSZ+5hj\noz/XNPacqmv5Gf6Q8uj/04uqs9Y1xv/Dr/nzbRLud5jsH84zyWBu3pT8O80E9yFJ51Y87iweq+Lu\nmyRtkqTu7u7J+8rPWyRd/TWlJM0t/gGAmayZNe5fSnq7mS03s5ykj0r6YbxlAQBqaTjjdvdBM/tb\nSQ9KSku6092fib0yAECkpta43f1+SffHXAsAoAnh3jkJAIhEcANAwhDcAJAwBDcAJAzBDQAJY3Hs\nLGNmfZJePMNPXyjp6CSWE5rpPj5p+o+R8SVfiGN8m7t3NHNiLME9EWbW4+7dra4jLtN9fNL0HyPj\nS76kj5GlEgBIGIIbABImxODe1OoCYjbdxydN/zEyvuRL9BiDW+MGANQX4owbAFBHMMHdkg2JY2Zm\nd5rZETPbU3HsbDN7yMz2Ff9e0MoaJ8LMzjWzbWb2rJk9Y2YbisenxRjNrM3MfmFmTxXH96Xi8eVm\n9njxtfqfxbc7TiwzS5vZk2b2o+Lj6Ta+A2b2tJntNrOe4rFEv0aDCO6WbUgcv+9IWjvq2EZJP3P3\nt0v6WfFxUg1K+qy7XyBptaSbil+36TLGfkmXu/u7JK2QtNbMVkv6Z0n/4u6/L+lVSZ9sYY2TYYOk\n5yoeT7fxSdL73H1FRQtgol+jQQS3WrkhcYzcfYek3446fK2k7xY//q6k66a0qEnk7i+7F3YCdfcT\nKnzzL9U0GaMXnCw+zBb/uKTLJd1TPJ7Y8UmSmXVKukrSHcXHpmk0vjoS/RoNJbijNiRe2qJa4rbI\n3V8ufnxY0qJWFjNZzKxL0kpJj2sajbG4jLBb0hFJD0naL+m4uw8WT0n6a/Vrkj4nqbSZZrum1/ik\nwg/bn5jZruLeuFLCX6OTtlkwxs/d3cwS39ZjZnMl3SvpZnf/nVVsTpz0Mbr7kKQVZjZf0lZJ57e4\npEljZh+UdMTdd5nZZa2uJ0aXuvshMztH0kNmtrfyySS+RkOZcTe1IfE08YqZLZGk4t9HWlzPhJhZ\nVoXQ3uzuPygenlZjlCR3Py5pm6T3SppvZqVJT5Jfq2skXWNmB1RYnrxc0tc1fcYnSXL3Q8W/j6jw\nw3eVEv4aDSW4Z9KGxD+U9PHixx+XdF8La5mQ4nrotyU95+5frXhqWozRzDqKM22Z2SxJ71dhHX+b\npA8XT0vs+Nz98+7e6e5dKnzP/dzdb9A0GZ8kmdkcM5tX+ljSByTtUcJfo8HcgGNmV6qw3lbakPgr\nLS5pwszsbkmXqfBOZK9I+oKk/5K0RdIyFd5B8Xp3H/0LzEQws0sl/Y+kpzWyRvoPKqxzJ36MZvZO\nFX5xlVZhkrPF3f/JzH5PhRnq2ZKelHSju/e3rtKJKy6V3OLuH5xO4yuOZWvxYUbSXe7+FTNrV4Jf\no8EENwCgOaEslQAAmkRwA0DCENwAkDAENwAkDMENAAlDcANAwhDcAJAwBDcAJMz/A8K3EgUGda9p\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 2s 10ms/step - loss: 6.3910 - mean_squared_error: 6.3910 - val_loss: 0.5412 - val_mean_squared_error: 0.5412\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.54124, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0409 - mean_squared_error: 0.0409 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.54124 to 0.00177, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00177 to 0.00129, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00129\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00129\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00129\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00129\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00129\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00129\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00129\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00129\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00129\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.8698e-04 - mean_squared_error: 9.8698e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00129\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.6859e-04 - mean_squared_error: 9.6859e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00129\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.5214e-04 - mean_squared_error: 9.5214e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00129\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.3410e-04 - mean_squared_error: 9.3410e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00129\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.1133e-04 - mean_squared_error: 9.1133e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00129\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.8365e-04 - mean_squared_error: 8.8365e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00129\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.5258e-04 - mean_squared_error: 8.5258e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00129\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.2151e-04 - mean_squared_error: 8.2151e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00129\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.9398e-04 - mean_squared_error: 7.9398e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00129\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.7226e-04 - mean_squared_error: 7.7226e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00129\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.5690e-04 - mean_squared_error: 7.5690e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00129\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.4648e-04 - mean_squared_error: 7.4648e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00129\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.3854e-04 - mean_squared_error: 7.3854e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00129\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.2882e-04 - mean_squared_error: 7.2882e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00129\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.1583e-04 - mean_squared_error: 7.1583e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00129\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.9473e-04 - mean_squared_error: 6.9473e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00129\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.7029e-04 - mean_squared_error: 6.7029e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00129\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.4170e-04 - mean_squared_error: 6.4170e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00129\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.0997e-04 - mean_squared_error: 6.0997e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00129\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.7812e-04 - mean_squared_error: 5.7812e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.00129 to 0.00129, saving model to weights.best_mlp.hdf5\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.4698e-04 - mean_squared_error: 5.4698e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00129 to 0.00112, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.1972e-04 - mean_squared_error: 5.1972e-04 - val_loss: 9.7338e-04 - val_mean_squared_error: 9.7338e-04\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00112 to 0.00097, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.8939e-04 - mean_squared_error: 4.8939e-04 - val_loss: 8.3787e-04 - val_mean_squared_error: 8.3787e-04\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00097 to 0.00084, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.7037e-04 - mean_squared_error: 4.7037e-04 - val_loss: 7.2001e-04 - val_mean_squared_error: 7.2001e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00084 to 0.00072, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5651e-04 - mean_squared_error: 4.5651e-04 - val_loss: 6.1891e-04 - val_mean_squared_error: 6.1891e-04\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.00072 to 0.00062, saving model to weights.best_mlp.hdf5\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4332e-04 - mean_squared_error: 4.4332e-04 - val_loss: 5.4097e-04 - val_mean_squared_error: 5.4097e-04\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.00062 to 0.00054, saving model to weights.best_mlp.hdf5\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3637e-04 - mean_squared_error: 4.3637e-04 - val_loss: 4.8159e-04 - val_mean_squared_error: 4.8159e-04\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.00054 to 0.00048, saving model to weights.best_mlp.hdf5\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3021e-04 - mean_squared_error: 4.3021e-04 - val_loss: 4.3585e-04 - val_mean_squared_error: 4.3585e-04\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.00048 to 0.00044, saving model to weights.best_mlp.hdf5\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2537e-04 - mean_squared_error: 4.2537e-04 - val_loss: 4.0025e-04 - val_mean_squared_error: 4.0025e-04\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.00044 to 0.00040, saving model to weights.best_mlp.hdf5\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2223e-04 - mean_squared_error: 4.2223e-04 - val_loss: 3.6872e-04 - val_mean_squared_error: 3.6872e-04\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.00040 to 0.00037, saving model to weights.best_mlp.hdf5\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2163e-04 - mean_squared_error: 4.2163e-04 - val_loss: 3.3937e-04 - val_mean_squared_error: 3.3937e-04\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.00037 to 0.00034, saving model to weights.best_mlp.hdf5\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1969e-04 - mean_squared_error: 4.1969e-04 - val_loss: 3.1048e-04 - val_mean_squared_error: 3.1048e-04\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.00034 to 0.00031, saving model to weights.best_mlp.hdf5\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1872e-04 - mean_squared_error: 4.1872e-04 - val_loss: 2.8757e-04 - val_mean_squared_error: 2.8757e-04\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.00031 to 0.00029, saving model to weights.best_mlp.hdf5\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1628e-04 - mean_squared_error: 4.1628e-04 - val_loss: 2.7175e-04 - val_mean_squared_error: 2.7175e-04\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00029 to 0.00027, saving model to weights.best_mlp.hdf5\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1445e-04 - mean_squared_error: 4.1445e-04 - val_loss: 2.5520e-04 - val_mean_squared_error: 2.5520e-04\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.00027 to 0.00026, saving model to weights.best_mlp.hdf5\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1205e-04 - mean_squared_error: 4.1205e-04 - val_loss: 2.4418e-04 - val_mean_squared_error: 2.4418e-04\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.00026 to 0.00024, saving model to weights.best_mlp.hdf5\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0991e-04 - mean_squared_error: 4.0991e-04 - val_loss: 2.3816e-04 - val_mean_squared_error: 2.3816e-04\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.00024 to 0.00024, saving model to weights.best_mlp.hdf5\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0449e-04 - mean_squared_error: 4.0449e-04 - val_loss: 2.3497e-04 - val_mean_squared_error: 2.3497e-04\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.00024 to 0.00023, saving model to weights.best_mlp.hdf5\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9818e-04 - mean_squared_error: 3.9818e-04 - val_loss: 2.3008e-04 - val_mean_squared_error: 2.3008e-04\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.00023 to 0.00023, saving model to weights.best_mlp.hdf5\n",
            "Epoch 52/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8909e-04 - mean_squared_error: 3.8909e-04 - val_loss: 2.2827e-04 - val_mean_squared_error: 2.2827e-04\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.00023 to 0.00023, saving model to weights.best_mlp.hdf5\n",
            "Epoch 53/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9569e-04 - mean_squared_error: 3.9569e-04 - val_loss: 2.3038e-04 - val_mean_squared_error: 2.3038e-04\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.00023\n",
            "Epoch 54/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8712e-04 - mean_squared_error: 3.8712e-04 - val_loss: 2.2461e-04 - val_mean_squared_error: 2.2461e-04\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.00023 to 0.00022, saving model to weights.best_mlp.hdf5\n",
            "Epoch 55/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.8917e-04 - mean_squared_error: 4.8917e-04 - val_loss: 2.3811e-04 - val_mean_squared_error: 2.3811e-04\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.00022\n",
            "Epoch 56/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.6072e-04 - mean_squared_error: 4.6072e-04 - val_loss: 2.2998e-04 - val_mean_squared_error: 2.2998e-04\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.00022\n",
            "Epoch 57/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.6281e-04 - mean_squared_error: 4.6281e-04 - val_loss: 2.3820e-04 - val_mean_squared_error: 2.3820e-04\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.00022\n",
            "Epoch 58/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8148e-04 - mean_squared_error: 3.8148e-04 - val_loss: 2.2978e-04 - val_mean_squared_error: 2.2978e-04\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.00022\n",
            "Epoch 59/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0724e-04 - mean_squared_error: 4.0724e-04 - val_loss: 2.3467e-04 - val_mean_squared_error: 2.3467e-04\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.00022\n",
            "Epoch 60/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.5382e-04 - mean_squared_error: 3.5382e-04 - val_loss: 2.3052e-04 - val_mean_squared_error: 2.3052e-04\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.00022\n",
            "Epoch 61/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.8470e-04 - mean_squared_error: 3.8470e-04 - val_loss: 2.3237e-04 - val_mean_squared_error: 2.3237e-04\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.00022\n",
            "Epoch 62/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.4077e-04 - mean_squared_error: 3.4077e-04 - val_loss: 2.2671e-04 - val_mean_squared_error: 2.2671e-04\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.00022\n",
            "Epoch 63/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0636e-04 - mean_squared_error: 4.0636e-04 - val_loss: 2.3656e-04 - val_mean_squared_error: 2.3656e-04\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.00022\n",
            "Epoch 64/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.4738e-04 - mean_squared_error: 3.4738e-04 - val_loss: 2.2721e-04 - val_mean_squared_error: 2.2721e-04\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.00022\n",
            "Epoch 65/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2216e-04 - mean_squared_error: 4.2216e-04 - val_loss: 2.4196e-04 - val_mean_squared_error: 2.4196e-04\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.00022\n",
            "Epoch 66/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.4572e-04 - mean_squared_error: 3.4572e-04 - val_loss: 2.3350e-04 - val_mean_squared_error: 2.3350e-04\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.00022\n",
            "Epoch 00066: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 100\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 4\n",
            "MLP: Train: 0.011525, Validation: 0.000233\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGUNwA0DGENwAkDH/D3XwZn2KtHXOAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 2s 10ms/step - loss: 7.1089 - mean_squared_error: 7.1089 - val_loss: 0.9934 - val_mean_squared_error: 0.9934\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.99337, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0851 - mean_squared_error: 0.0851 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.99337 to 0.00163, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00163 to 0.00132, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00132\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00132\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00132\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00132\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00132\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00132\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00132\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00132\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00132\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00132\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.8201e-04 - mean_squared_error: 9.8201e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00132\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.6620e-04 - mean_squared_error: 9.6620e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00132\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.4963e-04 - mean_squared_error: 9.4963e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00132\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.2945e-04 - mean_squared_error: 9.2945e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00132\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.0390e-04 - mean_squared_error: 9.0390e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00132\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.7450e-04 - mean_squared_error: 8.7450e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00132\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.4404e-04 - mean_squared_error: 8.4404e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00132\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.1492e-04 - mean_squared_error: 8.1492e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00132\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.9254e-04 - mean_squared_error: 7.9254e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00132\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.7382e-04 - mean_squared_error: 7.7382e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00132\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.6211e-04 - mean_squared_error: 7.6211e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00132\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.5270e-04 - mean_squared_error: 7.5270e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00132\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.4389e-04 - mean_squared_error: 7.4389e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00132\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.3348e-04 - mean_squared_error: 7.3348e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00132\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.2027e-04 - mean_squared_error: 7.2027e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00132\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.9751e-04 - mean_squared_error: 6.9751e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00132\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.7373e-04 - mean_squared_error: 6.7373e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00132\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.4519e-04 - mean_squared_error: 6.4519e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00132\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.1717e-04 - mean_squared_error: 6.1717e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00132\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.8554e-04 - mean_squared_error: 5.8554e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00132 to 0.00130, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.5649e-04 - mean_squared_error: 5.5649e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00130 to 0.00113, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.2807e-04 - mean_squared_error: 5.2807e-04 - val_loss: 9.7679e-04 - val_mean_squared_error: 9.7679e-04\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00113 to 0.00098, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.0466e-04 - mean_squared_error: 5.0466e-04 - val_loss: 8.5930e-04 - val_mean_squared_error: 8.5930e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00098 to 0.00086, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.9409e-04 - mean_squared_error: 4.9409e-04 - val_loss: 7.4744e-04 - val_mean_squared_error: 7.4744e-04\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.00086 to 0.00075, saving model to weights.best_mlp.hdf5\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.7531e-04 - mean_squared_error: 4.7531e-04 - val_loss: 6.5809e-04 - val_mean_squared_error: 6.5809e-04\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.00075 to 0.00066, saving model to weights.best_mlp.hdf5\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.6629e-04 - mean_squared_error: 4.6629e-04 - val_loss: 5.8439e-04 - val_mean_squared_error: 5.8439e-04\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.00066 to 0.00058, saving model to weights.best_mlp.hdf5\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.5758e-04 - mean_squared_error: 4.5758e-04 - val_loss: 5.2846e-04 - val_mean_squared_error: 5.2846e-04\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.00058 to 0.00053, saving model to weights.best_mlp.hdf5\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5109e-04 - mean_squared_error: 4.5109e-04 - val_loss: 4.8976e-04 - val_mean_squared_error: 4.8976e-04\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.00053 to 0.00049, saving model to weights.best_mlp.hdf5\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4424e-04 - mean_squared_error: 4.4424e-04 - val_loss: 4.5719e-04 - val_mean_squared_error: 4.5719e-04\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.00049 to 0.00046, saving model to weights.best_mlp.hdf5\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4022e-04 - mean_squared_error: 4.4022e-04 - val_loss: 4.3489e-04 - val_mean_squared_error: 4.3489e-04\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.00046 to 0.00043, saving model to weights.best_mlp.hdf5\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3825e-04 - mean_squared_error: 4.3825e-04 - val_loss: 4.1794e-04 - val_mean_squared_error: 4.1794e-04\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.00043 to 0.00042, saving model to weights.best_mlp.hdf5\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3523e-04 - mean_squared_error: 4.3523e-04 - val_loss: 4.0407e-04 - val_mean_squared_error: 4.0407e-04\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.00042 to 0.00040, saving model to weights.best_mlp.hdf5\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3421e-04 - mean_squared_error: 4.3421e-04 - val_loss: 3.8380e-04 - val_mean_squared_error: 3.8380e-04\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00040 to 0.00038, saving model to weights.best_mlp.hdf5\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3239e-04 - mean_squared_error: 4.3239e-04 - val_loss: 3.7367e-04 - val_mean_squared_error: 3.7367e-04\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.00038 to 0.00037, saving model to weights.best_mlp.hdf5\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3121e-04 - mean_squared_error: 4.3121e-04 - val_loss: 3.5927e-04 - val_mean_squared_error: 3.5927e-04\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.00037 to 0.00036, saving model to weights.best_mlp.hdf5\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3088e-04 - mean_squared_error: 4.3088e-04 - val_loss: 3.4663e-04 - val_mean_squared_error: 3.4663e-04\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.00036 to 0.00035, saving model to weights.best_mlp.hdf5\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2804e-04 - mean_squared_error: 4.2804e-04 - val_loss: 3.3815e-04 - val_mean_squared_error: 3.3815e-04\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.00035 to 0.00034, saving model to weights.best_mlp.hdf5\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3226e-04 - mean_squared_error: 4.3226e-04 - val_loss: 3.3142e-04 - val_mean_squared_error: 3.3142e-04\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.00034 to 0.00033, saving model to weights.best_mlp.hdf5\n",
            "Epoch 52/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3310e-04 - mean_squared_error: 4.3310e-04 - val_loss: 3.1773e-04 - val_mean_squared_error: 3.1773e-04\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.00033 to 0.00032, saving model to weights.best_mlp.hdf5\n",
            "Epoch 00052: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 100\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 2\n",
            "MLP: Train: 0.011307, Validation: 0.000318\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n67YjBeMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 2s 11ms/step - loss: 6.2807 - mean_squared_error: 6.2807 - val_loss: 0.4611 - val_mean_squared_error: 0.4611\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.46107, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0350 - mean_squared_error: 0.0350 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.46107 to 0.00214, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00214 to 0.00169, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00169 to 0.00167, saving model to weights.best_mlp.hdf5\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00167\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00167\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00167\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00167\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00167\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00167\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00167\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.8656e-04 - mean_squared_error: 9.8656e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00167\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.5896e-04 - mean_squared_error: 9.5896e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00167\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.3701e-04 - mean_squared_error: 9.3701e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00167\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.1640e-04 - mean_squared_error: 9.1640e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00167\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.9250e-04 - mean_squared_error: 8.9250e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00167\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.6414e-04 - mean_squared_error: 8.6414e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00167\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.3222e-04 - mean_squared_error: 8.3222e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00167\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.9979e-04 - mean_squared_error: 7.9979e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00167\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.7043e-04 - mean_squared_error: 7.7043e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00167\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.4723e-04 - mean_squared_error: 7.4723e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00167\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.2984e-04 - mean_squared_error: 7.2984e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00167\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.1970e-04 - mean_squared_error: 7.1970e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00167\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.1166e-04 - mean_squared_error: 7.1166e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00167\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.0246e-04 - mean_squared_error: 7.0246e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00167\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.9186e-04 - mean_squared_error: 6.9186e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00167\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.7494e-04 - mean_squared_error: 6.7494e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00167\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.5169e-04 - mean_squared_error: 6.5169e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00167\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.2639e-04 - mean_squared_error: 6.2639e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00167\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.9969e-04 - mean_squared_error: 5.9969e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00167\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.6662e-04 - mean_squared_error: 5.6662e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.00167 to 0.00148, saving model to weights.best_mlp.hdf5\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.4048e-04 - mean_squared_error: 5.4048e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.00148 to 0.00126, saving model to weights.best_mlp.hdf5\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.0794e-04 - mean_squared_error: 5.0794e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00126 to 0.00110, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.8209e-04 - mean_squared_error: 4.8209e-04 - val_loss: 9.3888e-04 - val_mean_squared_error: 9.3888e-04\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00110 to 0.00094, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5740e-04 - mean_squared_error: 4.5740e-04 - val_loss: 8.0203e-04 - val_mean_squared_error: 8.0203e-04\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00094 to 0.00080, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4084e-04 - mean_squared_error: 4.4084e-04 - val_loss: 6.8010e-04 - val_mean_squared_error: 6.8010e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00080 to 0.00068, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.2682e-04 - mean_squared_error: 4.2682e-04 - val_loss: 5.7932e-04 - val_mean_squared_error: 5.7932e-04\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.00068 to 0.00058, saving model to weights.best_mlp.hdf5\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1891e-04 - mean_squared_error: 4.1891e-04 - val_loss: 5.0111e-04 - val_mean_squared_error: 5.0111e-04\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.00058 to 0.00050, saving model to weights.best_mlp.hdf5\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1140e-04 - mean_squared_error: 4.1140e-04 - val_loss: 4.4807e-04 - val_mean_squared_error: 4.4807e-04\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.00050 to 0.00045, saving model to weights.best_mlp.hdf5\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0655e-04 - mean_squared_error: 4.0655e-04 - val_loss: 4.0154e-04 - val_mean_squared_error: 4.0154e-04\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.00045 to 0.00040, saving model to weights.best_mlp.hdf5\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0372e-04 - mean_squared_error: 4.0372e-04 - val_loss: 3.6149e-04 - val_mean_squared_error: 3.6149e-04\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.00040 to 0.00036, saving model to weights.best_mlp.hdf5\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.0254e-04 - mean_squared_error: 4.0254e-04 - val_loss: 3.2709e-04 - val_mean_squared_error: 3.2709e-04\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.00036 to 0.00033, saving model to weights.best_mlp.hdf5\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0141e-04 - mean_squared_error: 4.0141e-04 - val_loss: 2.9678e-04 - val_mean_squared_error: 2.9678e-04\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.00033 to 0.00030, saving model to weights.best_mlp.hdf5\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9957e-04 - mean_squared_error: 3.9957e-04 - val_loss: 2.7304e-04 - val_mean_squared_error: 2.7304e-04\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.00030 to 0.00027, saving model to weights.best_mlp.hdf5\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9602e-04 - mean_squared_error: 3.9602e-04 - val_loss: 2.5401e-04 - val_mean_squared_error: 2.5401e-04\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.00027 to 0.00025, saving model to weights.best_mlp.hdf5\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9329e-04 - mean_squared_error: 3.9329e-04 - val_loss: 2.4098e-04 - val_mean_squared_error: 2.4098e-04\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00025 to 0.00024, saving model to weights.best_mlp.hdf5\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.8544e-04 - mean_squared_error: 3.8544e-04 - val_loss: 2.3240e-04 - val_mean_squared_error: 2.3240e-04\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.00024 to 0.00023, saving model to weights.best_mlp.hdf5\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.7671e-04 - mean_squared_error: 3.7671e-04 - val_loss: 2.2504e-04 - val_mean_squared_error: 2.2504e-04\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.00023 to 0.00023, saving model to weights.best_mlp.hdf5\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.6483e-04 - mean_squared_error: 3.6483e-04 - val_loss: 2.2128e-04 - val_mean_squared_error: 2.2128e-04\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.00023 to 0.00022, saving model to weights.best_mlp.hdf5\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.4717e-04 - mean_squared_error: 3.4717e-04 - val_loss: 2.2217e-04 - val_mean_squared_error: 2.2217e-04\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.00022\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.4813e-04 - mean_squared_error: 3.4813e-04 - val_loss: 2.2049e-04 - val_mean_squared_error: 2.2049e-04\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.00022 to 0.00022, saving model to weights.best_mlp.hdf5\n",
            "Epoch 52/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.4129e-04 - mean_squared_error: 3.4129e-04 - val_loss: 2.2783e-04 - val_mean_squared_error: 2.2783e-04\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.00022\n",
            "Epoch 53/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.2457e-04 - mean_squared_error: 3.2457e-04 - val_loss: 2.1928e-04 - val_mean_squared_error: 2.1928e-04\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.00022 to 0.00022, saving model to weights.best_mlp.hdf5\n",
            "Epoch 54/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.7368e-04 - mean_squared_error: 3.7368e-04 - val_loss: 2.2363e-04 - val_mean_squared_error: 2.2363e-04\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.00022\n",
            "Epoch 55/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.3983e-04 - mean_squared_error: 3.3983e-04 - val_loss: 2.2731e-04 - val_mean_squared_error: 2.2731e-04\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.00022\n",
            "Epoch 56/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.7784e-04 - mean_squared_error: 3.7784e-04 - val_loss: 2.4290e-04 - val_mean_squared_error: 2.4290e-04\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.00022\n",
            "Epoch 00056: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 100\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 3\n",
            "MLP: Train: 0.011540, Validation: 0.000243\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDQApQ3ADQMoQ3ACQMgQ3AKTM/wO0nCQ5Wni44wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 2s 11ms/step - loss: 6.9664 - mean_squared_error: 6.9664 - val_loss: 0.9982 - val_mean_squared_error: 0.9982\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.99817, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0925 - mean_squared_error: 0.0925 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.99817 to 0.00273, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00273 to 0.00198, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00198 to 0.00177, saving model to weights.best_mlp.hdf5\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00177 to 0.00175, saving model to weights.best_mlp.hdf5\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00175\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00175\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00175\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00175\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00175\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00175\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00175\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.8570e-04 - mean_squared_error: 9.8570e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00175\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.6136e-04 - mean_squared_error: 9.6136e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00175\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.4398e-04 - mean_squared_error: 9.4398e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00175\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.2844e-04 - mean_squared_error: 9.2844e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00175\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.1094e-04 - mean_squared_error: 9.1094e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00175\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.8919e-04 - mean_squared_error: 8.8919e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00175\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.6305e-04 - mean_squared_error: 8.6305e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00175\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.3410e-04 - mean_squared_error: 8.3410e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00175\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.0625e-04 - mean_squared_error: 8.0625e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00175\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.8152e-04 - mean_squared_error: 7.8152e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00175\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.6295e-04 - mean_squared_error: 7.6295e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00175\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.5053e-04 - mean_squared_error: 7.5053e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00175\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.4090e-04 - mean_squared_error: 7.4090e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00175\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.3369e-04 - mean_squared_error: 7.3369e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00175\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.2502e-04 - mean_squared_error: 7.2502e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00175\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.1326e-04 - mean_squared_error: 7.1326e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00175\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.9764e-04 - mean_squared_error: 6.9764e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00175\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.7763e-04 - mean_squared_error: 6.7763e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00175\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.5375e-04 - mean_squared_error: 6.5375e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00175\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.2804e-04 - mean_squared_error: 6.2804e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.00175 to 0.00161, saving model to weights.best_mlp.hdf5\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.0444e-04 - mean_squared_error: 6.0444e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00161 to 0.00142, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.7882e-04 - mean_squared_error: 5.7882e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00142 to 0.00125, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.4821e-04 - mean_squared_error: 5.4821e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00125 to 0.00110, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.2158e-04 - mean_squared_error: 5.2158e-04 - val_loss: 9.7172e-04 - val_mean_squared_error: 9.7172e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00110 to 0.00097, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.0087e-04 - mean_squared_error: 5.0087e-04 - val_loss: 8.5783e-04 - val_mean_squared_error: 8.5783e-04\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.00097 to 0.00086, saving model to weights.best_mlp.hdf5\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.7977e-04 - mean_squared_error: 4.7977e-04 - val_loss: 7.5634e-04 - val_mean_squared_error: 7.5634e-04\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.00086 to 0.00076, saving model to weights.best_mlp.hdf5\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.6230e-04 - mean_squared_error: 4.6230e-04 - val_loss: 6.6943e-04 - val_mean_squared_error: 6.6943e-04\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.00076 to 0.00067, saving model to weights.best_mlp.hdf5\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4660e-04 - mean_squared_error: 4.4660e-04 - val_loss: 5.9271e-04 - val_mean_squared_error: 5.9271e-04\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.00067 to 0.00059, saving model to weights.best_mlp.hdf5\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3266e-04 - mean_squared_error: 4.3266e-04 - val_loss: 5.3367e-04 - val_mean_squared_error: 5.3367e-04\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.00059 to 0.00053, saving model to weights.best_mlp.hdf5\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2394e-04 - mean_squared_error: 4.2394e-04 - val_loss: 4.8102e-04 - val_mean_squared_error: 4.8102e-04\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.00053 to 0.00048, saving model to weights.best_mlp.hdf5\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1841e-04 - mean_squared_error: 4.1841e-04 - val_loss: 4.4435e-04 - val_mean_squared_error: 4.4435e-04\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.00048 to 0.00044, saving model to weights.best_mlp.hdf5\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1286e-04 - mean_squared_error: 4.1286e-04 - val_loss: 4.0898e-04 - val_mean_squared_error: 4.0898e-04\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.00044 to 0.00041, saving model to weights.best_mlp.hdf5\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0813e-04 - mean_squared_error: 4.0813e-04 - val_loss: 3.8237e-04 - val_mean_squared_error: 3.8237e-04\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.00041 to 0.00038, saving model to weights.best_mlp.hdf5\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.0654e-04 - mean_squared_error: 4.0654e-04 - val_loss: 3.5259e-04 - val_mean_squared_error: 3.5259e-04\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00038 to 0.00035, saving model to weights.best_mlp.hdf5\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.0582e-04 - mean_squared_error: 4.0582e-04 - val_loss: 3.3221e-04 - val_mean_squared_error: 3.3221e-04\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.00035 to 0.00033, saving model to weights.best_mlp.hdf5\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0747e-04 - mean_squared_error: 4.0747e-04 - val_loss: 3.1486e-04 - val_mean_squared_error: 3.1486e-04\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.00033 to 0.00031, saving model to weights.best_mlp.hdf5\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0299e-04 - mean_squared_error: 4.0299e-04 - val_loss: 3.0120e-04 - val_mean_squared_error: 3.0120e-04\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.00031 to 0.00030, saving model to weights.best_mlp.hdf5\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.0648e-04 - mean_squared_error: 4.0648e-04 - val_loss: 2.8417e-04 - val_mean_squared_error: 2.8417e-04\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.00030 to 0.00028, saving model to weights.best_mlp.hdf5\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0837e-04 - mean_squared_error: 4.0837e-04 - val_loss: 2.7539e-04 - val_mean_squared_error: 2.7539e-04\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.00028 to 0.00028, saving model to weights.best_mlp.hdf5\n",
            "Epoch 52/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0604e-04 - mean_squared_error: 4.0604e-04 - val_loss: 2.6589e-04 - val_mean_squared_error: 2.6589e-04\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.00028 to 0.00027, saving model to weights.best_mlp.hdf5\n",
            "Epoch 53/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0216e-04 - mean_squared_error: 4.0216e-04 - val_loss: 2.5921e-04 - val_mean_squared_error: 2.5921e-04\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.00027 to 0.00026, saving model to weights.best_mlp.hdf5\n",
            "Epoch 54/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1278e-04 - mean_squared_error: 4.1278e-04 - val_loss: 2.5230e-04 - val_mean_squared_error: 2.5230e-04\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.00026 to 0.00025, saving model to weights.best_mlp.hdf5\n",
            "Epoch 55/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.9602e-04 - mean_squared_error: 3.9602e-04 - val_loss: 2.5104e-04 - val_mean_squared_error: 2.5104e-04\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.00025 to 0.00025, saving model to weights.best_mlp.hdf5\n",
            "Epoch 56/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0805e-04 - mean_squared_error: 4.0805e-04 - val_loss: 2.4254e-04 - val_mean_squared_error: 2.4254e-04\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.00025 to 0.00024, saving model to weights.best_mlp.hdf5\n",
            "Epoch 57/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.0274e-04 - mean_squared_error: 4.0274e-04 - val_loss: 2.4722e-04 - val_mean_squared_error: 2.4722e-04\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.00024\n",
            "Epoch 58/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2327e-04 - mean_squared_error: 4.2327e-04 - val_loss: 2.3992e-04 - val_mean_squared_error: 2.3992e-04\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.00024 to 0.00024, saving model to weights.best_mlp.hdf5\n",
            "Epoch 59/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.8898e-04 - mean_squared_error: 3.8898e-04 - val_loss: 2.4181e-04 - val_mean_squared_error: 2.4181e-04\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.00024\n",
            "Epoch 60/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.4611e-04 - mean_squared_error: 4.4611e-04 - val_loss: 2.3654e-04 - val_mean_squared_error: 2.3654e-04\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.00024 to 0.00024, saving model to weights.best_mlp.hdf5\n",
            "Epoch 61/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.8015e-04 - mean_squared_error: 3.8015e-04 - val_loss: 2.3831e-04 - val_mean_squared_error: 2.3831e-04\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.00024\n",
            "Epoch 62/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4392e-04 - mean_squared_error: 4.4392e-04 - val_loss: 2.4050e-04 - val_mean_squared_error: 2.4050e-04\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.00024\n",
            "Epoch 63/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.8899e-04 - mean_squared_error: 3.8899e-04 - val_loss: 2.3442e-04 - val_mean_squared_error: 2.3442e-04\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.00024 to 0.00023, saving model to weights.best_mlp.hdf5\n",
            "Epoch 64/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.7952e-04 - mean_squared_error: 4.7952e-04 - val_loss: 2.4046e-04 - val_mean_squared_error: 2.4046e-04\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.00023\n",
            "Epoch 65/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.8692e-04 - mean_squared_error: 3.8692e-04 - val_loss: 2.3522e-04 - val_mean_squared_error: 2.3522e-04\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.00023\n",
            "Epoch 00065: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 100\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 4\n",
            "MLP: Train: 0.011547, Validation: 0.000235\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAFj1JREFUeJzt3XuMXOV5x/HfMzNnvLYx2NhroF7c\ndVSwuTUGNtQpKCKgRAbCRUqIE0GVtpHcP2iAKFFkFDVN2kilrZQGqlDqJCSVyiXEhJIiCOFiB6UB\n0jU4YcEGY2rqNZcdu9jYxGvv5ekfc3Y9u3tm5tjrs/Me+/uRVjuXM2eeXca/fXne95xj7i4AQH4U\nWl0AAODQENwAkDMENwDkDMENADlDcANAzhDcAJAzBDcA5AzBDQA5Q3ADQM6UstjpvHnzvLOzM4td\nA8BRaf369TvcvT3NtpkEd2dnp7q7u7PYNQAclczsjbTb0ioBgJwhuAEgZwhuAMiZTHrcAHCoBgYG\n1Nvbq/7+/laXkqm2tjZ1dHQoiqLD3kfT4DazxZJ+VPPQByR9zd2/fdjvCgDj9Pb2atasWers7JSZ\ntbqcTLi7du7cqd7eXi1atOiw99M0uN39FUlLJcnMipK2S3rwsN8RABL09/cf1aEtSWamuXPnqlKp\nTGo/h9rjvlTSFndPvWwFANI6mkN7xJH4GQ81uD8j6d46xaw0s24z6z7cvya3P7lZv3h1cn+JAOBo\nlzq4zaws6SpJP0563t1Xu3uXu3e1t6c6+GeCO3+xRb/cTHADmHq7du3SHXfcccivu/zyy7Vr164M\nKqrvUEbcl0l63t3fyaqYqFjQwBAXLwYw9eoF9+DgYMPXPfLII5o9e3ZWZSU6lOWAn1WdNsmREhUL\n2j84nOVbAECiVatWacuWLVq6dKmiKFJbW5vmzJmjTZs26dVXX9U111yjbdu2qb+/XzfddJNWrlwp\n6eApPvbu3avLLrtMF110kX71q19pwYIFeuihhzR9+vQjXmuq4DazmZI+JukvjngFNcpF08AQwQ0c\n677xny/p5TffO6L7PPP3jtdfX3lW3edvvfVW9fT0aMOGDVq3bp2uuOIK9fT0jC7bu+uuu3TiiSdq\n3759+tCHPqRPfvKTmjt37ph9bN68Wffee6+++93v6tOf/rQeeOABXX/99Uf055BSBre7vy9pbtMN\nJ6lcKhDcAIJwwQUXjFlrffvtt+vBB6srobdt26bNmzdPCO5FixZp6dKlkqTzzz9fW7duzaS2oI6c\nrPa4CW7gWNdoZDxVZs6cOXp73bp1euKJJ/TMM89oxowZuvjiixOP8Jw2bdro7WKxqH379mVSW1Dn\nKomKBR0YZHISwNSbNWuW9uzZk/jc7t27NWfOHM2YMUObNm3Ss88+O8XVjRXWiLtU0AFG3ABaYO7c\nubrwwgt19tlna/r06TrppJNGn1u+fLnuvPNOnXHGGVq8eLGWLVvWwkoDC+5pxYIGWFUCoEXuueee\nxMenTZumRx99NPG5kT72vHnz1NPTM/r4l7/85SNe34iwWiUlVpUAQDNhBTeTkwDQVHDBzQE4ANBY\nUMHNOm4AaC6s4OZcJQDQVFDBHXHIOwA0FVhwF3SAHjeAHDjuuONa9t7hBTcjbgBoKKwDcJicBNAi\nq1at0qmnnqobbrhBkvT1r39dpVJJa9eu1bvvvquBgQF985vf1NVXX93iSgMLbi6kAECS9Ogq6e0X\nj+w+Tz5HuuzWuk+vWLFCN99882hw33///Xrsscd044036vjjj9eOHTu0bNkyXXXVVS2/NmZwwT00\n7BoadhULR/9FQwGE49xzz1VfX5/efPNNVSoVzZkzRyeffLK++MUv6umnn1ahUND27dv1zjvv6OST\nT25prWEFd6ka1gNDwyoWii2uBkDLNBgZZ+naa6/VmjVr9Pbbb2vFihW6++67ValUtH79ekVRpM7O\nzsTTuU61oCYny8VqOUxQAmiFFStW6L777tOaNWt07bXXavfu3Zo/f76iKNLatWv1xhtvtLpESYGN\nuMulanBzhkAArXDWWWdpz549WrBggU455RRdd911uvLKK3XOOeeoq6tLS5YsaXWJkgIL7ogRN4AW\ne/HFg5Oi8+bN0zPPPJO43d69e6eqpAlStUrMbLaZrTGzTWa20cw+nEUxI8E9wFVwAKCutCPu2yT9\nzN0/ZWZlSTOyKGakVcKIGwDqaxrcZnaCpI9I+lNJcvcDkg5kUUy5eHBVCYBjj7u3fI101twn31FI\n0ypZJKki6Qdm9oKZfc/MZjZ70eEY7XEzOQkcc9ra2rRz584jEmyhcnft3LlTbW1tk9pPmlZJSdJ5\nkr7g7s+Z2W2SVkn6q9qNzGylpJWStHDhwsMqZrTHzYgbOOZ0dHSot7dXlUql1aVkqq2tTR0dHZPa\nR5rg7pXU6+7PxffXqBrcY7j7akmrJamrq+uw/mTS4waOXVEUadGiRa0uIxeatkrc/W1J28xscfzQ\npZJezqKYgyPuo/d/lQBgstKuKvmCpLvjFSWvS/qzLIop0+MGgKZSBbe7b5DUlXEtY85VAgBIFuS5\nSghuAKgvqOBmOSAANBdUcLOqBACaCyq4D56rhOAGgHqCCu7R07qyHBAA6goquKP4XCW0SgCgvrCC\nu8DkJAA0E1RwFwqmUsFYDggADQQV3FK1z01wA0B9wQV3VCwwOQkADQQZ3PvpcQNAXcEFd7lIjxsA\nGgkvuOlxA0BDwQV3tcdNcANAPUEGN+u4AaC+8IK7VNABVpUAQF3BBfe0YoGTTAFAA8EFd1RiVQkA\nNBJecDM5CQANpbrmpJltlbRH0pCkQXfP7PqTHIADAI2lvcq7JH3U3XdkVkmMddwA0FhwrZIy5yoB\ngIbSBrdL+rmZrTezlVkWFHHIOwA0lLZVcpG7bzez+ZIeN7NN7v507QZxoK+UpIULFx52QRyAAwCN\npRpxu/v2+HufpAclXZCwzWp373L3rvb29sMuqFwqcOkyAGigaXCb2UwzmzVyW9LHJfVkVVCZ5YAA\n0FCaVslJkh40s5Ht73H3n2VVEBdSAIDGmga3u78u6YNTUIukanAPDbuGhl3Fgk3V2wJAboS3HLBU\nLYl2CQAkCy64o2J1lM0EJQAkCy64R0fcLAkEgETBBXdUrJbEiBsAkgUX3OXiyIiblSUAkCS44I5K\njLgBoJHggrscT06yqgQAkgUX3KM9biYnASBRcMHNOm4AaCy44GZVCQA0Fmxwc74SAEgWXHCX6XED\nQEPhBTc9bgBoKLjgjlgOCAANBRjctEoAoJHggrvMkZMA0FBwwT26qoQRNwAkCi64D05OshwQAJIE\nF9xcSAEAGgsvuAtMTgJAI6mD28yKZvaCmT2caUEFU6lgLAcEgDoOZcR9k6SNWRVSq1wqENwAUEeq\n4DazDklXSPpetuVURcUCk5MAUEfaEfe3JX1FUt1hsJmtNLNuM+uuVCqTKioqFrSfHjcAJGoa3Gb2\nCUl97r6+0Xbuvtrdu9y9q729fVJFlYv0uAGgnjQj7gslXWVmWyXdJ+kSM/v3LIuixw0A9TUNbne/\nxd073L1T0mckPeXu12dZVLXHTXADQJLg1nFL1eBmHTcAJCsdysbuvk7SukwqqRGVCjrAqhIASBTk\niHtascBJpgCgjiCDOyqxqgQA6gkzuIsFTjIFAHWEG9y0SgAgUZDBzTpuAKgvzODmXCUAUFeQwR0V\njVYJANQRaHDTKgGAeoIM7nKJVSUAUE+Ywc2IGwDqCjK4uZACANQXbHAPDbuGhglvABgvyOAul6pl\n0S4BgImCDO6oaJLEBCUAJAgyuEdH3KzlBoAJggzuqFgtixE3AEwUZHCXiyMjbiYnAWC8IIM7KjHi\nBoB6ggzucjw5yaoSAJioaXCbWZuZ/drMfmNmL5nZN7IuarTHzeQkAEyQ5mLB+yVd4u57zSyS9Esz\ne9Tdn82qKNZxA0B9TYPb3V3S3vhuFH9lOmvIqhIAqC9Vj9vMima2QVKfpMfd/bksixoJbs5XAgAT\npQpudx9y96WSOiRdYGZnj9/GzFaaWbeZdVcqlUkVVabHDQB1HdKqEnffJWmtpOUJz6129y5372pv\nb59UUfS4AaC+NKtK2s1sdnx7uqSPSdqUZVERywEBoK40q0pOkfRvZlZUNejvd/eHsyyK5YAAUF+a\nVSW/lXTuFNQyqsyRkwBQV6BHTnJ2QACoJ8jgjkosBwSAesIMbi6kAAB1hRncBSYnAaCeIIO7UDBF\nRWM5IAAkCDK4peqSQIIbACYKPLiZnASA8YIO7v30uAFggmCDe1qJVgkAJAk2uJmcBIBkAQc3I24A\nSBJ0cLOOGwAmCja4y6WCDrCqBAAmCDe4iwVOMgUACYIN7qjE5CQAJAk3uIsFTjIFAAmCDe4yk5MA\nkCjY4I44AAcAEgUb3GXOVQIAiYIN7qhotEoAIEHT4DazU81srZm9bGYvmdlNU1FYmVYJACRqepV3\nSYOSvuTuz5vZLEnrzexxd385y8JYVQIAyZqOuN39LXd/Pr69R9JGSQuyLqzMuUoAINEh9bjNrFPS\nuZKeS3hupZl1m1l3pVKZdGGcqwQAkqUObjM7TtIDkm529/fGP+/uq929y9272tvbJ11YVCxo2KWh\nYVaWAECtVMFtZpGqoX23u/8k25KqyqVqabRLAGCsNKtKTNL3JW10929lX1JVVDRJYoISAMZJM+K+\nUNKfSLrEzDbEX5dnXNfoiJs+NwCM1XQ5oLv/UpJNQS1jREVaJQCQJNgjJ8sjwT3I5CQA1Ao2uKOR\nVgkjbgAYI9jgLseTk7RKAGCsYIN7pMfN5CQAjBVscLOOGwCSBRvcoyNughsAxgg+uLmYAgCMFWxw\nl+lxA0CicIObHjcAJAo2uCOWAwJAooCDm1YJACQJNrjLHDkJAInCDe7Rc5UQ3ABQK9jgjkosBwSA\nJOEGNxdSAIBE4QZ3gclJAEgSbHAXCqaoaCwHBIBxgg1uqbokkOAGgLFyENxMTgJArTRXeb/LzPrM\nrGcqCqoVFQvaT48bAMZIM+L+oaTlGdeRaFqJVgkAjNc0uN39aUn/l3kl7tKubdJ7b40+xOQkAEwU\nTo97eEj65/OkZ+8YfYjJSQCY6IgFt5mtNLNuM+uuVCqHvoNiSZp7mlTZNPpQVCywjhsAxjliwe3u\nq929y9272tvbD28n85dIfQeDu1wq6ACrSgBgjHBaJZI0/wxp9/9K+/dKqp5oipNMAcBYaZYD3ivp\nGUmLzazXzD6fWTXtZ1S/V16RJEUlJicBYLxSsw3c/bNTUYik6ohbkiobpY7zFRUL2tM/OGVvDwB5\nEFarZE6nVGqT+jZKqrZKmJwEgLHCCu5CUZp32mhwRxyAAwAThBXcUrXPHS8JLHOuEgCYILzgnr9E\nem+71L9bUdFolQDAOOEFd83KkjKtEgCYILzgnr+k+r1vY/XISYIbAMYIL7hnd0ql6VJlU9zjJrgB\noFZ4wV0oSO2nHxxx0+MGgDHCC25Jmn9mdcRdKmjYpaFhVpYAwIgwg7t9ibTnLc0cfl+SaJcAQI0w\ngzs+9H1+/+uSxAQlANQIM7jbqytL5u2Lg5s+NwCMCjO4TzhVimbqxN9Vg5tWCQAcFGZwFwpS+2LN\n2fuaJGlgkMlJABgRZnBL0vwzdPyeLZLocQNArXCDu32J2vbv0GztoccNADXCDe54Zcnp1kuPGwBq\nhBvc8cqS0wsENwDUCje4T+jQYHScTrNeetwAUCNVcJvZcjN7xcxeM7NVWRcVv6n6Z5+m02273t8/\nNCVvCQB5kOYq70VJ35F0maQzJX3WzM7MujBJKp50hk4v9OrGe1/QPz62Sbv3DUzF2wJA0NKMuC+Q\n9Jq7v+7uByTdJ+nqbMuqmr7gbM2193TN6dP0nbVb9JF/WKt//cUW9Q8wAgdw7Cql2GaBpG0193sl\n/VE25YwTT1D+3Zt/rr85saS9+4fU/5Tr3ackmcUbmUZuNTpMxxJuNXqo1Szhpxn/WHLZXvO8j9lu\n7Oub78v88A98cpu4x6R39IR39gbb+Oj3xIrrbn/wfv3X1a+13mvTvS5J0u+n+fuhkRB+X78rnqAz\nv/pfmb9PmuBOxcxWSlopSQsXLjwyO+28SPrjG6X97yly1xwfVt+efm1/93dyH/lHMnrj4H1Jcldy\n/MVPHbzXtIxWHbc54YPoh/4P3mtiO95FjbH7Sg7bQ//HkPhb94l/UBq91moi+uBzE/c1cZ8N3ifx\ndUma/9FM3iat5p+oyfzRbPT7RbYGy7Om5H3SBPd2SafW3O+IHxvD3VdLWi1JXV1dR+aTU5omffxv\nxzw0P/4CgGNVmh73f0s6zcwWmVlZ0mck/TTbsgAA9TQdcbv7oJn9paTHJBUl3eXuL2VeGQAgUaoe\nt7s/IumRjGsBAKQQ7pGTAIBEBDcA5AzBDQA5Q3ADQM4Q3ACQM+aTOEKr7k7NKpLeOMyXz5O04wiW\nM5XyXLuU7/rzXLtE/a0USu2/7+7taTbMJLgnw8y63b2r1XUcjjzXLuW7/jzXLlF/K+WxdlolAJAz\nBDcA5EyIwb261QVMQp5rl/Jdf55rl6i/lXJXe3A9bgBAYyGOuAEADQQT3C25IPEkmNldZtZnZj01\nj51oZo+b2eb4+5xW1liPmZ1qZmvN7GUze8nMboofz0v9bWb2azP7TVz/N+LHF5nZc/Fn6EfxaYiD\nZGZFM3vBzB6O7+ep9q1m9qKZbTCz7vixXHx2JMnMZpvZGjPbZGYbzezDeapfCiS4W3lB4kn4oaTl\n4x5bJelJdz9N0pPx/RANSvqSu58paZmkG+Lfd17q3y/pEnf/oKSlkpab2TJJfy/pn9z9DyS9K+nz\nLayxmZskbay5n6faJemj7r60ZhldXj47knSbpJ+5+xJJH1T1v0Oe6pfcveVfkj4s6bGa+7dIuqXV\ndaWou1NST839VySdEt8+RdIrra4x5c/xkKSP5bF+STMkPa/qdVB3SColfaZC+lL1KlJPSrpE0sOq\nXvUsF7XH9W2VNG/cY7n47Eg6QdL/KJ7fy1v9I19BjLiVfEHiBS2qZTJOcve34ttvSzqplcWkYWad\nks6V9JxyVH/catggqU/S45K2SNrl7oPxJiF/hr4t6SuShuP7c5Wf2qXqRTN/bmbr42vNSvn57CyS\nVJH0g7hV9T0zm6n81C8pkFbJ0cirf7qDXrJjZsdJekDSze7+Xu1zodfv7kPuvlTV0esFkpa0uKRU\nzOwTkvrcfX2ra5mEi9z9PFVbmzeY2Udqnwz8s1OSdJ6kf3H3cyW9r3FtkcDrlxROcKe6IHEOvGNm\np0hS/L2vxfXUZWaRqqF9t7v/JH44N/WPcPddktaq2l6YbWYjV3UK9TN0oaSrzGyrpPtUbZfcpnzU\nLkly9+3x9z5JD6r6hzMvn51eSb3u/lx8f42qQZ6X+iWFE9xHywWJfyrpc/Htz6naOw6OmZmk70va\n6O7fqnkqL/W3m9ns+PZ0VfvzG1UN8E/FmwVZv7vf4u4d7t6p6uf8KXe/TjmoXZLMbKaZzRq5Lenj\nknqUk8+Ou78taZuZLY4fulTSy8pJ/aNa3WSvmRy4XNKrqvYqv9rqelLUe6+ktyQNqPpX/POq9iqf\nlLRZ0hOSTmx1nXVqv0jV/xX8raQN8dflOar/DyW9ENffI+lr8eMfkPRrSa9J+rGkaa2utcnPcbGk\nh/NUe1znb+Kvl0b+reblsxPXulRSd/z5+Q9Jc/JUv7tz5CQA5E0orRIAQEoENwDkDMENADlDcANA\nzhDcAJAzBDcA5AzBDQA5Q3ADQM78P2XkltmK6QvZAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 2s 11ms/step - loss: 6.4352 - mean_squared_error: 6.4352 - val_loss: 0.5915 - val_mean_squared_error: 0.5915\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.59152, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0430 - mean_squared_error: 0.0430 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.59152 to 0.00145, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00145 to 0.00126, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00126\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00126\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00126\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00126\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00126\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00126\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00126\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.9806e-04 - mean_squared_error: 9.9806e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00126\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.6536e-04 - mean_squared_error: 9.6536e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00126\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.4300e-04 - mean_squared_error: 9.4300e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00126\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.2718e-04 - mean_squared_error: 9.2718e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00126\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.1261e-04 - mean_squared_error: 9.1261e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00126\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.9494e-04 - mean_squared_error: 8.9494e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00126\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.7256e-04 - mean_squared_error: 8.7256e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00126\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.4519e-04 - mean_squared_error: 8.4519e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00126\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.1506e-04 - mean_squared_error: 8.1506e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00126\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.8655e-04 - mean_squared_error: 7.8655e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00126\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.6201e-04 - mean_squared_error: 7.6201e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00126\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.4339e-04 - mean_squared_error: 7.4339e-04 - val_loss: 0.0034 - val_mean_squared_error: 0.0034\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00126\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.3016e-04 - mean_squared_error: 7.3016e-04 - val_loss: 0.0034 - val_mean_squared_error: 0.0034\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00126\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.2111e-04 - mean_squared_error: 7.2111e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00126\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.1343e-04 - mean_squared_error: 7.1343e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00126\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.0396e-04 - mean_squared_error: 7.0396e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00126\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.9342e-04 - mean_squared_error: 6.9342e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00126\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.7553e-04 - mean_squared_error: 6.7553e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00126\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.5592e-04 - mean_squared_error: 6.5592e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00126\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.3093e-04 - mean_squared_error: 6.3093e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00126\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.0792e-04 - mean_squared_error: 6.0792e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00126\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.7877e-04 - mean_squared_error: 5.7877e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00126\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.5195e-04 - mean_squared_error: 5.5195e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00126 to 0.00121, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.2738e-04 - mean_squared_error: 5.2738e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00121 to 0.00105, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.9807e-04 - mean_squared_error: 4.9807e-04 - val_loss: 9.0653e-04 - val_mean_squared_error: 9.0653e-04\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00105 to 0.00091, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.8155e-04 - mean_squared_error: 4.8155e-04 - val_loss: 7.8325e-04 - val_mean_squared_error: 7.8325e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00091 to 0.00078, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.6453e-04 - mean_squared_error: 4.6453e-04 - val_loss: 6.7657e-04 - val_mean_squared_error: 6.7657e-04\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.00078 to 0.00068, saving model to weights.best_mlp.hdf5\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5578e-04 - mean_squared_error: 4.5578e-04 - val_loss: 5.9598e-04 - val_mean_squared_error: 5.9598e-04\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.00068 to 0.00060, saving model to weights.best_mlp.hdf5\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4283e-04 - mean_squared_error: 4.4283e-04 - val_loss: 5.2983e-04 - val_mean_squared_error: 5.2983e-04\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.00060 to 0.00053, saving model to weights.best_mlp.hdf5\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4045e-04 - mean_squared_error: 4.4045e-04 - val_loss: 4.8450e-04 - val_mean_squared_error: 4.8450e-04\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.00053 to 0.00048, saving model to weights.best_mlp.hdf5\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3702e-04 - mean_squared_error: 4.3702e-04 - val_loss: 4.3794e-04 - val_mean_squared_error: 4.3794e-04\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.00048 to 0.00044, saving model to weights.best_mlp.hdf5\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4510e-04 - mean_squared_error: 4.4510e-04 - val_loss: 4.0642e-04 - val_mean_squared_error: 4.0642e-04\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.00044 to 0.00041, saving model to weights.best_mlp.hdf5\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3831e-04 - mean_squared_error: 4.3831e-04 - val_loss: 3.7198e-04 - val_mean_squared_error: 3.7198e-04\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.00041 to 0.00037, saving model to weights.best_mlp.hdf5\n",
            "Epoch 00043: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 100\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 2\n",
            "MLP: Train: 0.010548, Validation: 0.000372\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nALQYghsAWgzBDQAthuAGgBZDcANAi/l/x/T1OA53sxkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 2s 12ms/step - loss: 6.4927 - mean_squared_error: 6.4927 - val_loss: 0.5902 - val_mean_squared_error: 0.5902\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.59019, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0416 - mean_squared_error: 0.0416 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.59019 to 0.00198, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00198 to 0.00156, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00156 to 0.00152, saving model to weights.best_mlp.hdf5\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00152\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00152\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00152\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00152\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00152\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00152\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00152\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.9976e-04 - mean_squared_error: 9.9976e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00152\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.8010e-04 - mean_squared_error: 9.8010e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00152\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.6702e-04 - mean_squared_error: 9.6702e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00152\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.5542e-04 - mean_squared_error: 9.5542e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00152\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.4070e-04 - mean_squared_error: 9.4070e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00152\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.2030e-04 - mean_squared_error: 9.2030e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00152\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.9393e-04 - mean_squared_error: 8.9393e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00152\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.6385e-04 - mean_squared_error: 8.6385e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00152\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.3380e-04 - mean_squared_error: 8.3380e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00152\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.0708e-04 - mean_squared_error: 8.0708e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00152\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.8695e-04 - mean_squared_error: 7.8695e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00152\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.7285e-04 - mean_squared_error: 7.7285e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00152\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.6254e-04 - mean_squared_error: 7.6254e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00152\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.5440e-04 - mean_squared_error: 7.5440e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00152\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.4289e-04 - mean_squared_error: 7.4289e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00152\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.2865e-04 - mean_squared_error: 7.2865e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00152\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.0810e-04 - mean_squared_error: 7.0810e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00152\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.8658e-04 - mean_squared_error: 6.8658e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00152\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.5945e-04 - mean_squared_error: 6.5945e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00152\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.3227e-04 - mean_squared_error: 6.3227e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.00152 to 0.00151, saving model to weights.best_mlp.hdf5\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.0294e-04 - mean_squared_error: 6.0294e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.00151 to 0.00130, saving model to weights.best_mlp.hdf5\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.7246e-04 - mean_squared_error: 5.7246e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00130 to 0.00112, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.4580e-04 - mean_squared_error: 5.4580e-04 - val_loss: 9.6943e-04 - val_mean_squared_error: 9.6943e-04\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00112 to 0.00097, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.2766e-04 - mean_squared_error: 5.2766e-04 - val_loss: 8.3057e-04 - val_mean_squared_error: 8.3057e-04\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00097 to 0.00083, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.0900e-04 - mean_squared_error: 5.0900e-04 - val_loss: 7.1759e-04 - val_mean_squared_error: 7.1759e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00083 to 0.00072, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.9989e-04 - mean_squared_error: 4.9989e-04 - val_loss: 6.1866e-04 - val_mean_squared_error: 6.1866e-04\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.00072 to 0.00062, saving model to weights.best_mlp.hdf5\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.9211e-04 - mean_squared_error: 4.9211e-04 - val_loss: 5.3691e-04 - val_mean_squared_error: 5.3691e-04\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.00062 to 0.00054, saving model to weights.best_mlp.hdf5\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.8291e-04 - mean_squared_error: 4.8291e-04 - val_loss: 4.7763e-04 - val_mean_squared_error: 4.7763e-04\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.00054 to 0.00048, saving model to weights.best_mlp.hdf5\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.8052e-04 - mean_squared_error: 4.8052e-04 - val_loss: 4.2886e-04 - val_mean_squared_error: 4.2886e-04\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.00048 to 0.00043, saving model to weights.best_mlp.hdf5\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.7317e-04 - mean_squared_error: 4.7317e-04 - val_loss: 3.9504e-04 - val_mean_squared_error: 3.9504e-04\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.00043 to 0.00040, saving model to weights.best_mlp.hdf5\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.6939e-04 - mean_squared_error: 4.6939e-04 - val_loss: 3.6757e-04 - val_mean_squared_error: 3.6757e-04\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.00040 to 0.00037, saving model to weights.best_mlp.hdf5\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.6733e-04 - mean_squared_error: 4.6733e-04 - val_loss: 3.4375e-04 - val_mean_squared_error: 3.4375e-04\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.00037 to 0.00034, saving model to weights.best_mlp.hdf5\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.6390e-04 - mean_squared_error: 4.6390e-04 - val_loss: 3.2176e-04 - val_mean_squared_error: 3.2176e-04\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.00034 to 0.00032, saving model to weights.best_mlp.hdf5\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.6044e-04 - mean_squared_error: 4.6044e-04 - val_loss: 3.0305e-04 - val_mean_squared_error: 3.0305e-04\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.00032 to 0.00030, saving model to weights.best_mlp.hdf5\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.5588e-04 - mean_squared_error: 4.5588e-04 - val_loss: 2.9290e-04 - val_mean_squared_error: 2.9290e-04\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00030 to 0.00029, saving model to weights.best_mlp.hdf5\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5711e-04 - mean_squared_error: 4.5711e-04 - val_loss: 2.7520e-04 - val_mean_squared_error: 2.7520e-04\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.00029 to 0.00028, saving model to weights.best_mlp.hdf5\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5488e-04 - mean_squared_error: 4.5488e-04 - val_loss: 2.6803e-04 - val_mean_squared_error: 2.6803e-04\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.00028 to 0.00027, saving model to weights.best_mlp.hdf5\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5171e-04 - mean_squared_error: 4.5171e-04 - val_loss: 2.5568e-04 - val_mean_squared_error: 2.5568e-04\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.00027 to 0.00026, saving model to weights.best_mlp.hdf5\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5347e-04 - mean_squared_error: 4.5347e-04 - val_loss: 2.5622e-04 - val_mean_squared_error: 2.5622e-04\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.00026\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.5632e-04 - mean_squared_error: 4.5632e-04 - val_loss: 2.4666e-04 - val_mean_squared_error: 2.4666e-04\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.00026 to 0.00025, saving model to weights.best_mlp.hdf5\n",
            "Epoch 52/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.5561e-04 - mean_squared_error: 4.5561e-04 - val_loss: 2.4065e-04 - val_mean_squared_error: 2.4065e-04\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.00025 to 0.00024, saving model to weights.best_mlp.hdf5\n",
            "Epoch 00052: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 100\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 3\n",
            "MLP: Train: 0.011657, Validation: 0.000241\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAFRlJREFUeJzt3XuMXOV5x/HfM9e1jcFmvbaJF7qu\ngiDYNHazcR1BK0KUyNxBSXAiqNIqiv+hjUmDIqf9I0kVJPpPmiA1RU5CE6lAakEcUsQlJNiQNMZ0\nDU5ZbAcHZOQ1Mbs2GOyA7b08/WPO7M51Z2zPmXln9vuRLM+cOXv2OWb2ty/PvOe85u4CALSPRKsL\nAACcGoIbANoMwQ0AbYbgBoA2Q3ADQJshuAGgzRDcANBmCG4AaDMENwC0mVQcB12wYIH39fXFcWgA\n6Eg7duw45O499ewbS3D39fVpYGAgjkMDQEcys9fq3ZdWCQC0GYIbANoMwQ0AbSaWHjcAnKrR0VEN\nDQ3p+PHjrS4lVl1dXert7VU6nT7tYxDcAIIwNDSkuXPnqq+vT2bW6nJi4e46fPiwhoaGtHTp0tM+\nDq0SAEE4fvy4uru7Oza0JcnM1N3dfcb/V0FwAwhGJ4d2XiPOMajgvvuXe/X0yyOtLgMAghZUcN/z\n9Cv6FcENoAWOHDmi7373u6f8dVdffbWOHDkSQ0XVBRXc2VRCJ8cnWl0GgBmoWnCPjY1N+3WPPvqo\n5s2bF1dZFQU1qySbSurEKMENoPk2bNigV155RStWrFA6nVZXV5fmz5+vPXv26OWXX9aNN96o/fv3\n6/jx41q/fr3WrVsnaeoWH8eOHdNVV12lyy+/XL/5zW+0ZMkSPfzww5o1a1bDaw0quDOphE6Mjbe6\nDAAt9o3/fkm7Xn+noce85H1n62vXLav6+l133aXBwUHt3LlTW7du1TXXXKPBwcHJaXv33nuvzj33\nXL333nv68Ic/rE9+8pPq7u4uOsbevXv1wAMP6Hvf+55uvvlmPfTQQ7r11lsbeh5SYMGdTSV0YowR\nN4DWW7VqVdFc67vvvlubN2+WJO3fv1979+4tC+6lS5dqxYoVkqQPfehD2rdvXyy1hRXc6YROEtzA\njDfdyLhZ5syZM/l469at+sUvfqFt27Zp9uzZuuKKKyrOxc5ms5OPk8mk3nvvvVhqC+zDySQjbgAt\nMXfuXB09erTia2+//bbmz5+v2bNna8+ePXr22WebXF2xsEbc9LgBtEh3d7cuu+wyLV++XLNmzdKi\nRYsmX1uzZo3uuecefeADH9BFF12k1atXt7DSwII7k0ro2Inpp94AQFzuv//+ituz2awee+yxiq/l\n+9gLFizQ4ODg5PY77rij4fXlBdYqSTAdEABqCCy4k1yAAwA1BBbcCZ0YpccNANOpK7jNbJ6ZPWhm\ne8xst5l9JI5iMszjBoCa6v1w8juSHnf3T5lZRtLsOIphOiAA1FYzuM3sHEl/JelvJMndT0o6GUcx\n2TTTAQGglnpaJUsljUj6DzN7wcy+b2ZzSncys3VmNmBmAyMjp3dr1mwqodFx18SEn9bXA0CznHXW\nWS373vUEd0rSn0v6d3dfKemPkjaU7uTuG9293937e3p6TquYbCopScwsAYBp1NPjHpI05O7bo+cP\nqkJwN0Imlfs9cmJ0Ql3pZBzfAgAq2rBhg84//3zddtttkqSvf/3rSqVS2rJli9566y2Njo7qm9/8\npm644YYWV1pHcLv7QTPbb2YXufvvJH1M0q44isnmg3tsXNLpL10PoM09tkE6+GJjj7n4Uumqu6q+\nvHbtWt1+++2Twb1p0yY98cQT+uIXv6izzz5bhw4d0urVq3X99de3fG3MemeV/L2k+6IZJa9K+ts4\nipkKblolAJpr5cqVGh4e1uuvv66RkRHNnz9fixcv1pe+9CU988wzSiQSOnDggN544w0tXry4pbXW\nFdzuvlNSf8y1KBu1RwhuYIabZmQcp09/+tN68MEHdfDgQa1du1b33XefRkZGtGPHDqXTafX19VW8\nnWuzhXWTqWRhqwQAmmvt2rX6whe+oEOHDunpp5/Wpk2btHDhQqXTaW3ZskWvvfZaq0uUFFhwZ9O0\nSgC0zrJly3T06FEtWbJE5513nm655RZdd911uvTSS9Xf36+LL7641SVKCi24C2aVAEArvPji1Iei\nCxYs0LZt2yrud+zYsWaVVCawm0wxjxsAagksuPMjbnrcAFBNmMFNjxuYkdw7/3YXjTjHwIKb6YDA\nTNXV1aXDhw93dHi7uw4fPqyurq4zOk5YH05Gs0pOEtzAjNPb26uhoSGd7k3q2kVXV5d6e3vP6Bhh\nBXeKedzATJVOp7V06dJWl9EWgmqVZOhxA0BNYQV3knncAFBLUMGdSiaUShitEgCYRlDBLeX63Hw4\nCQDVhRfcaRYMBoDpBBfcmSQLBgPAdIIL7txK74y4AaCa8IKbHjcATCvA4KbHDQDTCS64Myl63AAw\nneCCO5tKcAEOAEwjzOCmVQIAVdV1kykz2yfpqKRxSWPuHtuK79lUkg8nAWAap3J3wI+6+6HYKonk\npgPS4waAaoJrleQuwGHEDQDV1BvcLunnZrbDzNbFWRAX4ADA9OptlVzu7gfMbKGkJ81sj7s/U7hD\nFOjrJOmCCy447YLocQPA9Ooacbv7gejvYUmbJa2qsM9Gd+939/6enp7TLijLPG4AmFbN4DazOWY2\nN/9Y0ickDcZVUDaV1Oi4a3yicxcMBYAzUU+rZJGkzWaW3/9+d388roLyy5edHJvQrEwyrm8DAG2r\nZnC7+6uSPtiEWiQVLxhMcANAueCmA2bTUyNuAEC58II7lRtlMyUQACoLLrgzBa0SAEC54II73+M+\nzh0CAaCiYIP75DjBDQCVBBjcUY+bETcAVBRecKfpcQPAdIIL7kwyH9yMuAGgkuCCuytNcAPAdIIL\n7nyPmwtwAKCyAIObHjcATCe44J68AIdZJQBQUXDBzSXvADC94IK78LauAIBywQV3MmFKJ40eNwBU\nEVxwS7l2Ca0SAKgsyODOsO4kAFQVZHBnUwlmlQBAFcEGN3cHBIDKAg3uJCNuAKgiyOCmxw0A1dUd\n3GaWNLMXzOyROAuSoh43s0oAoKJTGXGvl7Q7rkIKZdMJLsABgCrqCm4z65V0jaTvx1tODvO4AaC6\nekfc35b0FUlNSdMsPW4AqKpmcJvZtZKG3X1Hjf3WmdmAmQ2MjIycUVEZetwAUFU9I+7LJF1vZvsk\n/VjSlWb2n6U7uftGd+939/6enp4zKooLcACguprB7e5fdfded++T9BlJT7n7rXEWlU0luQAHAKoI\nch53bsRNjxsAKkmdys7uvlXS1lgqKUCPGwCqC3TEndTYhGt8wltdCgAEJ8zgTrMKDgBUE2Zws9I7\nAFQVaHCzYDAAVBNkcOcXDGYuNwCUCzK4aZUAQHWBBzcjbgAoFWZwp+lxA0A1QQZ3JkmrBACqCTK4\n8/O4GXEDQLkwgzvFBTgAUE2gwU2PGwCqCTS48/O46XEDQKmwg5sRNwCUCTS4aZUAQDVhBjd3BwSA\nqoIMbuZxA0B1QQZ3ImHKJFkFBwAqCTK4pWj5Mu4OCABlgg3ubCqhk+O0SgCgVNDBzYgbAMqFG9zp\nJD1uAKigZnCbWZeZPWdmvzWzl8zsG80oLPfhJK0SACiVqmOfE5KudPdjZpaW9Gsze8zdn42zsGya\nWSUAUEnN4HZ3l3QsepqO/nicRUnRh5MENwCUqavHbWZJM9spaVjSk+6+Pd6ycpe9M+IGgHJ1Bbe7\nj7v7Ckm9klaZ2fLSfcxsnZkNmNnAyMjIGReWTdHjBoBKTmlWibsfkbRF0poKr21093537+/p6Tnj\nwrgABwAqq2dWSY+ZzYsez5L0cUl74i4sdwEOwQ0ApeqZVXKepB+ZWVK5oN/k7o/EW1bU42bEDQBl\n6plV8n+SVjahliK56YD0uAGgVLBXTnJ3QACoLNjg5gIcAKgs3OBOJTU+4RrjA0oAKBJwcEfLlxHc\nAFAk+OBmZgkAFAs2uDOs9A4AFQUb3JOtEoIbAIqEG9xpVnoHgErCDW5aJQBQUbDBnUkx4gaASoIN\nbmaVAEBl4Qc387gBoEjAwR31uBlxA0CRcIObWSUAUFGwwZ1J5oObETcAFAo2uPMjbi7AAYBi4QY3\n87gBoKKAg5seNwBUEmxwT/a4mVUCAEWCDe5Ewli+DAAqCDa4pVy7hA8nAaBYzeA2s/PNbIuZ7TKz\nl8xsfTMKk1jpHQAqSdWxz5ikL7v782Y2V9IOM3vS3XfFXJuyqSStEgAoUXPE7e5/cPfno8dHJe2W\ntCTuwqTcHQIJbgAodko9bjPrk7RS0vY4iimVTSV0YpRWCQAUqju4zewsSQ9Jut3d36nw+jozGzCz\ngZGRkYYUl00lWOUdAErUFdxmllYutO9z959U2sfdN7p7v7v39/T0NKS4bCrJPG4AKFHPrBKT9ANJ\nu939W/GXNIVZJQBQrp4R92WS/lrSlWa2M/pzdcx1SRIX4ABABTWnA7r7ryVZE2opk01zAQ4AlAr8\nyknmcQNAqcCDmx43AJQKOri5AAcAygUd3LkLcAhuACgUeHAnuQAHAEoEHtwJjU+4xghvAJgUdnCn\nWekdAEoFHdyTy5cR3AAwKejgzqZzK71zEQ4ATAk7uFnpHQDKBB7cuRE3rRIAmBJ0cGfyI27mcgPA\npKCDm1YJAJRri+Dmw0kAmBJ2cKfpcQNAqbCDm1YJAJQJOrgnP5xkxA0Ak4IO7izBDQBlAg9uetwA\nUCrs4M7fZGqUHjcA5AUd3NxkCgDKBR3c9LgBoFzN4Daze81s2MwGm1FQyfdWJpXgAhwAKFDPiPuH\nktbEXEdVrPQOAMVqBre7PyPpzSbUUlE2laRVAgAFGtbjNrN1ZjZgZgMjIyONOiwrvQNAiYYFt7tv\ndPd+d+/v6elp1GGVTSVY6R0ACgQ9q0TKXfbOPG4AmBJ8cGfT9LgBoFA90wEfkLRN0kVmNmRmn4+/\nrCnZJLNKAKBQqtYO7v7ZZhRSTTad0LETY60sAQCCEn6rhAtwAKBIGwQ3PW4AKNQGwU2PGwAKBR/c\nGS7AAYAiwQc3F+AAQLHwgzudZMQNAAXCCe6Tf5R+eK20fWPR5nyP291bVBgAhCWc4M7Mkd59Uxp8\nqGhzNpXQhEtjEwQ3AEghBbckLbtJ2v+s9PaByU0ZVsEBgCKBBfeNub93/2xyU36ldy7CAYCcsIJ7\nwYXSwmXSSz+d3DS17iRzuQFACi24pal2yTuvS8rdq0QSM0sAIBJgcEftkl0PS5IyyVyrhB43AOSE\nF9wl7ZJ8q4QeNwDkhBfcUlG7ZLJVQo8bACQFG9xT7ZL8rBJaJQCQE2ZwF7RLmFUCAMXCDG5psl0y\n+8QbkphVAgB5AQd3rl0yf9/jksQdAgEgEm5wR+2Sua8+IokRNwDkhRvckrTsJmVff06L9CY9bgCI\n1BXcZrbGzH5nZr83sw1xFzUpapdclXyOWSUAEKkZ3GaWlPRvkq6SdImkz5rZJXEXJklacKEmFl6i\nq5PbCW4AiKTq2GeVpN+7+6uSZGY/lnSDpF1xFpZny27SquE7dcdT2/XUnmEtf9/ZWr7kHC1fco7e\nv/AspZNhd3sAoNHqCe4lkvYXPB+S9BfxlFPOlt0kbblTj2e+qndH0ho96Jp4XnKZhiWZWf3Hkhc8\nLn9UznUKh2+ZwvMq3l5Ntf0r/fuUHN9L9ol2tKLtzV30wguq9Qr/wYqrKdi36PGp7eMl/0K1j1nP\n19azf+3t1VT6t5l2/1M8/kxT6d/n3eQ5uuSf/if2711PcNfFzNZJWidJF1xwQaMOm5td8rGvafab\nr2i2SxNyHTs+qiPvntSRd09qbLz47e7KBUfuH9Unfxp88vUKP9hVc8an3uxNzKLT+1bVfrirfY9q\nP5TTh2DR13rpPvWE3Jmr/ouqwvY6fqEU/8Iqrnxqe+1jVvva6vUUvVCztnq2V3dq+1uDlgps9i/x\nVhvLzG3K96knuA9IOr/geW+0rYi7b5S0UZL6+/sb+1/rL/9h8mFC0tnRnwb+egCAtlFPg/h/JV1o\nZkvNLCPpM5J+VuNrAAAxqTnidvcxM/s7SU9ISkq6191fir0yAEBFdfW43f1RSY/GXAsAoA7MpQOA\nNkNwA0CbIbgBoM0Q3ADQZghuAGgz5g26QqrooGYjkl47zS9fIOlQA8sJ2Uw6V4nz7XQz6XzjONc/\ncfeeenaMJbjPhJkNuHt/q+tohpl0rhLn2+lm0vm2+lxplQBAmyG4AaDNhBjcG1tdQBPNpHOVON9O\nN5POt6XnGlyPGwAwvRBH3ACAaQQT3C1bkLhJzOxeMxs2s8GCbeea2ZNmtjf6e34ra2wkMzvfzLaY\n2S4ze8nM1kfbO+6czazLzJ4zs99G5/qNaPtSM9sevaf/K7otcscws6SZvWBmj0TPO/Z8zWyfmb1o\nZjvNbCDa1rL3chDB3dIFiZvnh5LWlGzbIOmX7n6hpF9GzzvFmKQvu/slklZLui36b9qJ53xC0pXu\n/kFJKyStMbPVkv5F0r+6+/slvSXp8y2sMQ7rJe0ueN7p5/tRd19RMA2wZe/lIIJbBQsSu/tJSfkF\niTuGuz8j6c2SzTdI+lH0+EeSbmxqUTFy9z+4+/PR46PK/YAvUQees+cci56moz8u6UpJD0bbO+Jc\n88ysV9I1kr4fPTd18PlW0bL3cijBXWlB4iUtqqWZFrn7H6LHByUtamUxcTGzPkkrJW1Xh55z1DbY\nKWlY0pOSXpF0xN3Hol067T39bUlfkTQRPe9WZ5+vS/q5me2I1teVWvhebthiwTgz7u5m1nFTfMzs\nLEkPSbrd3d+xgsWFO+mc3X1c0gozmydps6SLW1xSbMzsWknD7r7DzK5odT1Ncrm7HzCzhZKeNLM9\nhS82+70cyoi7rgWJO9AbZnaeJEV/D7e4noYys7RyoX2fu/8k2tzR5+zuRyRtkfQRSfPMLD846qT3\n9GWSrjezfcq1Na+U9B117vnK3Q9Efw8r94t5lVr4Xg4luGfqgsQ/k/S56PHnJD3cwloaKup5/kDS\nbnf/VsFLHXfOZtYTjbRlZrMkfVy5nv4WSZ+KduuIc5Ukd/+qu/e6e59yP6tPufst6tDzNbM5ZjY3\n/1jSJyQNqoXv5WAuwDGzq5Xrm+UXJL6zxSU1lJk9IOkK5e4q9oakr0n6qaRNki5Q7m6KN7t76QeY\nbcnMLpf0K0kvaqoP+o/K9bk76pzN7M+U+3AqqdxgaJO7/7OZ/alyI9JzJb0g6VZ3P9G6ShsvapXc\n4e7Xdur5Rue1OXqaknS/u99pZt1q0Xs5mOAGANQnlFYJAKBOBDcAtBmCGwDaDMENAG2G4AaANkNw\nA0CbIbgBoM0Q3ADQZv4fsrgV30aUD7wAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 2s 12ms/step - loss: 6.9421 - mean_squared_error: 6.9421 - val_loss: 1.0078 - val_mean_squared_error: 1.0078\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.00780, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0891 - mean_squared_error: 0.0891 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.00780 to 0.00247, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00247 to 0.00154, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00154 to 0.00139, saving model to weights.best_mlp.hdf5\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00139\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00139\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00139\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00139\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00139\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00139\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00139\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.9145e-04 - mean_squared_error: 9.9145e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00139\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.6681e-04 - mean_squared_error: 9.6681e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00139\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.5085e-04 - mean_squared_error: 9.5085e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00139\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.3882e-04 - mean_squared_error: 9.3882e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00139\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.2635e-04 - mean_squared_error: 9.2635e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00139\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.0972e-04 - mean_squared_error: 9.0972e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00139\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.8754e-04 - mean_squared_error: 8.8754e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00139\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.6052e-04 - mean_squared_error: 8.6052e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00139\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.3129e-04 - mean_squared_error: 8.3129e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00139\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.0222e-04 - mean_squared_error: 8.0222e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00139\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.7832e-04 - mean_squared_error: 7.7832e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00139\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.5896e-04 - mean_squared_error: 7.5896e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00139\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.4664e-04 - mean_squared_error: 7.4664e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00139\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.3820e-04 - mean_squared_error: 7.3820e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00139\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.3106e-04 - mean_squared_error: 7.3106e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00139\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.2369e-04 - mean_squared_error: 7.2369e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00139\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.1100e-04 - mean_squared_error: 7.1100e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00139\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.9481e-04 - mean_squared_error: 6.9481e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00139\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.7493e-04 - mean_squared_error: 6.7493e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00139\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.4793e-04 - mean_squared_error: 6.4793e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00139\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.2356e-04 - mean_squared_error: 6.2356e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00139\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.9616e-04 - mean_squared_error: 5.9616e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00139\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.6316e-04 - mean_squared_error: 5.6316e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00139 to 0.00127, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.3272e-04 - mean_squared_error: 5.3272e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00127 to 0.00111, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.0813e-04 - mean_squared_error: 5.0813e-04 - val_loss: 9.7925e-04 - val_mean_squared_error: 9.7925e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00111 to 0.00098, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.7923e-04 - mean_squared_error: 4.7923e-04 - val_loss: 8.5693e-04 - val_mean_squared_error: 8.5693e-04\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.00098 to 0.00086, saving model to weights.best_mlp.hdf5\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.6321e-04 - mean_squared_error: 4.6321e-04 - val_loss: 7.5646e-04 - val_mean_squared_error: 7.5646e-04\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.00086 to 0.00076, saving model to weights.best_mlp.hdf5\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.4819e-04 - mean_squared_error: 4.4819e-04 - val_loss: 6.6102e-04 - val_mean_squared_error: 6.6102e-04\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.00076 to 0.00066, saving model to weights.best_mlp.hdf5\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3669e-04 - mean_squared_error: 4.3669e-04 - val_loss: 5.8319e-04 - val_mean_squared_error: 5.8319e-04\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.00066 to 0.00058, saving model to weights.best_mlp.hdf5\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2632e-04 - mean_squared_error: 4.2632e-04 - val_loss: 5.2830e-04 - val_mean_squared_error: 5.2830e-04\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.00058 to 0.00053, saving model to weights.best_mlp.hdf5\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2500e-04 - mean_squared_error: 4.2500e-04 - val_loss: 4.8507e-04 - val_mean_squared_error: 4.8507e-04\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.00053 to 0.00049, saving model to weights.best_mlp.hdf5\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.1454e-04 - mean_squared_error: 4.1454e-04 - val_loss: 4.4262e-04 - val_mean_squared_error: 4.4262e-04\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.00049 to 0.00044, saving model to weights.best_mlp.hdf5\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.0525e-04 - mean_squared_error: 4.0525e-04 - val_loss: 4.0601e-04 - val_mean_squared_error: 4.0601e-04\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.00044 to 0.00041, saving model to weights.best_mlp.hdf5\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.0358e-04 - mean_squared_error: 4.0358e-04 - val_loss: 3.7492e-04 - val_mean_squared_error: 3.7492e-04\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.00041 to 0.00037, saving model to weights.best_mlp.hdf5\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0167e-04 - mean_squared_error: 4.0167e-04 - val_loss: 3.4981e-04 - val_mean_squared_error: 3.4981e-04\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00037 to 0.00035, saving model to weights.best_mlp.hdf5\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.0823e-04 - mean_squared_error: 4.0823e-04 - val_loss: 3.2431e-04 - val_mean_squared_error: 3.2431e-04\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.00035 to 0.00032, saving model to weights.best_mlp.hdf5\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.9455e-04 - mean_squared_error: 3.9455e-04 - val_loss: 3.0654e-04 - val_mean_squared_error: 3.0654e-04\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.00032 to 0.00031, saving model to weights.best_mlp.hdf5\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.0650e-04 - mean_squared_error: 4.0650e-04 - val_loss: 2.9128e-04 - val_mean_squared_error: 2.9128e-04\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.00031 to 0.00029, saving model to weights.best_mlp.hdf5\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.0732e-04 - mean_squared_error: 4.0732e-04 - val_loss: 2.8061e-04 - val_mean_squared_error: 2.8061e-04\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.00029 to 0.00028, saving model to weights.best_mlp.hdf5\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.0894e-04 - mean_squared_error: 4.0894e-04 - val_loss: 2.6531e-04 - val_mean_squared_error: 2.6531e-04\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.00028 to 0.00027, saving model to weights.best_mlp.hdf5\n",
            "Epoch 52/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.8688e-04 - mean_squared_error: 3.8688e-04 - val_loss: 2.5830e-04 - val_mean_squared_error: 2.5830e-04\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.00027 to 0.00026, saving model to weights.best_mlp.hdf5\n",
            "Epoch 53/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.8534e-04 - mean_squared_error: 3.8534e-04 - val_loss: 2.4848e-04 - val_mean_squared_error: 2.4848e-04\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.00026 to 0.00025, saving model to weights.best_mlp.hdf5\n",
            "Epoch 54/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.8308e-04 - mean_squared_error: 3.8308e-04 - val_loss: 2.4494e-04 - val_mean_squared_error: 2.4494e-04\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.00025 to 0.00024, saving model to weights.best_mlp.hdf5\n",
            "Epoch 55/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.7938e-04 - mean_squared_error: 3.7938e-04 - val_loss: 2.4431e-04 - val_mean_squared_error: 2.4431e-04\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.00024 to 0.00024, saving model to weights.best_mlp.hdf5\n",
            "Epoch 56/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.8206e-04 - mean_squared_error: 3.8206e-04 - val_loss: 2.4207e-04 - val_mean_squared_error: 2.4207e-04\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.00024 to 0.00024, saving model to weights.best_mlp.hdf5\n",
            "Epoch 57/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.6413e-04 - mean_squared_error: 3.6413e-04 - val_loss: 2.3882e-04 - val_mean_squared_error: 2.3882e-04\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.00024 to 0.00024, saving model to weights.best_mlp.hdf5\n",
            "Epoch 58/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.0663e-04 - mean_squared_error: 4.0663e-04 - val_loss: 2.3946e-04 - val_mean_squared_error: 2.3946e-04\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.00024\n",
            "Epoch 59/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.8168e-04 - mean_squared_error: 3.8168e-04 - val_loss: 2.2741e-04 - val_mean_squared_error: 2.2741e-04\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.00024 to 0.00023, saving model to weights.best_mlp.hdf5\n",
            "Epoch 60/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.3899e-04 - mean_squared_error: 5.3899e-04 - val_loss: 2.5062e-04 - val_mean_squared_error: 2.5062e-04\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.00023\n",
            "Epoch 61/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.8243e-04 - mean_squared_error: 3.8243e-04 - val_loss: 2.4244e-04 - val_mean_squared_error: 2.4244e-04\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.00023\n",
            "Epoch 00061: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 100\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 4\n",
            "MLP: Train: 0.011349, Validation: 0.000242\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyBiCGwAyhuAGgIwhuAEgYwhuAMiY/wd4fakjn2TB5QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 2s 12ms/step - loss: 7.2941 - mean_squared_error: 7.2941 - val_loss: 1.2330 - val_mean_squared_error: 1.2330\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.23300, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.1181 - mean_squared_error: 0.1181 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.23300 to 0.00234, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00234 to 0.00177, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00177 to 0.00163, saving model to weights.best_mlp.hdf5\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00163\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00163\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00163\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00163\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00163\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00163\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00163\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00163\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00163\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00163\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.8950e-04 - mean_squared_error: 9.8950e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00163\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.7575e-04 - mean_squared_error: 9.7575e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00163\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 9.5757e-04 - mean_squared_error: 9.5757e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00163\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.3233e-04 - mean_squared_error: 9.3233e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00163\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.0097e-04 - mean_squared_error: 9.0097e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00163\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.6672e-04 - mean_squared_error: 8.6672e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00163\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.3353e-04 - mean_squared_error: 8.3353e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00163\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.0418e-04 - mean_squared_error: 8.0418e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00163\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.8028e-04 - mean_squared_error: 7.8028e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00163\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.6180e-04 - mean_squared_error: 7.6180e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00163\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.4884e-04 - mean_squared_error: 7.4884e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00163\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.3675e-04 - mean_squared_error: 7.3675e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00163\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.2455e-04 - mean_squared_error: 7.2455e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00163\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.1042e-04 - mean_squared_error: 7.1042e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00163\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.9522e-04 - mean_squared_error: 6.9522e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00163\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.7352e-04 - mean_squared_error: 6.7352e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00163\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 6.4996e-04 - mean_squared_error: 6.4996e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00163\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.2225e-04 - mean_squared_error: 6.2225e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00163\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.9038e-04 - mean_squared_error: 5.9038e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00163 to 0.00146, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.6113e-04 - mean_squared_error: 5.6113e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00146 to 0.00129, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.3073e-04 - mean_squared_error: 5.3073e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00129 to 0.00113, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.0487e-04 - mean_squared_error: 5.0487e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00113 to 0.00101, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.8108e-04 - mean_squared_error: 4.8108e-04 - val_loss: 8.8912e-04 - val_mean_squared_error: 8.8912e-04\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.00101 to 0.00089, saving model to weights.best_mlp.hdf5\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.6399e-04 - mean_squared_error: 4.6399e-04 - val_loss: 7.8005e-04 - val_mean_squared_error: 7.8005e-04\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.00089 to 0.00078, saving model to weights.best_mlp.hdf5\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.4862e-04 - mean_squared_error: 4.4862e-04 - val_loss: 6.9030e-04 - val_mean_squared_error: 6.9030e-04\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.00078 to 0.00069, saving model to weights.best_mlp.hdf5\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3606e-04 - mean_squared_error: 4.3606e-04 - val_loss: 6.1236e-04 - val_mean_squared_error: 6.1236e-04\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.00069 to 0.00061, saving model to weights.best_mlp.hdf5\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2609e-04 - mean_squared_error: 4.2609e-04 - val_loss: 5.5994e-04 - val_mean_squared_error: 5.5994e-04\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.00061 to 0.00056, saving model to weights.best_mlp.hdf5\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2017e-04 - mean_squared_error: 4.2017e-04 - val_loss: 5.0505e-04 - val_mean_squared_error: 5.0505e-04\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.00056 to 0.00051, saving model to weights.best_mlp.hdf5\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.1377e-04 - mean_squared_error: 4.1377e-04 - val_loss: 4.6698e-04 - val_mean_squared_error: 4.6698e-04\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.00051 to 0.00047, saving model to weights.best_mlp.hdf5\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.1091e-04 - mean_squared_error: 4.1091e-04 - val_loss: 4.3359e-04 - val_mean_squared_error: 4.3359e-04\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.00047 to 0.00043, saving model to weights.best_mlp.hdf5\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.0959e-04 - mean_squared_error: 4.0959e-04 - val_loss: 3.9860e-04 - val_mean_squared_error: 3.9860e-04\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.00043 to 0.00040, saving model to weights.best_mlp.hdf5\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0756e-04 - mean_squared_error: 4.0756e-04 - val_loss: 3.7761e-04 - val_mean_squared_error: 3.7761e-04\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00040 to 0.00038, saving model to weights.best_mlp.hdf5\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.0634e-04 - mean_squared_error: 4.0634e-04 - val_loss: 3.4988e-04 - val_mean_squared_error: 3.4988e-04\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.00038 to 0.00035, saving model to weights.best_mlp.hdf5\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.0230e-04 - mean_squared_error: 4.0230e-04 - val_loss: 3.2304e-04 - val_mean_squared_error: 3.2304e-04\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.00035 to 0.00032, saving model to weights.best_mlp.hdf5\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.0668e-04 - mean_squared_error: 4.0668e-04 - val_loss: 3.1083e-04 - val_mean_squared_error: 3.1083e-04\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.00032 to 0.00031, saving model to weights.best_mlp.hdf5\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.0169e-04 - mean_squared_error: 4.0169e-04 - val_loss: 2.9899e-04 - val_mean_squared_error: 2.9899e-04\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.00031 to 0.00030, saving model to weights.best_mlp.hdf5\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.1046e-04 - mean_squared_error: 4.1046e-04 - val_loss: 2.9119e-04 - val_mean_squared_error: 2.9119e-04\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.00030 to 0.00029, saving model to weights.best_mlp.hdf5\n",
            "Epoch 52/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9848e-04 - mean_squared_error: 3.9848e-04 - val_loss: 2.8343e-04 - val_mean_squared_error: 2.8343e-04\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.00029 to 0.00028, saving model to weights.best_mlp.hdf5\n",
            "Epoch 53/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.1412e-04 - mean_squared_error: 4.1412e-04 - val_loss: 2.7572e-04 - val_mean_squared_error: 2.7572e-04\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.00028 to 0.00028, saving model to weights.best_mlp.hdf5\n",
            "Epoch 54/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9695e-04 - mean_squared_error: 3.9695e-04 - val_loss: 2.7051e-04 - val_mean_squared_error: 2.7051e-04\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.00028 to 0.00027, saving model to weights.best_mlp.hdf5\n",
            "Epoch 55/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.1671e-04 - mean_squared_error: 4.1671e-04 - val_loss: 2.6664e-04 - val_mean_squared_error: 2.6664e-04\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.00027 to 0.00027, saving model to weights.best_mlp.hdf5\n",
            "Epoch 56/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.9141e-04 - mean_squared_error: 3.9141e-04 - val_loss: 2.5972e-04 - val_mean_squared_error: 2.5972e-04\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.00027 to 0.00026, saving model to weights.best_mlp.hdf5\n",
            "Epoch 57/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.1760e-04 - mean_squared_error: 4.1760e-04 - val_loss: 2.5846e-04 - val_mean_squared_error: 2.5846e-04\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.00026 to 0.00026, saving model to weights.best_mlp.hdf5\n",
            "Epoch 58/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.8159e-04 - mean_squared_error: 3.8159e-04 - val_loss: 2.5386e-04 - val_mean_squared_error: 2.5386e-04\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.00026 to 0.00025, saving model to weights.best_mlp.hdf5\n",
            "Epoch 59/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3093e-04 - mean_squared_error: 4.3093e-04 - val_loss: 2.5795e-04 - val_mean_squared_error: 2.5795e-04\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.00025\n",
            "Epoch 60/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.1002e-04 - mean_squared_error: 4.1002e-04 - val_loss: 2.5778e-04 - val_mean_squared_error: 2.5778e-04\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.00025\n",
            "Epoch 00060: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 100\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 2\n",
            "MLP: Train: 0.011568, Validation: 0.000258\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 2s 13ms/step - loss: 6.4154 - mean_squared_error: 6.4154 - val_loss: 0.5164 - val_mean_squared_error: 0.5164\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.51645, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0356 - mean_squared_error: 0.0356 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.51645 to 0.00175, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00175 to 0.00148, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00148\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00148\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00148\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00148\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00148\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00148\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00148\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00148\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00148\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00148\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.8471e-04 - mean_squared_error: 9.8471e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00148\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.6949e-04 - mean_squared_error: 9.6949e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00148\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.5161e-04 - mean_squared_error: 9.5161e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00148\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.2834e-04 - mean_squared_error: 9.2834e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00148\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.9942e-04 - mean_squared_error: 8.9942e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00148\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.6844e-04 - mean_squared_error: 8.6844e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00148\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.3741e-04 - mean_squared_error: 8.3741e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00148\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.1192e-04 - mean_squared_error: 8.1192e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00148\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.9224e-04 - mean_squared_error: 7.9224e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00148\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.7847e-04 - mean_squared_error: 7.7847e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00148\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.6774e-04 - mean_squared_error: 7.6774e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00148\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.5963e-04 - mean_squared_error: 7.5963e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00148\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.4846e-04 - mean_squared_error: 7.4846e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00148\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.3368e-04 - mean_squared_error: 7.3368e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00148\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.1315e-04 - mean_squared_error: 7.1315e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00148\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.9189e-04 - mean_squared_error: 6.9189e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00148\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.6830e-04 - mean_squared_error: 6.6830e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00148\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.4119e-04 - mean_squared_error: 6.4119e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.00148 to 0.00145, saving model to weights.best_mlp.hdf5\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.0007e-04 - mean_squared_error: 6.0007e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.00145 to 0.00126, saving model to weights.best_mlp.hdf5\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.7471e-04 - mean_squared_error: 5.7471e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00126 to 0.00110, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.4089e-04 - mean_squared_error: 5.4089e-04 - val_loss: 9.5246e-04 - val_mean_squared_error: 9.5246e-04\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00110 to 0.00095, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.1831e-04 - mean_squared_error: 5.1831e-04 - val_loss: 8.2627e-04 - val_mean_squared_error: 8.2627e-04\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00095 to 0.00083, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.9980e-04 - mean_squared_error: 4.9980e-04 - val_loss: 7.1278e-04 - val_mean_squared_error: 7.1278e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00083 to 0.00071, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.7958e-04 - mean_squared_error: 4.7958e-04 - val_loss: 6.1662e-04 - val_mean_squared_error: 6.1662e-04\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.00071 to 0.00062, saving model to weights.best_mlp.hdf5\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.6574e-04 - mean_squared_error: 4.6574e-04 - val_loss: 5.4056e-04 - val_mean_squared_error: 5.4056e-04\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.00062 to 0.00054, saving model to weights.best_mlp.hdf5\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.5521e-04 - mean_squared_error: 4.5521e-04 - val_loss: 4.8080e-04 - val_mean_squared_error: 4.8080e-04\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.00054 to 0.00048, saving model to weights.best_mlp.hdf5\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.4272e-04 - mean_squared_error: 4.4272e-04 - val_loss: 4.3134e-04 - val_mean_squared_error: 4.3134e-04\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.00048 to 0.00043, saving model to weights.best_mlp.hdf5\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3370e-04 - mean_squared_error: 4.3370e-04 - val_loss: 3.9065e-04 - val_mean_squared_error: 3.9065e-04\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.00043 to 0.00039, saving model to weights.best_mlp.hdf5\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3046e-04 - mean_squared_error: 4.3046e-04 - val_loss: 3.5706e-04 - val_mean_squared_error: 3.5706e-04\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.00039 to 0.00036, saving model to weights.best_mlp.hdf5\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2924e-04 - mean_squared_error: 4.2924e-04 - val_loss: 3.2884e-04 - val_mean_squared_error: 3.2884e-04\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.00036 to 0.00033, saving model to weights.best_mlp.hdf5\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2370e-04 - mean_squared_error: 4.2370e-04 - val_loss: 3.0071e-04 - val_mean_squared_error: 3.0071e-04\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.00033 to 0.00030, saving model to weights.best_mlp.hdf5\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2485e-04 - mean_squared_error: 4.2485e-04 - val_loss: 2.7524e-04 - val_mean_squared_error: 2.7524e-04\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.00030 to 0.00028, saving model to weights.best_mlp.hdf5\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2244e-04 - mean_squared_error: 4.2244e-04 - val_loss: 2.5340e-04 - val_mean_squared_error: 2.5340e-04\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00028 to 0.00025, saving model to weights.best_mlp.hdf5\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2456e-04 - mean_squared_error: 4.2456e-04 - val_loss: 2.4616e-04 - val_mean_squared_error: 2.4616e-04\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.00025 to 0.00025, saving model to weights.best_mlp.hdf5\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.1043e-04 - mean_squared_error: 4.1043e-04 - val_loss: 2.3754e-04 - val_mean_squared_error: 2.3754e-04\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.00025 to 0.00024, saving model to weights.best_mlp.hdf5\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.1520e-04 - mean_squared_error: 4.1520e-04 - val_loss: 2.2813e-04 - val_mean_squared_error: 2.2813e-04\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.00024 to 0.00023, saving model to weights.best_mlp.hdf5\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0907e-04 - mean_squared_error: 4.0907e-04 - val_loss: 2.1988e-04 - val_mean_squared_error: 2.1988e-04\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.00023 to 0.00022, saving model to weights.best_mlp.hdf5\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2089e-04 - mean_squared_error: 4.2089e-04 - val_loss: 2.2201e-04 - val_mean_squared_error: 2.2201e-04\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.00022\n",
            "Epoch 52/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.0145e-04 - mean_squared_error: 4.0145e-04 - val_loss: 2.1284e-04 - val_mean_squared_error: 2.1284e-04\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.00022 to 0.00021, saving model to weights.best_mlp.hdf5\n",
            "Epoch 53/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0681e-04 - mean_squared_error: 4.0681e-04 - val_loss: 2.1985e-04 - val_mean_squared_error: 2.1985e-04\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.00021\n",
            "Epoch 54/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.8915e-04 - mean_squared_error: 3.8915e-04 - val_loss: 2.1415e-04 - val_mean_squared_error: 2.1415e-04\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.00021\n",
            "Epoch 55/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.8443e-04 - mean_squared_error: 3.8443e-04 - val_loss: 2.1110e-04 - val_mean_squared_error: 2.1110e-04\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.00021 to 0.00021, saving model to weights.best_mlp.hdf5\n",
            "Epoch 56/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 3.7250e-04 - mean_squared_error: 3.7250e-04 - val_loss: 2.1171e-04 - val_mean_squared_error: 2.1171e-04\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.00021\n",
            "Epoch 57/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.7056e-04 - mean_squared_error: 3.7056e-04 - val_loss: 2.1648e-04 - val_mean_squared_error: 2.1648e-04\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.00021\n",
            "Epoch 58/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.5843e-04 - mean_squared_error: 3.5843e-04 - val_loss: 2.1187e-04 - val_mean_squared_error: 2.1187e-04\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.00021\n",
            "Epoch 59/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.9651e-04 - mean_squared_error: 3.9651e-04 - val_loss: 2.2551e-04 - val_mean_squared_error: 2.2551e-04\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.00021\n",
            "Epoch 60/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.0586e-04 - mean_squared_error: 4.0586e-04 - val_loss: 2.1183e-04 - val_mean_squared_error: 2.1183e-04\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.00021\n",
            "Epoch 61/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3291e-04 - mean_squared_error: 4.3291e-04 - val_loss: 2.2243e-04 - val_mean_squared_error: 2.2243e-04\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.00021\n",
            "Epoch 00061: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 100\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 3\n",
            "MLP: Train: 0.011848, Validation: 0.000222\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3ACQMgQ3AKQMwQ0AKfP/OmdX9b3ftTwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 2s 13ms/step - loss: 6.9329 - mean_squared_error: 6.9329 - val_loss: 0.9656 - val_mean_squared_error: 0.9656\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.96563, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 0.0983 - mean_squared_error: 0.0983 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.96563 to 0.00209, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00209 to 0.00142, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00142\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00142\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00142\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00142\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00142\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00142\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00142\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00142\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00142\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00142\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00142\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00142\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.8432e-04 - mean_squared_error: 9.8432e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00142\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.6251e-04 - mean_squared_error: 9.6251e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00142\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.3587e-04 - mean_squared_error: 9.3587e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00142\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.0532e-04 - mean_squared_error: 9.0532e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00142\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 8.7352e-04 - mean_squared_error: 8.7352e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00142\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.4426e-04 - mean_squared_error: 8.4426e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00142\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.1958e-04 - mean_squared_error: 8.1958e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00142\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.0181e-04 - mean_squared_error: 8.0181e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00142\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.8997e-04 - mean_squared_error: 7.8997e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00142\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 7.8153e-04 - mean_squared_error: 7.8153e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00142\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.7245e-04 - mean_squared_error: 7.7245e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00142\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.6185e-04 - mean_squared_error: 7.6185e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00142\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.4704e-04 - mean_squared_error: 7.4704e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00142\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.2699e-04 - mean_squared_error: 7.2699e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00142\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.0000e-04 - mean_squared_error: 7.0000e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00142\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.7307e-04 - mean_squared_error: 6.7307e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00142\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.4065e-04 - mean_squared_error: 6.4065e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00142\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.1151e-04 - mean_squared_error: 6.1151e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00142 to 0.00132, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.7764e-04 - mean_squared_error: 5.7764e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00132 to 0.00115, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 5.4569e-04 - mean_squared_error: 5.4569e-04 - val_loss: 9.8866e-04 - val_mean_squared_error: 9.8866e-04\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00115 to 0.00099, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.1676e-04 - mean_squared_error: 5.1676e-04 - val_loss: 8.6051e-04 - val_mean_squared_error: 8.6051e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00099 to 0.00086, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.9306e-04 - mean_squared_error: 4.9306e-04 - val_loss: 7.4619e-04 - val_mean_squared_error: 7.4619e-04\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.00086 to 0.00075, saving model to weights.best_mlp.hdf5\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.7389e-04 - mean_squared_error: 4.7389e-04 - val_loss: 6.5018e-04 - val_mean_squared_error: 6.5018e-04\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.00075 to 0.00065, saving model to weights.best_mlp.hdf5\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.5470e-04 - mean_squared_error: 4.5470e-04 - val_loss: 5.7248e-04 - val_mean_squared_error: 5.7248e-04\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.00065 to 0.00057, saving model to weights.best_mlp.hdf5\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.5065e-04 - mean_squared_error: 4.5065e-04 - val_loss: 5.0664e-04 - val_mean_squared_error: 5.0664e-04\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.00057 to 0.00051, saving model to weights.best_mlp.hdf5\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3837e-04 - mean_squared_error: 4.3837e-04 - val_loss: 4.5343e-04 - val_mean_squared_error: 4.5343e-04\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.00051 to 0.00045, saving model to weights.best_mlp.hdf5\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3522e-04 - mean_squared_error: 4.3522e-04 - val_loss: 4.2420e-04 - val_mean_squared_error: 4.2420e-04\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.00045 to 0.00042, saving model to weights.best_mlp.hdf5\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 1ms/step - loss: 4.3586e-04 - mean_squared_error: 4.3586e-04 - val_loss: 3.9612e-04 - val_mean_squared_error: 3.9612e-04\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.00042 to 0.00040, saving model to weights.best_mlp.hdf5\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3549e-04 - mean_squared_error: 4.3549e-04 - val_loss: 3.6907e-04 - val_mean_squared_error: 3.6907e-04\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.00040 to 0.00037, saving model to weights.best_mlp.hdf5\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3758e-04 - mean_squared_error: 4.3758e-04 - val_loss: 3.4920e-04 - val_mean_squared_error: 3.4920e-04\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.00037 to 0.00035, saving model to weights.best_mlp.hdf5\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3785e-04 - mean_squared_error: 4.3785e-04 - val_loss: 3.3769e-04 - val_mean_squared_error: 3.3769e-04\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00035 to 0.00034, saving model to weights.best_mlp.hdf5\n",
            "Epoch 00046: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 100\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 4\n",
            "MLP: Train: 0.010808, Validation: 0.000338\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIbgBoMUQ3ADQYv4ft7s1Yp134jgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 2s 13ms/step - loss: 5.2107 - mean_squared_error: 5.2107 - val_loss: 0.0080 - val_mean_squared_error: 0.0080\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00798, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00798 to 0.00146, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00146\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00146\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00146\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00146\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00146\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.7245e-04 - mean_squared_error: 9.7245e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00146\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.4721e-04 - mean_squared_error: 9.4721e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00146\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.3141e-04 - mean_squared_error: 9.3141e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00146\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.1566e-04 - mean_squared_error: 9.1566e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00146\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.9427e-04 - mean_squared_error: 8.9427e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00146\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.6622e-04 - mean_squared_error: 8.6622e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00146\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.3476e-04 - mean_squared_error: 8.3476e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00146\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.0496e-04 - mean_squared_error: 8.0496e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00146\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.8142e-04 - mean_squared_error: 7.8142e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00146\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.6477e-04 - mean_squared_error: 7.6477e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00146\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.5458e-04 - mean_squared_error: 7.5458e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00146\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.4624e-04 - mean_squared_error: 7.4624e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00146\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.3617e-04 - mean_squared_error: 7.3617e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00146\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.2051e-04 - mean_squared_error: 7.2051e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00146\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.9790e-04 - mean_squared_error: 6.9790e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00146\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.7068e-04 - mean_squared_error: 6.7068e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00146\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.3838e-04 - mean_squared_error: 6.3838e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.00146 to 0.00144, saving model to weights.best_mlp.hdf5\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.0418e-04 - mean_squared_error: 6.0418e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.00144 to 0.00121, saving model to weights.best_mlp.hdf5\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.6932e-04 - mean_squared_error: 5.6932e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.00121 to 0.00100, saving model to weights.best_mlp.hdf5\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.3679e-04 - mean_squared_error: 5.3679e-04 - val_loss: 8.2668e-04 - val_mean_squared_error: 8.2668e-04\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.00100 to 0.00083, saving model to weights.best_mlp.hdf5\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.0803e-04 - mean_squared_error: 5.0803e-04 - val_loss: 6.8062e-04 - val_mean_squared_error: 6.8062e-04\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.00083 to 0.00068, saving model to weights.best_mlp.hdf5\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.8475e-04 - mean_squared_error: 4.8475e-04 - val_loss: 5.6297e-04 - val_mean_squared_error: 5.6297e-04\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.00068 to 0.00056, saving model to weights.best_mlp.hdf5\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.7043e-04 - mean_squared_error: 4.7043e-04 - val_loss: 4.7033e-04 - val_mean_squared_error: 4.7033e-04\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.00056 to 0.00047, saving model to weights.best_mlp.hdf5\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.5670e-04 - mean_squared_error: 4.5670e-04 - val_loss: 4.0434e-04 - val_mean_squared_error: 4.0434e-04\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.00047 to 0.00040, saving model to weights.best_mlp.hdf5\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.4510e-04 - mean_squared_error: 4.4510e-04 - val_loss: 3.5800e-04 - val_mean_squared_error: 3.5800e-04\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.00040 to 0.00036, saving model to weights.best_mlp.hdf5\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3860e-04 - mean_squared_error: 4.3860e-04 - val_loss: 3.2086e-04 - val_mean_squared_error: 3.2086e-04\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00036 to 0.00032, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3450e-04 - mean_squared_error: 4.3450e-04 - val_loss: 2.8657e-04 - val_mean_squared_error: 2.8657e-04\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00032 to 0.00029, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2581e-04 - mean_squared_error: 4.2581e-04 - val_loss: 2.6051e-04 - val_mean_squared_error: 2.6051e-04\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00029 to 0.00026, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2143e-04 - mean_squared_error: 4.2143e-04 - val_loss: 2.4026e-04 - val_mean_squared_error: 2.4026e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00026 to 0.00024, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.1133e-04 - mean_squared_error: 4.1133e-04 - val_loss: 2.3258e-04 - val_mean_squared_error: 2.3258e-04\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.00024 to 0.00023, saving model to weights.best_mlp.hdf5\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.9577e-04 - mean_squared_error: 3.9577e-04 - val_loss: 2.2860e-04 - val_mean_squared_error: 2.2860e-04\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.00023 to 0.00023, saving model to weights.best_mlp.hdf5\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.7971e-04 - mean_squared_error: 3.7971e-04 - val_loss: 2.3163e-04 - val_mean_squared_error: 2.3163e-04\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00023\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.6129e-04 - mean_squared_error: 3.6129e-04 - val_loss: 2.3676e-04 - val_mean_squared_error: 2.3676e-04\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00023\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.4175e-04 - mean_squared_error: 3.4175e-04 - val_loss: 2.5401e-04 - val_mean_squared_error: 2.5401e-04\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00023\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.2984e-04 - mean_squared_error: 3.2984e-04 - val_loss: 2.6546e-04 - val_mean_squared_error: 2.6546e-04\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00023\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.1667e-04 - mean_squared_error: 3.1667e-04 - val_loss: 2.9681e-04 - val_mean_squared_error: 2.9681e-04\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00023\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.1406e-04 - mean_squared_error: 3.1406e-04 - val_loss: 3.1381e-04 - val_mean_squared_error: 3.1381e-04\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00023\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.8726e-04 - mean_squared_error: 2.8726e-04 - val_loss: 3.8679e-04 - val_mean_squared_error: 3.8679e-04\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00023\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.0097e-04 - mean_squared_error: 3.0097e-04 - val_loss: 4.0562e-04 - val_mean_squared_error: 4.0562e-04\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.00023\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.9022e-04 - mean_squared_error: 2.9022e-04 - val_loss: 5.0059e-04 - val_mean_squared_error: 5.0059e-04\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00023\n",
            "Epoch 00047: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 200\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 2\n",
            "MLP: Train: 0.009893, Validation: 0.000501\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nENwA0GYIbgBoM/8PX3OKtNIvU8IAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 2s 14ms/step - loss: 5.0103 - mean_squared_error: 5.0103 - val_loss: 0.0077 - val_mean_squared_error: 0.0077\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00773, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00773 to 0.00149, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00149\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00149\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00149\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00149\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00149\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.6608e-04 - mean_squared_error: 9.6608e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00149\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.4000e-04 - mean_squared_error: 9.4000e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00149\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.1909e-04 - mean_squared_error: 9.1909e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00149\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.9591e-04 - mean_squared_error: 8.9591e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00149\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.6729e-04 - mean_squared_error: 8.6729e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00149\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.3522e-04 - mean_squared_error: 8.3522e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00149\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.0454e-04 - mean_squared_error: 8.0454e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00149\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.7963e-04 - mean_squared_error: 7.7963e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00149\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.6293e-04 - mean_squared_error: 7.6293e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00149\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.5350e-04 - mean_squared_error: 7.5350e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00149\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.4745e-04 - mean_squared_error: 7.4745e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00149\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.4028e-04 - mean_squared_error: 7.4028e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00149\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.3031e-04 - mean_squared_error: 7.3031e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00149\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.1316e-04 - mean_squared_error: 7.1316e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00149\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.8753e-04 - mean_squared_error: 6.8753e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00149\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.5777e-04 - mean_squared_error: 6.5777e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00149\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.2464e-04 - mean_squared_error: 6.2464e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.00149 to 0.00132, saving model to weights.best_mlp.hdf5\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.9134e-04 - mean_squared_error: 5.9134e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.00132 to 0.00110, saving model to weights.best_mlp.hdf5\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.5975e-04 - mean_squared_error: 5.5975e-04 - val_loss: 9.0663e-04 - val_mean_squared_error: 9.0663e-04\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.00110 to 0.00091, saving model to weights.best_mlp.hdf5\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.2850e-04 - mean_squared_error: 5.2850e-04 - val_loss: 7.5099e-04 - val_mean_squared_error: 7.5099e-04\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.00091 to 0.00075, saving model to weights.best_mlp.hdf5\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.0605e-04 - mean_squared_error: 5.0605e-04 - val_loss: 6.1677e-04 - val_mean_squared_error: 6.1677e-04\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.00075 to 0.00062, saving model to weights.best_mlp.hdf5\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.8770e-04 - mean_squared_error: 4.8770e-04 - val_loss: 5.1126e-04 - val_mean_squared_error: 5.1126e-04\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.00062 to 0.00051, saving model to weights.best_mlp.hdf5\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.7469e-04 - mean_squared_error: 4.7469e-04 - val_loss: 4.2969e-04 - val_mean_squared_error: 4.2969e-04\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.00051 to 0.00043, saving model to weights.best_mlp.hdf5\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.6332e-04 - mean_squared_error: 4.6332e-04 - val_loss: 3.7244e-04 - val_mean_squared_error: 3.7244e-04\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.00043 to 0.00037, saving model to weights.best_mlp.hdf5\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.5410e-04 - mean_squared_error: 4.5410e-04 - val_loss: 3.2497e-04 - val_mean_squared_error: 3.2497e-04\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.00037 to 0.00032, saving model to weights.best_mlp.hdf5\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.4815e-04 - mean_squared_error: 4.4815e-04 - val_loss: 2.8383e-04 - val_mean_squared_error: 2.8383e-04\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00032 to 0.00028, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3951e-04 - mean_squared_error: 4.3951e-04 - val_loss: 2.5107e-04 - val_mean_squared_error: 2.5107e-04\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00028 to 0.00025, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3154e-04 - mean_squared_error: 4.3154e-04 - val_loss: 2.3415e-04 - val_mean_squared_error: 2.3415e-04\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00025 to 0.00023, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.1756e-04 - mean_squared_error: 4.1756e-04 - val_loss: 2.2539e-04 - val_mean_squared_error: 2.2539e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00023 to 0.00023, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.0219e-04 - mean_squared_error: 4.0219e-04 - val_loss: 2.2372e-04 - val_mean_squared_error: 2.2372e-04\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.00023 to 0.00022, saving model to weights.best_mlp.hdf5\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.7902e-04 - mean_squared_error: 3.7902e-04 - val_loss: 2.2992e-04 - val_mean_squared_error: 2.2992e-04\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00022\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.5951e-04 - mean_squared_error: 3.5951e-04 - val_loss: 2.3498e-04 - val_mean_squared_error: 2.3498e-04\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00022\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.3466e-04 - mean_squared_error: 3.3466e-04 - val_loss: 2.5365e-04 - val_mean_squared_error: 2.5365e-04\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00022\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.2012e-04 - mean_squared_error: 3.2012e-04 - val_loss: 2.7941e-04 - val_mean_squared_error: 2.7941e-04\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00022\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.9974e-04 - mean_squared_error: 2.9974e-04 - val_loss: 3.1458e-04 - val_mean_squared_error: 3.1458e-04\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00022\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.8695e-04 - mean_squared_error: 2.8695e-04 - val_loss: 3.6617e-04 - val_mean_squared_error: 3.6617e-04\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00022\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.7520e-04 - mean_squared_error: 2.7520e-04 - val_loss: 4.4406e-04 - val_mean_squared_error: 4.4406e-04\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00022\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.6811e-04 - mean_squared_error: 2.6811e-04 - val_loss: 5.0762e-04 - val_mean_squared_error: 5.0762e-04\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00022\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.7655e-04 - mean_squared_error: 2.7655e-04 - val_loss: 6.1479e-04 - val_mean_squared_error: 6.1479e-04\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.00022\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.7783e-04 - mean_squared_error: 2.7783e-04 - val_loss: 6.1624e-04 - val_mean_squared_error: 6.1624e-04\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00022\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.6321e-04 - mean_squared_error: 3.6321e-04 - val_loss: 7.0806e-04 - val_mean_squared_error: 7.0806e-04\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.00022\n",
            "Epoch 00048: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 200\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 3\n",
            "MLP: Train: 0.009185, Validation: 0.000708\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAE49JREFUeJzt3WuMXPV5x/HfMzOHORhMMfaCqdd0\nXTUCbAh2WbtuCRVFCjI3g0TBiSDKiwi/oeWioMhpXwQiKtE3KUUqpU5iBalAakFoUwSlQGxIJUO6\nBlLMpXVIjVhu3jUY7NRr1svTF3Nmdy5nZmcvZ+c/O9+PZO1czpx55njnt4/+5/I3dxcAoHPk2l0A\nAGBqCG4A6DAENwB0GIIbADoMwQ0AHYbgBoAOQ3ADQIchuAGgwxDcANBhClmsdMmSJd7X15fFqgFg\nXtq9e/ewu/e0smwmwd3X16eBgYEsVg0A85KZvd3qsgyVAECHIbgBoMMQ3ADQYTIZ4waAqRodHdXg\n4KBGRkbaXUqm4jhWb2+voiia9joIbgBBGBwc1MKFC9XX1ycza3c5mXB3HThwQIODg1qxYsW019NS\ncJvZPkmHJI1JOubu/dN+RwBIMTIyMq9DW5LMTIsXL9bQ0NCM1jOVjvtP3H14Ru8GAE3M59Aum43P\nGNTOyXuf3avn/mdmf4kAYL5rNbhd0r+b2W4z25xVMf/w3Fv6OcENoA0OHjyo++67b8qvu+yyy3Tw\n4MEMKmqs1eD+krv/vqRLJd1kZn9cu4CZbTazATMbmO74TTHKa+TY2LReCwAz0Si4jx071vR1Tzzx\nhE4++eSsykrVUnC7+7vJz/2SHpO0LmWZre7e7+79PT0tnW5fp1jI6ejo59N6LQDMxJYtW/TWW29p\n9erVWrt2rS688EJt3LhRK1eulCRdffXVOv/887Vq1Spt3bp1/HV9fX0aHh7Wvn37dPbZZ+vGG2/U\nqlWrdMkll+jIkSOZ1DrpzkkzO0FSzt0PJbcvkfTdLIqJo7xGjhHcQLe7819f0+vvfTqr61z52yfp\nO1euavj83XffrT179uiVV17Rzp07dfnll2vPnj3jh+1t27ZNp5xyio4cOaK1a9fqmmuu0eLFi6vW\nsXfvXj388MP6/ve/r+uuu06PPvqobrjhhln9HFJrR5WcJumxZE9oQdJD7v5vs16Jyh03QyUA2m/d\nunVVx1rfe++9euyxxyRJ77zzjvbu3VsX3CtWrNDq1aslSeeff7727duXSW2TBre7/1rSeZm8e41i\nIaejdNxA12vWGc+VE044Yfz2zp079cwzz2jXrl1asGCBLrrootQzPIvF4vjtfD6f2VBJUIcDFqO8\nRui4AbTBwoULdejQodTnPvnkEy1atEgLFizQm2++qRdeeGGOq6sW1CnvxUJOh0aa78EFgCwsXrxY\nF1xwgc455xwdf/zxOu2008af27Bhg+6//36dffbZOvPMM7V+/fo2VhpYcMdRXsOHP2t3GQC61EMP\nPZT6eLFY1JNPPpn6XHkce8mSJdqzZ8/447fffvus11cW1lAJOycBYFKBBXeenZMAMImggjuOcuyc\nBIBJBBXcdNwAMLmggjuOcjrKtUoAoKmggrtYyGt0zDX2ube7FAAIVljBHZXKoesGELoTTzyxbe8d\nVHDHhVI5I1whEAAaCuoEnGKUl0THDWDubdmyRcuXL9dNN90kSbrjjjtUKBS0Y8cOffzxxxodHdVd\nd92lq666qs2VBhbccXmohI4b6G5PbpE+eHV217n0XOnSuxs+vWnTJt16663jwb19+3Y99dRTuvnm\nm3XSSSdpeHhY69ev18aNG9s+N2ZQwV0slDpuZsEBMNfWrFmj/fv367333tPQ0JAWLVqkpUuX6rbb\nbtPzzz+vXC6nd999Vx9++KGWLl3a1loDC246bgBq2hln6dprr9UjjzyiDz74QJs2bdKDDz6ooaEh\n7d69W1EUqa+vL/VyrnMtqOCOkzFuzp4E0A6bNm3SjTfeqOHhYT333HPavn27Tj31VEVRpB07dujt\nt99ud4mSAgvu8Y6bsycBtMGqVat06NAhLVu2TKeffrquv/56XXnllTr33HPV39+vs846q90lSgos\nuOPxo0oIbgDt8eqrEztFlyxZol27dqUud/jw4bkqqU5Qx3EXx4/jZqgEABoJLLjpuAFgMkEFd/k4\nbjpuoDu5z//rFM3GZwwquOm4ge4Vx7EOHDgwr8Pb3XXgwAHFcTyj9QS1c5KLTAHdq7e3V4ODgxoa\nGmp3KZmK41i9vb0zWkdYwc1FpoCuFUWRVqxY0e4yOkJQQyVmpuMKTKYAAM0EFdxS6dKunPIOAI0F\nF9zFKE/HDQBNhBfcdNwA0FRwwR1HeS7rCgBNBBfcdNwA0FzLwW1meTN72cwez7KgOMpzAg4ANDGV\njvsWSW9kVUhZsZDjlHcAaKKl4DazXkmXS/pBtuUkQyV03ADQUKsd9z2SviUp80SNozwdNwA0MWlw\nm9kVkva7++5JlttsZgNmNjCTaw3QcQNAc6103BdI2mhm+yT9WNLFZvaPtQu5+1Z373f3/p6enmkX\nFHMCDgA0NWlwu/u33b3X3fskfUXSz9z9hqwKKu2cpOMGgEbCO46bjhsAmprSZV3dfaeknZlUkoiT\njtvdZWZZvhUAdKQgO25J+myM4RIASBNecBfKs+AQ3ACQJrzgTjpujuUGgHThBXe54+bIEgBIFVxw\nx1F5pnc6bgBIE1xwM2EwADQXbHCzcxIA0gUX3ONDJeycBIBUwQU3HTcANBdccLNzEgCaCy642TkJ\nAM2FF9x03ADQVHDBHdNxA0BTwQU3HTcANBdccMec8g4ATQUX3IV8TvmcaYSOGwBSBRfcUjJhMB03\nAKQKMrjjKE/HDQANBBncdNwA0FiQwR1HeU55B4AGggzuYiHHDDgA0ECwwU3HDQDpwgzuKE/HDQAN\nhBncdNwA0FCgwc3OSQBoJMjgjqMcM+AAQANBBjcdNwA0FmRwx1GOqwMCQANBBnexkOd63ADQQJjB\nTccNAA0FGdxxIa/RMdfY597uUgAgOJMGt5nFZvYLM/ulmb1mZndmXVQxSiZToOsGgDqtdNxHJV3s\n7udJWi1pg5mtz7IoZsEBgMYKky3g7i7pcHI3Sv5lOoZRnneSa3IDQL2WxrjNLG9mr0jaL+lpd38x\ny6KKdNwA0FBLwe3uY+6+WlKvpHVmdk7tMma22cwGzGxgaGhoRkXFdNwA0NCUjipx94OSdkjakPLc\nVnfvd/f+np6eGRVFxw0AjbVyVEmPmZ2c3D5e0pclvZllUeWOm9PeAaDepDsnJZ0u6QEzy6sU9Nvd\n/fEsiyp33FyTGwDqtXJUyX9JWjMHtYwrFui4AaCRMM+cjOi4AaCRIIObjhsAGgsyuGNOeQeAhoIM\n7nLHzaVdAaBemMFNxw0ADYUZ3OOHA9JxA0CtIIPbzHRcgckUACBNkMEtlbpuTnkHgHrBBncc5em4\nASBFsMFNxw0A6YIN7lLHTXADQK1gg7tYyHHKOwCkCDq46bgBoF6wwR1HeTpuAEgRbHDTcQNAumCD\nm8MBASBdsMFd2jlJxw0AtQIObjpuAEgTbHDHER03AKQJNriLjHEDQKpggztOjipx93aXAgBBCTa4\ni1Fe7tJnYwyXAEClcIO7UJ4Fh+AGgErhBndUnneScW4AqBRucJc7bo4sAYAqwQZ3nHTcDJUAQLVg\ng3tiwmCGSgCgUvDBTccNANWCDe7xoRI6bgCoEmxw03EDQLpJg9vMlpvZDjN73cxeM7Nb5qKwYqG8\nc5KOGwAqFVpY5pikb7r7S2a2UNJuM3va3V/PsrA4Ku+cpOMGgEqTdtzu/r67v5TcPiTpDUnLsi6s\nGNFxA0CaKY1xm1mfpDWSXsyimEoxY9wAkKrl4DazEyU9KulWd/805fnNZjZgZgNDQ0MzLoxT3gEg\nXUvBbWaRSqH9oLv/JG0Zd9/q7v3u3t/T0zPjwjjlHQDStXJUiUn6oaQ33P172ZdUEuVzyudMI4xx\nA0CVVjruCyR9TdLFZvZK8u+yjOuSVOq66bgBoNqkhwO6+39IsjmopU4c5dk5CQA1gj1zUip13Oyc\nBIBqwQc3HTcAVAs6uOMoT8cNADWCDm46bgCoF3ZwR3lOeQeAGmEHdyHHRaYAoEbgwc3hgABQK+jg\njqMcM+AAQI2gg5uOGwDqBR3ccZRj5yQA1Ag6uIuFPDsnAaBG2MFNxw0AdYIO7riQ1+iYa+xzb3cp\nABCMoIO7GJWnL6PrBoCysIObWXAAoE7QwR2X552k4waAcUEHNx03ANQLOrjLHTcn4QDAhKCDu9xx\nc01uAJgQeHDTcQNAraCDO47ouAGgVtDBTccNAPWCDu6YE3AAoE7QwV3uuLnQFABMCDu46bgBoE7Q\nwR3TcQNAnaCDm44bAOqFHdyc8g4AdYIObjPTcYUcF5kCgApBB7dU6rrpuAFgQvDBHUd5xrgBoMKk\nwW1m28xsv5ntmYuCatFxA0C1VjruH0nakHEdDRULOU55B4AKkwa3uz8v6aM5qCVVHOW5yBQAVJi1\nMW4z22xmA2Y2MDQ0NFurpeMGgBqzFtzuvtXd+929v6enZ7ZWS8cNADWCP6qEjhsAqnVAcHM4IABU\nauVwwIcl7ZJ0ppkNmtk3si9rQhzluMgUAFQoTLaAu391LgpphI4bAKoFP1QSR4xxA0Cl4IO7yFEl\nAFAl/OBOjipx93aXAgBBCD644ygvd+mzMYZLAEDqgOAen0yBcW4AkNQJwR2V5p3kCoEAUBJ+cCcd\nNzsoAaCkY4KboRIAKAk+uONkqISOGwBKgg9uOm4AqBZ8cJc7bk57B4CS4IN7vOPmqBIAkNQRwU3H\nDQCVgg/uOCofDkjHDQBSBwR3kTFuAKgSfnBzVAkAVAk+uDmOGwCqBR/cHFUCANWCD+4on1M+Zxph\njBsAJHVAcEvJZAp03AAgqZOCm52TACCpQ4I7Zt5JABjXEcFNxw0AEzoiuOMozwk4AJDoiOAuFnKc\n8g4AiQ4JbjpuACjrjOCO6LgBoKwzgruQZ+ckACQ6IrjjKMdQCQAkOiK4i4U8Z04CQKKl4DazDWb2\n32b2KzPbknVRtYp03AAwbtLgNrO8pL+TdKmklZK+amYrsy6sUlzIs3MSABKtdNzrJP3K3X/t7p9J\n+rGkqzKp5qP/lT59TzrysXTsqOQuiY4bACoVWlhmmaR3Ku4PSvqDTKr5+z+SRv+v4gGTogX6cx2n\nrxVMH9xhMnn5meSnN12lNX12Nl+Uspqmpfn4+5hPLFj/uRo/V7tM4+dTamtaedOqK25b6u3ax2p/\nNnq+av2Wvm5PeUzj62m03pT1N1lP2nKNXpP2uVNfY+nLtfr66tfMlanVNtl3cfbeP/19pvP+0/ke\nVH5fa/2m8Fs68y9fmMZap6aV4G6JmW2WtFmSzjjjjOmt5Mp7pdHfSKNHSgE+ekQaPaLPDx/S0PvD\nGnOTXCoPmrhM7p4SCPWqt/Xc/eo3+2JWV1EfBs3CsBxsjcKwMr4bvv8Uf2utwTas/sLUvmty35v/\nkalaZ8ofsurXNnrvtPdp9scr5b3V/IvZ+HM31nC5pu8zxXV1mUbfq+n8IZxOfDf67oxFC6fx/lPX\nSnC/K2l5xf3e5LEq7r5V0lZJ6u/vn95v1xevTX34REnnTWuFADD/tDLG/Z+SvmBmK8zsOElfkfTT\nbMsCADQyacft7sfM7M8kPSUpL2mbu7+WeWUAgFQtjXG7+xOSnsi4FgBACzrizEkAwASCGwA6DMEN\nAB2G4AaADkNwA0CHMZ/G2VuTrtRsSNLb03z5EknDs1hOJ+r2bdDtn19iG0jdtw1+x917Wlkwk+Ce\nCTMbcPf+dtfRTt2+Dbr980tsA4lt0AxDJQDQYQhuAOgwIQb31nYXEIBu3wbd/vkltoHENmgouDFu\nAEBzIXbcAIAmggnudk9I3A5mts3M9pvZnorHTjGzp81sb/JzUTtrzJqZLTezHWb2upm9Zma3JI93\nzXYws9jMfmFmv0y2wZ3J4yvM7MXkO/FPyWWV5y0zy5vZy2b2eHK/qz7/VAQR3CFMSNwmP5K0oeax\nLZKedfcvSHo2uT+fHZP0TXdfKWm9pJuS//tu2g5HJV3s7udJWi1pg5mtl/TXkv7G3X9P0seSvtHG\nGufCLZLeqLjfbZ+/ZUEEt+ZyQuKAuPvzkj6qefgqSQ8ktx+QdPWcFjXH3P19d38puX1IpS/uMnXR\ndvCSw8ndKPnnki6W9Ejy+LzeBmbWK+lyST9I7pu66PNPVSjBnTYh8bI21dJup7n7+8ntDySd1s5i\n5pKZ9UlaI+lFddl2SIYJXpG0X9LTkt6SdNDdjyWLzPfvxD2SvqWJKWUXq7s+/5SEEtxI4aVDfrri\nsB8zO1HSo5JudfdPK5/rhu3g7mPuvlqlOV3XSTqrzSXNGTO7QtJ+d9/d7lo6xazN8j5DLU1I3CU+\nNLPT3f19MztdpQ5sXjOzSKXQftDdf5I83HXbQZLc/aCZ7ZD0h5JONrNC0nXO5+/EBZI2mtllkmJJ\nJ0n6W3XP55+yUDpuJiSe8FNJX09uf13Sv7SxlswlY5k/lPSGu3+v4qmu2Q5m1mNmJye3j5f0ZZXG\n+ndI+tNksXm7Ddz92+7e6+59Kn33f+bu16tLPv90BHMCTvLX9h5NTEj8V20uKXNm9rCki1S6CtqH\nkr4j6Z8lbZd0hkpXWLzO3Wt3YM4bZvYlST+X9Komxjf/QqVx7q7YDmb2RZV2vuVVaqa2u/t3zex3\nVdpRf4qklyXd4O5H21dp9szsIkm3u/sV3fj5WxVMcAMAWhPKUAkAoEUENwB0GIIbADoMwQ0AHYbg\nBoAOQ3ADQIchuAGgwxDcANBh/h9HBoRqIW54jwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 2s 14ms/step - loss: 4.6853 - mean_squared_error: 4.6853 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00233, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0040 - mean_squared_error: 0.0040 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00233 to 0.00207, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00207 to 0.00204, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00204 to 0.00203, saving model to weights.best_mlp.hdf5\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00203 to 0.00198, saving model to weights.best_mlp.hdf5\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00198 to 0.00192, saving model to weights.best_mlp.hdf5\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00192 to 0.00187, saving model to weights.best_mlp.hdf5\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.7331e-04 - mean_squared_error: 9.7331e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00187 to 0.00186, saving model to weights.best_mlp.hdf5\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.3678e-04 - mean_squared_error: 9.3678e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00186\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.0594e-04 - mean_squared_error: 9.0594e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00186\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.7391e-04 - mean_squared_error: 8.7391e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00186\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.3927e-04 - mean_squared_error: 8.3927e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00186\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.0420e-04 - mean_squared_error: 8.0420e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00186\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.7352e-04 - mean_squared_error: 7.7352e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00186\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.5104e-04 - mean_squared_error: 7.5104e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00186\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.3775e-04 - mean_squared_error: 7.3775e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00186\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.3070e-04 - mean_squared_error: 7.3070e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00186\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.2608e-04 - mean_squared_error: 7.2608e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00186\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.1951e-04 - mean_squared_error: 7.1951e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00186\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.0692e-04 - mean_squared_error: 7.0692e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00186\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.8842e-04 - mean_squared_error: 6.8842e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00186\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.6099e-04 - mean_squared_error: 6.6099e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.00186 to 0.00168, saving model to weights.best_mlp.hdf5\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.3129e-04 - mean_squared_error: 6.3129e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.00168 to 0.00142, saving model to weights.best_mlp.hdf5\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.9947e-04 - mean_squared_error: 5.9947e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.00142 to 0.00119, saving model to weights.best_mlp.hdf5\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.6733e-04 - mean_squared_error: 5.6733e-04 - val_loss: 9.7837e-04 - val_mean_squared_error: 9.7837e-04\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.00119 to 0.00098, saving model to weights.best_mlp.hdf5\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.3736e-04 - mean_squared_error: 5.3736e-04 - val_loss: 8.0368e-04 - val_mean_squared_error: 8.0368e-04\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.00098 to 0.00080, saving model to weights.best_mlp.hdf5\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.1323e-04 - mean_squared_error: 5.1323e-04 - val_loss: 6.5809e-04 - val_mean_squared_error: 6.5809e-04\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.00080 to 0.00066, saving model to weights.best_mlp.hdf5\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.9506e-04 - mean_squared_error: 4.9506e-04 - val_loss: 5.3837e-04 - val_mean_squared_error: 5.3837e-04\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.00066 to 0.00054, saving model to weights.best_mlp.hdf5\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.8121e-04 - mean_squared_error: 4.8121e-04 - val_loss: 4.4756e-04 - val_mean_squared_error: 4.4756e-04\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.00054 to 0.00045, saving model to weights.best_mlp.hdf5\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.6433e-04 - mean_squared_error: 4.6433e-04 - val_loss: 3.8601e-04 - val_mean_squared_error: 3.8601e-04\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.00045 to 0.00039, saving model to weights.best_mlp.hdf5\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.5355e-04 - mean_squared_error: 4.5355e-04 - val_loss: 3.3720e-04 - val_mean_squared_error: 3.3720e-04\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.00039 to 0.00034, saving model to weights.best_mlp.hdf5\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.4229e-04 - mean_squared_error: 4.4229e-04 - val_loss: 2.9549e-04 - val_mean_squared_error: 2.9549e-04\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.00034 to 0.00030, saving model to weights.best_mlp.hdf5\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3349e-04 - mean_squared_error: 4.3349e-04 - val_loss: 2.5897e-04 - val_mean_squared_error: 2.5897e-04\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00030 to 0.00026, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2260e-04 - mean_squared_error: 4.2260e-04 - val_loss: 2.3500e-04 - val_mean_squared_error: 2.3500e-04\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00026 to 0.00023, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.1177e-04 - mean_squared_error: 4.1177e-04 - val_loss: 2.2443e-04 - val_mean_squared_error: 2.2443e-04\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00023 to 0.00022, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.9596e-04 - mean_squared_error: 3.9596e-04 - val_loss: 2.2161e-04 - val_mean_squared_error: 2.2161e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00022 to 0.00022, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.7915e-04 - mean_squared_error: 3.7915e-04 - val_loss: 2.2590e-04 - val_mean_squared_error: 2.2590e-04\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00022\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.5576e-04 - mean_squared_error: 3.5576e-04 - val_loss: 2.4201e-04 - val_mean_squared_error: 2.4201e-04\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00022\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.3077e-04 - mean_squared_error: 3.3077e-04 - val_loss: 2.6639e-04 - val_mean_squared_error: 2.6639e-04\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00022\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.1019e-04 - mean_squared_error: 3.1019e-04 - val_loss: 2.9973e-04 - val_mean_squared_error: 2.9973e-04\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00022\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.9443e-04 - mean_squared_error: 2.9443e-04 - val_loss: 3.4301e-04 - val_mean_squared_error: 3.4301e-04\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00022\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.8180e-04 - mean_squared_error: 2.8180e-04 - val_loss: 4.0733e-04 - val_mean_squared_error: 4.0733e-04\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00022\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.7774e-04 - mean_squared_error: 2.7774e-04 - val_loss: 4.7160e-04 - val_mean_squared_error: 4.7160e-04\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00022\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.7097e-04 - mean_squared_error: 2.7097e-04 - val_loss: 5.5958e-04 - val_mean_squared_error: 5.5958e-04\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00022\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.7067e-04 - mean_squared_error: 2.7067e-04 - val_loss: 6.3895e-04 - val_mean_squared_error: 6.3895e-04\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00022\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.7596e-04 - mean_squared_error: 2.7596e-04 - val_loss: 6.8628e-04 - val_mean_squared_error: 6.8628e-04\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.00022\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.9848e-04 - mean_squared_error: 2.9848e-04 - val_loss: 7.2878e-04 - val_mean_squared_error: 7.2878e-04\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00022\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.3881e-04 - mean_squared_error: 3.3881e-04 - val_loss: 7.0325e-04 - val_mean_squared_error: 7.0325e-04\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.00022\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.9178e-04 - mean_squared_error: 4.9178e-04 - val_loss: 7.2050e-04 - val_mean_squared_error: 7.2050e-04\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.00022\n",
            "Epoch 00049: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 200\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 4\n",
            "MLP: Train: 0.009249, Validation: 0.000720\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkDL/Dxtfi1cVLlC+AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 2s 14ms/step - loss: 4.8642 - mean_squared_error: 4.8642 - val_loss: 0.0062 - val_mean_squared_error: 0.0062\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00619, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00619 to 0.00156, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00156\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00156\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00156\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00156\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00156\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.6072e-04 - mean_squared_error: 9.6072e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00156\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.2773e-04 - mean_squared_error: 9.2773e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00156\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.0327e-04 - mean_squared_error: 9.0327e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00156\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.7930e-04 - mean_squared_error: 8.7930e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00156\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.5107e-04 - mean_squared_error: 8.5107e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00156\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.1879e-04 - mean_squared_error: 8.1879e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00156\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.8644e-04 - mean_squared_error: 7.8644e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00156\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.5841e-04 - mean_squared_error: 7.5841e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00156\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.3802e-04 - mean_squared_error: 7.3802e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00156\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.2538e-04 - mean_squared_error: 7.2538e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00156\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.1764e-04 - mean_squared_error: 7.1764e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00156\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.0998e-04 - mean_squared_error: 7.0998e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00156\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.0042e-04 - mean_squared_error: 7.0042e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00156\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.8340e-04 - mean_squared_error: 6.8340e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00156\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.5938e-04 - mean_squared_error: 6.5938e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00156\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.3071e-04 - mean_squared_error: 6.3071e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00156\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.9849e-04 - mean_squared_error: 5.9849e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.00156 to 0.00136, saving model to weights.best_mlp.hdf5\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.6731e-04 - mean_squared_error: 5.6731e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.00136 to 0.00113, saving model to weights.best_mlp.hdf5\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.3627e-04 - mean_squared_error: 5.3627e-04 - val_loss: 9.3814e-04 - val_mean_squared_error: 9.3814e-04\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.00113 to 0.00094, saving model to weights.best_mlp.hdf5\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.0740e-04 - mean_squared_error: 5.0740e-04 - val_loss: 7.7753e-04 - val_mean_squared_error: 7.7753e-04\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.00094 to 0.00078, saving model to weights.best_mlp.hdf5\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.8362e-04 - mean_squared_error: 4.8362e-04 - val_loss: 6.4385e-04 - val_mean_squared_error: 6.4385e-04\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.00078 to 0.00064, saving model to weights.best_mlp.hdf5\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.6742e-04 - mean_squared_error: 4.6742e-04 - val_loss: 5.3364e-04 - val_mean_squared_error: 5.3364e-04\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.00064 to 0.00053, saving model to weights.best_mlp.hdf5\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.5392e-04 - mean_squared_error: 4.5392e-04 - val_loss: 4.5564e-04 - val_mean_squared_error: 4.5564e-04\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.00053 to 0.00046, saving model to weights.best_mlp.hdf5\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.4694e-04 - mean_squared_error: 4.4694e-04 - val_loss: 3.9510e-04 - val_mean_squared_error: 3.9510e-04\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.00046 to 0.00040, saving model to weights.best_mlp.hdf5\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3883e-04 - mean_squared_error: 4.3883e-04 - val_loss: 3.4749e-04 - val_mean_squared_error: 3.4749e-04\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.00040 to 0.00035, saving model to weights.best_mlp.hdf5\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2939e-04 - mean_squared_error: 4.2939e-04 - val_loss: 3.0329e-04 - val_mean_squared_error: 3.0329e-04\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00035 to 0.00030, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2413e-04 - mean_squared_error: 4.2413e-04 - val_loss: 2.7312e-04 - val_mean_squared_error: 2.7312e-04\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00030 to 0.00027, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.1983e-04 - mean_squared_error: 4.1983e-04 - val_loss: 2.5104e-04 - val_mean_squared_error: 2.5104e-04\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00027 to 0.00025, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.0938e-04 - mean_squared_error: 4.0938e-04 - val_loss: 2.3956e-04 - val_mean_squared_error: 2.3956e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00025 to 0.00024, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.9388e-04 - mean_squared_error: 3.9388e-04 - val_loss: 2.4001e-04 - val_mean_squared_error: 2.4001e-04\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00024\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.7791e-04 - mean_squared_error: 3.7791e-04 - val_loss: 2.4328e-04 - val_mean_squared_error: 2.4328e-04\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00024\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.5585e-04 - mean_squared_error: 3.5585e-04 - val_loss: 2.5653e-04 - val_mean_squared_error: 2.5653e-04\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00024\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.2911e-04 - mean_squared_error: 3.2911e-04 - val_loss: 2.7335e-04 - val_mean_squared_error: 2.7335e-04\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00024\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.1141e-04 - mean_squared_error: 3.1141e-04 - val_loss: 3.0196e-04 - val_mean_squared_error: 3.0196e-04\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00024\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.9571e-04 - mean_squared_error: 2.9571e-04 - val_loss: 3.3836e-04 - val_mean_squared_error: 3.3836e-04\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00024\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.8123e-04 - mean_squared_error: 2.8123e-04 - val_loss: 4.0625e-04 - val_mean_squared_error: 4.0625e-04\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00024\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.7163e-04 - mean_squared_error: 2.7163e-04 - val_loss: 4.7530e-04 - val_mean_squared_error: 4.7530e-04\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00024\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.6670e-04 - mean_squared_error: 2.6670e-04 - val_loss: 5.4573e-04 - val_mean_squared_error: 5.4573e-04\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00024\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.7467e-04 - mean_squared_error: 2.7467e-04 - val_loss: 5.8857e-04 - val_mean_squared_error: 5.8857e-04\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.00024\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.9787e-04 - mean_squared_error: 2.9787e-04 - val_loss: 6.7687e-04 - val_mean_squared_error: 6.7687e-04\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00024\n",
            "Epoch 00047: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 200\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 2\n",
            "MLP: Train: 0.009266, Validation: 0.000677\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 2s 14ms/step - loss: 4.5331 - mean_squared_error: 4.5331 - val_loss: 0.0038 - val_mean_squared_error: 0.0038\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00379, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00379 to 0.00164, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00164\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00164\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00164\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.7686e-04 - mean_squared_error: 9.7686e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00164\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.2011e-04 - mean_squared_error: 9.2011e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00164\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.8703e-04 - mean_squared_error: 8.8703e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00164\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.6429e-04 - mean_squared_error: 8.6429e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00164\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.4264e-04 - mean_squared_error: 8.4264e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00164\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.1607e-04 - mean_squared_error: 8.1607e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00164\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.8609e-04 - mean_squared_error: 7.8609e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00164\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.5645e-04 - mean_squared_error: 7.5645e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00164\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.3192e-04 - mean_squared_error: 7.3192e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00164\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.1491e-04 - mean_squared_error: 7.1491e-04 - val_loss: 0.0034 - val_mean_squared_error: 0.0034\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00164\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.0515e-04 - mean_squared_error: 7.0515e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00164\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.9865e-04 - mean_squared_error: 6.9865e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00164\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.9197e-04 - mean_squared_error: 6.9197e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00164\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.8043e-04 - mean_squared_error: 6.8043e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00164\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.6344e-04 - mean_squared_error: 6.6344e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00164\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.3790e-04 - mean_squared_error: 6.3790e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00164\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.0749e-04 - mean_squared_error: 6.0749e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00164\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.7497e-04 - mean_squared_error: 5.7497e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.00164 to 0.00142, saving model to weights.best_mlp.hdf5\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.4182e-04 - mean_squared_error: 5.4182e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.00142 to 0.00118, saving model to weights.best_mlp.hdf5\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.1004e-04 - mean_squared_error: 5.1004e-04 - val_loss: 9.6208e-04 - val_mean_squared_error: 9.6208e-04\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.00118 to 0.00096, saving model to weights.best_mlp.hdf5\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.8231e-04 - mean_squared_error: 4.8231e-04 - val_loss: 7.8195e-04 - val_mean_squared_error: 7.8195e-04\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.00096 to 0.00078, saving model to weights.best_mlp.hdf5\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.6246e-04 - mean_squared_error: 4.6246e-04 - val_loss: 6.3566e-04 - val_mean_squared_error: 6.3566e-04\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.00078 to 0.00064, saving model to weights.best_mlp.hdf5\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.4937e-04 - mean_squared_error: 4.4937e-04 - val_loss: 5.1690e-04 - val_mean_squared_error: 5.1690e-04\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.00064 to 0.00052, saving model to weights.best_mlp.hdf5\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3783e-04 - mean_squared_error: 4.3783e-04 - val_loss: 4.2823e-04 - val_mean_squared_error: 4.2823e-04\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.00052 to 0.00043, saving model to weights.best_mlp.hdf5\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3304e-04 - mean_squared_error: 4.3304e-04 - val_loss: 3.6455e-04 - val_mean_squared_error: 3.6455e-04\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.00043 to 0.00036, saving model to weights.best_mlp.hdf5\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3349e-04 - mean_squared_error: 4.3349e-04 - val_loss: 3.0882e-04 - val_mean_squared_error: 3.0882e-04\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.00036 to 0.00031, saving model to weights.best_mlp.hdf5\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2898e-04 - mean_squared_error: 4.2898e-04 - val_loss: 2.6890e-04 - val_mean_squared_error: 2.6890e-04\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.00031 to 0.00027, saving model to weights.best_mlp.hdf5\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2591e-04 - mean_squared_error: 4.2591e-04 - val_loss: 2.4269e-04 - val_mean_squared_error: 2.4269e-04\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00027 to 0.00024, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.1199e-04 - mean_squared_error: 4.1199e-04 - val_loss: 2.3163e-04 - val_mean_squared_error: 2.3163e-04\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00024 to 0.00023, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.9392e-04 - mean_squared_error: 3.9392e-04 - val_loss: 2.2383e-04 - val_mean_squared_error: 2.2383e-04\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00023 to 0.00022, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.7192e-04 - mean_squared_error: 3.7192e-04 - val_loss: 2.3455e-04 - val_mean_squared_error: 2.3455e-04\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00022\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.4530e-04 - mean_squared_error: 3.4530e-04 - val_loss: 2.4516e-04 - val_mean_squared_error: 2.4516e-04\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00022\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.2640e-04 - mean_squared_error: 3.2640e-04 - val_loss: 2.7382e-04 - val_mean_squared_error: 2.7382e-04\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00022\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.0696e-04 - mean_squared_error: 3.0696e-04 - val_loss: 3.0714e-04 - val_mean_squared_error: 3.0714e-04\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00022\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.9967e-04 - mean_squared_error: 2.9967e-04 - val_loss: 3.5699e-04 - val_mean_squared_error: 3.5699e-04\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00022\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.8152e-04 - mean_squared_error: 2.8152e-04 - val_loss: 4.2500e-04 - val_mean_squared_error: 4.2500e-04\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00022\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.7353e-04 - mean_squared_error: 2.7353e-04 - val_loss: 5.1460e-04 - val_mean_squared_error: 5.1460e-04\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00022\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.7921e-04 - mean_squared_error: 2.7921e-04 - val_loss: 5.8475e-04 - val_mean_squared_error: 5.8475e-04\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00022\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.6871e-04 - mean_squared_error: 2.6871e-04 - val_loss: 6.4785e-04 - val_mean_squared_error: 6.4785e-04\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00022\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.8488e-04 - mean_squared_error: 2.8488e-04 - val_loss: 6.8061e-04 - val_mean_squared_error: 6.8061e-04\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00022\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.3337e-04 - mean_squared_error: 3.3337e-04 - val_loss: 7.1285e-04 - val_mean_squared_error: 7.1285e-04\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.00022\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.8477e-04 - mean_squared_error: 4.8477e-04 - val_loss: 6.2065e-04 - val_mean_squared_error: 6.2065e-04\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00022\n",
            "Epoch 00047: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 200\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 3\n",
            "MLP: Train: 0.009365, Validation: 0.000621\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDQAd5v8B+8JjbavHEHEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 3s 15ms/step - loss: 4.9407 - mean_squared_error: 4.9407 - val_loss: 0.0063 - val_mean_squared_error: 0.0063\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00626, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00626 to 0.00146, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00146\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00146\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00146\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00146\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.8709e-04 - mean_squared_error: 9.8709e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00146\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.5049e-04 - mean_squared_error: 9.5049e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00146\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.3016e-04 - mean_squared_error: 9.3016e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00146\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.1315e-04 - mean_squared_error: 9.1315e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00146\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.9455e-04 - mean_squared_error: 8.9455e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00146\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.6887e-04 - mean_squared_error: 8.6887e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00146\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.3898e-04 - mean_squared_error: 8.3898e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00146\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.0954e-04 - mean_squared_error: 8.0954e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00146\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.8466e-04 - mean_squared_error: 7.8466e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00146\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.6783e-04 - mean_squared_error: 7.6783e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00146\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.5796e-04 - mean_squared_error: 7.5796e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00146\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.5170e-04 - mean_squared_error: 7.5170e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00146\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.4635e-04 - mean_squared_error: 7.4635e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00146\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.3624e-04 - mean_squared_error: 7.3624e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00146\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.1799e-04 - mean_squared_error: 7.1799e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00146\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.9290e-04 - mean_squared_error: 6.9290e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00146\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.6261e-04 - mean_squared_error: 6.6261e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00146\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.2788e-04 - mean_squared_error: 6.2788e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.00146 to 0.00133, saving model to weights.best_mlp.hdf5\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.9214e-04 - mean_squared_error: 5.9214e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.00133 to 0.00110, saving model to weights.best_mlp.hdf5\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.5863e-04 - mean_squared_error: 5.5863e-04 - val_loss: 9.1226e-04 - val_mean_squared_error: 9.1226e-04\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.00110 to 0.00091, saving model to weights.best_mlp.hdf5\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.3018e-04 - mean_squared_error: 5.3018e-04 - val_loss: 7.5043e-04 - val_mean_squared_error: 7.5043e-04\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.00091 to 0.00075, saving model to weights.best_mlp.hdf5\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.0730e-04 - mean_squared_error: 5.0730e-04 - val_loss: 6.1065e-04 - val_mean_squared_error: 6.1065e-04\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.00075 to 0.00061, saving model to weights.best_mlp.hdf5\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.9035e-04 - mean_squared_error: 4.9035e-04 - val_loss: 5.0072e-04 - val_mean_squared_error: 5.0072e-04\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.00061 to 0.00050, saving model to weights.best_mlp.hdf5\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.7841e-04 - mean_squared_error: 4.7841e-04 - val_loss: 4.2055e-04 - val_mean_squared_error: 4.2055e-04\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.00050 to 0.00042, saving model to weights.best_mlp.hdf5\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.6848e-04 - mean_squared_error: 4.6848e-04 - val_loss: 3.6288e-04 - val_mean_squared_error: 3.6288e-04\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.00042 to 0.00036, saving model to weights.best_mlp.hdf5\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.6030e-04 - mean_squared_error: 4.6030e-04 - val_loss: 3.1716e-04 - val_mean_squared_error: 3.1716e-04\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.00036 to 0.00032, saving model to weights.best_mlp.hdf5\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.5430e-04 - mean_squared_error: 4.5430e-04 - val_loss: 2.7724e-04 - val_mean_squared_error: 2.7724e-04\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00032 to 0.00028, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.4502e-04 - mean_squared_error: 4.4502e-04 - val_loss: 2.4579e-04 - val_mean_squared_error: 2.4579e-04\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00028 to 0.00025, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3656e-04 - mean_squared_error: 4.3656e-04 - val_loss: 2.2628e-04 - val_mean_squared_error: 2.2628e-04\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00025 to 0.00023, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2102e-04 - mean_squared_error: 4.2102e-04 - val_loss: 2.1967e-04 - val_mean_squared_error: 2.1967e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00023 to 0.00022, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.0483e-04 - mean_squared_error: 4.0483e-04 - val_loss: 2.2109e-04 - val_mean_squared_error: 2.2109e-04\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00022\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.8174e-04 - mean_squared_error: 3.8174e-04 - val_loss: 2.3041e-04 - val_mean_squared_error: 2.3041e-04\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00022\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.6260e-04 - mean_squared_error: 3.6260e-04 - val_loss: 2.4287e-04 - val_mean_squared_error: 2.4287e-04\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00022\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.4116e-04 - mean_squared_error: 3.4116e-04 - val_loss: 2.5923e-04 - val_mean_squared_error: 2.5923e-04\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00022\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.2679e-04 - mean_squared_error: 3.2679e-04 - val_loss: 2.8680e-04 - val_mean_squared_error: 2.8680e-04\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00022\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.2176e-04 - mean_squared_error: 3.2176e-04 - val_loss: 3.1412e-04 - val_mean_squared_error: 3.1412e-04\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00022\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.9728e-04 - mean_squared_error: 2.9728e-04 - val_loss: 3.7144e-04 - val_mean_squared_error: 3.7144e-04\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00022\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.9667e-04 - mean_squared_error: 2.9667e-04 - val_loss: 4.2189e-04 - val_mean_squared_error: 4.2189e-04\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00022\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.7462e-04 - mean_squared_error: 2.7462e-04 - val_loss: 5.2098e-04 - val_mean_squared_error: 5.2098e-04\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00022\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.1013e-04 - mean_squared_error: 3.1013e-04 - val_loss: 5.3216e-04 - val_mean_squared_error: 5.3216e-04\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.00022\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.4064e-04 - mean_squared_error: 3.4064e-04 - val_loss: 6.5773e-04 - val_mean_squared_error: 6.5773e-04\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00022\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3992e-04 - mean_squared_error: 4.3992e-04 - val_loss: 4.8262e-04 - val_mean_squared_error: 4.8262e-04\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.00022\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.1123e-04 - mean_squared_error: 5.1123e-04 - val_loss: 6.5733e-04 - val_mean_squared_error: 6.5733e-04\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.00022\n",
            "Epoch 00049: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 200\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 4\n",
            "MLP: Train: 0.009429, Validation: 0.000657\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 3s 15ms/step - loss: 4.9104 - mean_squared_error: 4.9104 - val_loss: 0.0039 - val_mean_squared_error: 0.0039\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00391, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00391 to 0.00200, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00200\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00200\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00200 to 0.00200, saving model to weights.best_mlp.hdf5\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00200 to 0.00194, saving model to weights.best_mlp.hdf5\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00194 to 0.00188, saving model to weights.best_mlp.hdf5\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.8092e-04 - mean_squared_error: 9.8092e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00188 to 0.00185, saving model to weights.best_mlp.hdf5\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.3886e-04 - mean_squared_error: 9.3886e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00185\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.0761e-04 - mean_squared_error: 9.0761e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00185\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.7835e-04 - mean_squared_error: 8.7835e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00185\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.4694e-04 - mean_squared_error: 8.4694e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00185\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.1302e-04 - mean_squared_error: 8.1302e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00185\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.8044e-04 - mean_squared_error: 7.8044e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00185\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.5373e-04 - mean_squared_error: 7.5373e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00185\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.3483e-04 - mean_squared_error: 7.3483e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00185\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.2405e-04 - mean_squared_error: 7.2405e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00185\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.1817e-04 - mean_squared_error: 7.1817e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00185\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.1335e-04 - mean_squared_error: 7.1335e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00185\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.0320e-04 - mean_squared_error: 7.0320e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00185\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.8807e-04 - mean_squared_error: 6.8807e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00185\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.6650e-04 - mean_squared_error: 6.6650e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00185\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.3818e-04 - mean_squared_error: 6.3818e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.00185 to 0.00166, saving model to weights.best_mlp.hdf5\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.0732e-04 - mean_squared_error: 6.0732e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.00166 to 0.00141, saving model to weights.best_mlp.hdf5\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.7568e-04 - mean_squared_error: 5.7568e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.00141 to 0.00119, saving model to weights.best_mlp.hdf5\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.4535e-04 - mean_squared_error: 5.4535e-04 - val_loss: 9.9022e-04 - val_mean_squared_error: 9.9022e-04\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.00119 to 0.00099, saving model to weights.best_mlp.hdf5\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.1926e-04 - mean_squared_error: 5.1926e-04 - val_loss: 8.1785e-04 - val_mean_squared_error: 8.1785e-04\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.00099 to 0.00082, saving model to weights.best_mlp.hdf5\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.9534e-04 - mean_squared_error: 4.9534e-04 - val_loss: 6.6942e-04 - val_mean_squared_error: 6.6942e-04\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.00082 to 0.00067, saving model to weights.best_mlp.hdf5\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.7716e-04 - mean_squared_error: 4.7716e-04 - val_loss: 5.5304e-04 - val_mean_squared_error: 5.5304e-04\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.00067 to 0.00055, saving model to weights.best_mlp.hdf5\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.6200e-04 - mean_squared_error: 4.6200e-04 - val_loss: 4.6828e-04 - val_mean_squared_error: 4.6828e-04\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.00055 to 0.00047, saving model to weights.best_mlp.hdf5\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.5275e-04 - mean_squared_error: 4.5275e-04 - val_loss: 4.0889e-04 - val_mean_squared_error: 4.0889e-04\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.00047 to 0.00041, saving model to weights.best_mlp.hdf5\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.4514e-04 - mean_squared_error: 4.4514e-04 - val_loss: 3.5788e-04 - val_mean_squared_error: 3.5788e-04\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.00041 to 0.00036, saving model to weights.best_mlp.hdf5\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3975e-04 - mean_squared_error: 4.3975e-04 - val_loss: 3.1566e-04 - val_mean_squared_error: 3.1566e-04\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00036 to 0.00032, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3400e-04 - mean_squared_error: 4.3400e-04 - val_loss: 2.7633e-04 - val_mean_squared_error: 2.7633e-04\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00032 to 0.00028, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2497e-04 - mean_squared_error: 4.2497e-04 - val_loss: 2.5465e-04 - val_mean_squared_error: 2.5465e-04\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00028 to 0.00025, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.1637e-04 - mean_squared_error: 4.1637e-04 - val_loss: 2.3875e-04 - val_mean_squared_error: 2.3875e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00025 to 0.00024, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.9749e-04 - mean_squared_error: 3.9749e-04 - val_loss: 2.4046e-04 - val_mean_squared_error: 2.4046e-04\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00024\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.8369e-04 - mean_squared_error: 3.8369e-04 - val_loss: 2.3674e-04 - val_mean_squared_error: 2.3674e-04\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.00024 to 0.00024, saving model to weights.best_mlp.hdf5\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.6277e-04 - mean_squared_error: 3.6277e-04 - val_loss: 2.5696e-04 - val_mean_squared_error: 2.5696e-04\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00024\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.5226e-04 - mean_squared_error: 3.5226e-04 - val_loss: 2.6128e-04 - val_mean_squared_error: 2.6128e-04\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00024\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.3166e-04 - mean_squared_error: 3.3166e-04 - val_loss: 2.9112e-04 - val_mean_squared_error: 2.9112e-04\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00024\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.1823e-04 - mean_squared_error: 3.1823e-04 - val_loss: 3.1532e-04 - val_mean_squared_error: 3.1532e-04\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00024\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.9869e-04 - mean_squared_error: 2.9869e-04 - val_loss: 3.7970e-04 - val_mean_squared_error: 3.7970e-04\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00024\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.0658e-04 - mean_squared_error: 3.0658e-04 - val_loss: 4.0592e-04 - val_mean_squared_error: 4.0592e-04\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00024\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.0591e-04 - mean_squared_error: 3.0591e-04 - val_loss: 5.1477e-04 - val_mean_squared_error: 5.1477e-04\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00024\n",
            "Epoch 00045: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 200\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 2\n",
            "MLP: Train: 0.009930, Validation: 0.000515\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 3s 15ms/step - loss: 5.0879 - mean_squared_error: 5.0879 - val_loss: 0.0068 - val_mean_squared_error: 0.0068\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00676, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00676 to 0.00157, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00157\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00157\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00157\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00157\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.9159e-04 - mean_squared_error: 9.9159e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00157\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.5039e-04 - mean_squared_error: 9.5039e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00157\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.2762e-04 - mean_squared_error: 9.2762e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00157\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.1171e-04 - mean_squared_error: 9.1171e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00157\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.9399e-04 - mean_squared_error: 8.9399e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00157\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.7042e-04 - mean_squared_error: 8.7042e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00157\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.4167e-04 - mean_squared_error: 8.4167e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00157\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.1235e-04 - mean_squared_error: 8.1235e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00157\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.8758e-04 - mean_squared_error: 7.8758e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00157\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.7018e-04 - mean_squared_error: 7.7018e-04 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00157\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.5899e-04 - mean_squared_error: 7.5899e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00157\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.5301e-04 - mean_squared_error: 7.5301e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00157\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.4668e-04 - mean_squared_error: 7.4668e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00157\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.3622e-04 - mean_squared_error: 7.3622e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00157\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.2012e-04 - mean_squared_error: 7.2012e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00157\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.9709e-04 - mean_squared_error: 6.9709e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00157\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.6929e-04 - mean_squared_error: 6.6929e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00157\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.3583e-04 - mean_squared_error: 6.3583e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.00157 to 0.00135, saving model to weights.best_mlp.hdf5\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.9951e-04 - mean_squared_error: 5.9951e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.00135 to 0.00112, saving model to weights.best_mlp.hdf5\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.6718e-04 - mean_squared_error: 5.6718e-04 - val_loss: 9.3098e-04 - val_mean_squared_error: 9.3098e-04\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.00112 to 0.00093, saving model to weights.best_mlp.hdf5\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.3837e-04 - mean_squared_error: 5.3837e-04 - val_loss: 7.7354e-04 - val_mean_squared_error: 7.7354e-04\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.00093 to 0.00077, saving model to weights.best_mlp.hdf5\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.1542e-04 - mean_squared_error: 5.1542e-04 - val_loss: 6.3560e-04 - val_mean_squared_error: 6.3560e-04\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.00077 to 0.00064, saving model to weights.best_mlp.hdf5\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.9563e-04 - mean_squared_error: 4.9563e-04 - val_loss: 5.2951e-04 - val_mean_squared_error: 5.2951e-04\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.00064 to 0.00053, saving model to weights.best_mlp.hdf5\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.8178e-04 - mean_squared_error: 4.8178e-04 - val_loss: 4.4122e-04 - val_mean_squared_error: 4.4122e-04\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.00053 to 0.00044, saving model to weights.best_mlp.hdf5\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.7092e-04 - mean_squared_error: 4.7092e-04 - val_loss: 3.8831e-04 - val_mean_squared_error: 3.8831e-04\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.00044 to 0.00039, saving model to weights.best_mlp.hdf5\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.6206e-04 - mean_squared_error: 4.6206e-04 - val_loss: 3.4099e-04 - val_mean_squared_error: 3.4099e-04\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.00039 to 0.00034, saving model to weights.best_mlp.hdf5\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.5444e-04 - mean_squared_error: 4.5444e-04 - val_loss: 3.0137e-04 - val_mean_squared_error: 3.0137e-04\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00034 to 0.00030, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.5000e-04 - mean_squared_error: 4.5000e-04 - val_loss: 2.6625e-04 - val_mean_squared_error: 2.6625e-04\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00030 to 0.00027, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3926e-04 - mean_squared_error: 4.3926e-04 - val_loss: 2.4523e-04 - val_mean_squared_error: 2.4523e-04\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00027 to 0.00025, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3521e-04 - mean_squared_error: 4.3521e-04 - val_loss: 2.3394e-04 - val_mean_squared_error: 2.3394e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00025 to 0.00023, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.1342e-04 - mean_squared_error: 4.1342e-04 - val_loss: 2.2998e-04 - val_mean_squared_error: 2.2998e-04\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.00023 to 0.00023, saving model to weights.best_mlp.hdf5\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.0905e-04 - mean_squared_error: 4.0905e-04 - val_loss: 2.2668e-04 - val_mean_squared_error: 2.2668e-04\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.00023 to 0.00023, saving model to weights.best_mlp.hdf5\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.7203e-04 - mean_squared_error: 3.7203e-04 - val_loss: 2.3993e-04 - val_mean_squared_error: 2.3993e-04\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00023\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.8102e-04 - mean_squared_error: 3.8102e-04 - val_loss: 2.4550e-04 - val_mean_squared_error: 2.4550e-04\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00023\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.4335e-04 - mean_squared_error: 3.4335e-04 - val_loss: 2.7206e-04 - val_mean_squared_error: 2.7206e-04\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00023\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.3531e-04 - mean_squared_error: 3.3531e-04 - val_loss: 2.9165e-04 - val_mean_squared_error: 2.9165e-04\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00023\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.1258e-04 - mean_squared_error: 3.1258e-04 - val_loss: 3.3167e-04 - val_mean_squared_error: 3.3167e-04\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00023\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.0841e-04 - mean_squared_error: 3.0841e-04 - val_loss: 3.7207e-04 - val_mean_squared_error: 3.7207e-04\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00023\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.8840e-04 - mean_squared_error: 2.8840e-04 - val_loss: 4.4112e-04 - val_mean_squared_error: 4.4112e-04\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00023\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.8765e-04 - mean_squared_error: 2.8765e-04 - val_loss: 5.0590e-04 - val_mean_squared_error: 5.0590e-04\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.00023\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.7919e-04 - mean_squared_error: 2.7919e-04 - val_loss: 5.8122e-04 - val_mean_squared_error: 5.8122e-04\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00023\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.7001e-04 - mean_squared_error: 2.7001e-04 - val_loss: 6.2143e-04 - val_mean_squared_error: 6.2143e-04\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.00023\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.8448e-04 - mean_squared_error: 2.8448e-04 - val_loss: 7.0095e-04 - val_mean_squared_error: 7.0095e-04\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.00023\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.9933e-04 - mean_squared_error: 2.9933e-04 - val_loss: 6.7080e-04 - val_mean_squared_error: 6.7080e-04\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.00023\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.9089e-04 - mean_squared_error: 2.9089e-04 - val_loss: 6.8037e-04 - val_mean_squared_error: 6.8037e-04\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.00023\n",
            "Epoch 00051: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 200\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 3\n",
            "MLP: Train: 0.009146, Validation: 0.000680\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzfzfUwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 3s 16ms/step - loss: 4.7851 - mean_squared_error: 4.7851 - val_loss: 0.0050 - val_mean_squared_error: 0.0050\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00498, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00498 to 0.00165, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00165\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00165\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00165\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00165\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.8104e-04 - mean_squared_error: 9.8104e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00165\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.3933e-04 - mean_squared_error: 9.3933e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00165\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.1194e-04 - mean_squared_error: 9.1194e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00165\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.8777e-04 - mean_squared_error: 8.8777e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00165\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.6020e-04 - mean_squared_error: 8.6020e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00165\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.2761e-04 - mean_squared_error: 8.2761e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00165\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.9315e-04 - mean_squared_error: 7.9315e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00165\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.6200e-04 - mean_squared_error: 7.6200e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00165\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.3829e-04 - mean_squared_error: 7.3829e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00165\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.2366e-04 - mean_squared_error: 7.2366e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00165\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.1625e-04 - mean_squared_error: 7.1625e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00165\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.1120e-04 - mean_squared_error: 7.1120e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00165\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.0423e-04 - mean_squared_error: 7.0423e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00165\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.9143e-04 - mean_squared_error: 6.9143e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00165\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.7270e-04 - mean_squared_error: 6.7270e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00165\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.4750e-04 - mean_squared_error: 6.4750e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00165\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.1840e-04 - mean_squared_error: 6.1840e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.00165 to 0.00148, saving model to weights.best_mlp.hdf5\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.8531e-04 - mean_squared_error: 5.8531e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.00148 to 0.00124, saving model to weights.best_mlp.hdf5\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.5447e-04 - mean_squared_error: 5.5447e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.00124 to 0.00103, saving model to weights.best_mlp.hdf5\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.2561e-04 - mean_squared_error: 5.2561e-04 - val_loss: 8.4685e-04 - val_mean_squared_error: 8.4685e-04\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.00103 to 0.00085, saving model to weights.best_mlp.hdf5\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.0295e-04 - mean_squared_error: 5.0295e-04 - val_loss: 6.9262e-04 - val_mean_squared_error: 6.9262e-04\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.00085 to 0.00069, saving model to weights.best_mlp.hdf5\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.8299e-04 - mean_squared_error: 4.8299e-04 - val_loss: 5.6235e-04 - val_mean_squared_error: 5.6235e-04\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.00069 to 0.00056, saving model to weights.best_mlp.hdf5\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.6923e-04 - mean_squared_error: 4.6923e-04 - val_loss: 4.6240e-04 - val_mean_squared_error: 4.6240e-04\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.00056 to 0.00046, saving model to weights.best_mlp.hdf5\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.5701e-04 - mean_squared_error: 4.5701e-04 - val_loss: 3.9243e-04 - val_mean_squared_error: 3.9243e-04\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.00046 to 0.00039, saving model to weights.best_mlp.hdf5\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.4651e-04 - mean_squared_error: 4.4651e-04 - val_loss: 3.3798e-04 - val_mean_squared_error: 3.3798e-04\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.00039 to 0.00034, saving model to weights.best_mlp.hdf5\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.4003e-04 - mean_squared_error: 4.4003e-04 - val_loss: 2.8631e-04 - val_mean_squared_error: 2.8631e-04\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.00034 to 0.00029, saving model to weights.best_mlp.hdf5\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3386e-04 - mean_squared_error: 4.3386e-04 - val_loss: 2.5650e-04 - val_mean_squared_error: 2.5650e-04\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00029 to 0.00026, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2857e-04 - mean_squared_error: 4.2857e-04 - val_loss: 2.3234e-04 - val_mean_squared_error: 2.3234e-04\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00026 to 0.00023, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.1765e-04 - mean_squared_error: 4.1765e-04 - val_loss: 2.2265e-04 - val_mean_squared_error: 2.2265e-04\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00023 to 0.00022, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.9872e-04 - mean_squared_error: 3.9872e-04 - val_loss: 2.2055e-04 - val_mean_squared_error: 2.2055e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00022 to 0.00022, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.7927e-04 - mean_squared_error: 3.7927e-04 - val_loss: 2.2528e-04 - val_mean_squared_error: 2.2528e-04\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00022\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.6622e-04 - mean_squared_error: 3.6622e-04 - val_loss: 2.3254e-04 - val_mean_squared_error: 2.3254e-04\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00022\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.4652e-04 - mean_squared_error: 3.4652e-04 - val_loss: 2.4834e-04 - val_mean_squared_error: 2.4834e-04\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00022\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.1931e-04 - mean_squared_error: 3.1931e-04 - val_loss: 2.7737e-04 - val_mean_squared_error: 2.7737e-04\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00022\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.0020e-04 - mean_squared_error: 3.0020e-04 - val_loss: 3.1694e-04 - val_mean_squared_error: 3.1694e-04\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00022\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.8354e-04 - mean_squared_error: 2.8354e-04 - val_loss: 3.7607e-04 - val_mean_squared_error: 3.7607e-04\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00022\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.7201e-04 - mean_squared_error: 2.7201e-04 - val_loss: 4.5388e-04 - val_mean_squared_error: 4.5388e-04\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00022\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.7405e-04 - mean_squared_error: 2.7405e-04 - val_loss: 5.4037e-04 - val_mean_squared_error: 5.4037e-04\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00022\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.9661e-04 - mean_squared_error: 2.9661e-04 - val_loss: 6.3168e-04 - val_mean_squared_error: 6.3168e-04\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00022\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.6614e-04 - mean_squared_error: 2.6614e-04 - val_loss: 7.1063e-04 - val_mean_squared_error: 7.1063e-04\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.00022\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.8033e-04 - mean_squared_error: 2.8033e-04 - val_loss: 6.9137e-04 - val_mean_squared_error: 6.9137e-04\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00022\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.6998e-04 - mean_squared_error: 3.6998e-04 - val_loss: 7.7339e-04 - val_mean_squared_error: 7.7339e-04\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.00022\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.5859e-04 - mean_squared_error: 4.5859e-04 - val_loss: 6.2751e-04 - val_mean_squared_error: 6.2751e-04\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.00022\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.6204e-04 - mean_squared_error: 4.6204e-04 - val_loss: 7.0449e-04 - val_mean_squared_error: 7.0449e-04\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.00022\n",
            "Epoch 00050: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 200\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 4\n",
            "MLP: Train: 0.009276, Validation: 0.000704\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 3s 15ms/step - loss: 4.9757 - mean_squared_error: 4.9757 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00313, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00313 to 0.00162, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00162\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00162\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00162\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00162\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.9229e-04 - mean_squared_error: 9.9229e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00162\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.5250e-04 - mean_squared_error: 9.5250e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00162\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.2676e-04 - mean_squared_error: 9.2676e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00162\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.0574e-04 - mean_squared_error: 9.0574e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00162\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.8193e-04 - mean_squared_error: 8.8193e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00162\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.5236e-04 - mean_squared_error: 8.5236e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00162\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.1872e-04 - mean_squared_error: 8.1872e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00162\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.8554e-04 - mean_squared_error: 7.8554e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00162\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.5869e-04 - mean_squared_error: 7.5869e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00162\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.3871e-04 - mean_squared_error: 7.3871e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00162\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.2716e-04 - mean_squared_error: 7.2716e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00162\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.1908e-04 - mean_squared_error: 7.1908e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00162\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.1151e-04 - mean_squared_error: 7.1151e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00162\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.9932e-04 - mean_squared_error: 6.9932e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00162\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.8031e-04 - mean_squared_error: 6.8031e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00162\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.5557e-04 - mean_squared_error: 6.5557e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00162\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.2592e-04 - mean_squared_error: 6.2592e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00162\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.9162e-04 - mean_squared_error: 5.9162e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.00162 to 0.00137, saving model to weights.best_mlp.hdf5\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.5503e-04 - mean_squared_error: 5.5503e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.00137 to 0.00115, saving model to weights.best_mlp.hdf5\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.2197e-04 - mean_squared_error: 5.2197e-04 - val_loss: 9.7207e-04 - val_mean_squared_error: 9.7207e-04\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.00115 to 0.00097, saving model to weights.best_mlp.hdf5\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.9193e-04 - mean_squared_error: 4.9193e-04 - val_loss: 8.1744e-04 - val_mean_squared_error: 8.1744e-04\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.00097 to 0.00082, saving model to weights.best_mlp.hdf5\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.6960e-04 - mean_squared_error: 4.6960e-04 - val_loss: 6.8664e-04 - val_mean_squared_error: 6.8664e-04\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.00082 to 0.00069, saving model to weights.best_mlp.hdf5\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.5319e-04 - mean_squared_error: 4.5319e-04 - val_loss: 5.7781e-04 - val_mean_squared_error: 5.7781e-04\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.00069 to 0.00058, saving model to weights.best_mlp.hdf5\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.4181e-04 - mean_squared_error: 4.4181e-04 - val_loss: 4.9911e-04 - val_mean_squared_error: 4.9911e-04\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.00058 to 0.00050, saving model to weights.best_mlp.hdf5\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3534e-04 - mean_squared_error: 4.3534e-04 - val_loss: 4.3976e-04 - val_mean_squared_error: 4.3976e-04\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.00050 to 0.00044, saving model to weights.best_mlp.hdf5\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3234e-04 - mean_squared_error: 4.3234e-04 - val_loss: 3.8884e-04 - val_mean_squared_error: 3.8884e-04\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.00044 to 0.00039, saving model to weights.best_mlp.hdf5\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2598e-04 - mean_squared_error: 4.2598e-04 - val_loss: 3.3897e-04 - val_mean_squared_error: 3.3897e-04\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00039 to 0.00034, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2278e-04 - mean_squared_error: 4.2278e-04 - val_loss: 2.9793e-04 - val_mean_squared_error: 2.9793e-04\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00034 to 0.00030, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.1396e-04 - mean_squared_error: 4.1396e-04 - val_loss: 2.7125e-04 - val_mean_squared_error: 2.7125e-04\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00030 to 0.00027, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.0389e-04 - mean_squared_error: 4.0389e-04 - val_loss: 2.5956e-04 - val_mean_squared_error: 2.5956e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00027 to 0.00026, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.8712e-04 - mean_squared_error: 3.8712e-04 - val_loss: 2.5515e-04 - val_mean_squared_error: 2.5515e-04\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.00026 to 0.00026, saving model to weights.best_mlp.hdf5\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.6879e-04 - mean_squared_error: 3.6879e-04 - val_loss: 2.5910e-04 - val_mean_squared_error: 2.5910e-04\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00026\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.5073e-04 - mean_squared_error: 3.5073e-04 - val_loss: 2.6551e-04 - val_mean_squared_error: 2.6551e-04\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00026\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.3210e-04 - mean_squared_error: 3.3210e-04 - val_loss: 2.8417e-04 - val_mean_squared_error: 2.8417e-04\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00026\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.1622e-04 - mean_squared_error: 3.1622e-04 - val_loss: 3.0907e-04 - val_mean_squared_error: 3.0907e-04\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00026\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.0774e-04 - mean_squared_error: 3.0774e-04 - val_loss: 3.2755e-04 - val_mean_squared_error: 3.2755e-04\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00026\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.0376e-04 - mean_squared_error: 3.0376e-04 - val_loss: 3.6514e-04 - val_mean_squared_error: 3.6514e-04\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00026\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.8434e-04 - mean_squared_error: 2.8434e-04 - val_loss: 4.2922e-04 - val_mean_squared_error: 4.2922e-04\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00026\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.8615e-04 - mean_squared_error: 2.8615e-04 - val_loss: 4.8115e-04 - val_mean_squared_error: 4.8115e-04\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00026\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.7684e-04 - mean_squared_error: 2.7684e-04 - val_loss: 5.4609e-04 - val_mean_squared_error: 5.4609e-04\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.00026\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.1126e-04 - mean_squared_error: 3.1126e-04 - val_loss: 6.0558e-04 - val_mean_squared_error: 6.0558e-04\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00026\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.1913e-04 - mean_squared_error: 3.1913e-04 - val_loss: 6.3728e-04 - val_mean_squared_error: 6.3728e-04\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.00026\n",
            "Epoch 00048: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 200\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 2\n",
            "MLP: Train: 0.009462, Validation: 0.000637\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nboPnAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 3s 16ms/step - loss: 5.1769 - mean_squared_error: 5.1769 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00875, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00875 to 0.00179, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00179\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00179\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00179\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00179\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00179\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00179\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.7325e-04 - mean_squared_error: 9.7325e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00179\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.4951e-04 - mean_squared_error: 9.4951e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00179\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.2750e-04 - mean_squared_error: 9.2750e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00179\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.0172e-04 - mean_squared_error: 9.0172e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00179\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.7043e-04 - mean_squared_error: 8.7043e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00179\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.3682e-04 - mean_squared_error: 8.3682e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00179\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.0530e-04 - mean_squared_error: 8.0530e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00179\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.8073e-04 - mean_squared_error: 7.8073e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00179\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.6494e-04 - mean_squared_error: 7.6494e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00179\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.5484e-04 - mean_squared_error: 7.5484e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00179\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.4939e-04 - mean_squared_error: 7.4939e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00179\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.4107e-04 - mean_squared_error: 7.4107e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00179\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.2872e-04 - mean_squared_error: 7.2872e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00179\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.1041e-04 - mean_squared_error: 7.1041e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00179\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.8138e-04 - mean_squared_error: 6.8138e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.00179 to 0.00172, saving model to weights.best_mlp.hdf5\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.4956e-04 - mean_squared_error: 6.4956e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.00172 to 0.00147, saving model to weights.best_mlp.hdf5\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.1295e-04 - mean_squared_error: 6.1295e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.00147 to 0.00124, saving model to weights.best_mlp.hdf5\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.7720e-04 - mean_squared_error: 5.7720e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.00124 to 0.00104, saving model to weights.best_mlp.hdf5\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.3802e-04 - mean_squared_error: 5.3802e-04 - val_loss: 8.7294e-04 - val_mean_squared_error: 8.7294e-04\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.00104 to 0.00087, saving model to weights.best_mlp.hdf5\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.0711e-04 - mean_squared_error: 5.0711e-04 - val_loss: 7.3182e-04 - val_mean_squared_error: 7.3182e-04\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.00087 to 0.00073, saving model to weights.best_mlp.hdf5\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.8187e-04 - mean_squared_error: 4.8187e-04 - val_loss: 6.1468e-04 - val_mean_squared_error: 6.1468e-04\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.00073 to 0.00061, saving model to weights.best_mlp.hdf5\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.6409e-04 - mean_squared_error: 4.6409e-04 - val_loss: 5.1926e-04 - val_mean_squared_error: 5.1926e-04\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.00061 to 0.00052, saving model to weights.best_mlp.hdf5\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.5434e-04 - mean_squared_error: 4.5434e-04 - val_loss: 4.4127e-04 - val_mean_squared_error: 4.4127e-04\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.00052 to 0.00044, saving model to weights.best_mlp.hdf5\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.4212e-04 - mean_squared_error: 4.4212e-04 - val_loss: 3.8775e-04 - val_mean_squared_error: 3.8775e-04\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.00044 to 0.00039, saving model to weights.best_mlp.hdf5\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.4336e-04 - mean_squared_error: 4.4336e-04 - val_loss: 3.4319e-04 - val_mean_squared_error: 3.4319e-04\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00039 to 0.00034, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.4419e-04 - mean_squared_error: 4.4419e-04 - val_loss: 3.0625e-04 - val_mean_squared_error: 3.0625e-04\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00034 to 0.00031, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3934e-04 - mean_squared_error: 4.3934e-04 - val_loss: 2.6859e-04 - val_mean_squared_error: 2.6859e-04\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00031 to 0.00027, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2997e-04 - mean_squared_error: 4.2997e-04 - val_loss: 2.5095e-04 - val_mean_squared_error: 2.5095e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00027 to 0.00025, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.1906e-04 - mean_squared_error: 4.1906e-04 - val_loss: 2.3112e-04 - val_mean_squared_error: 2.3112e-04\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.00025 to 0.00023, saving model to weights.best_mlp.hdf5\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.0749e-04 - mean_squared_error: 4.0749e-04 - val_loss: 2.3154e-04 - val_mean_squared_error: 2.3154e-04\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00023\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.8236e-04 - mean_squared_error: 3.8236e-04 - val_loss: 2.2523e-04 - val_mean_squared_error: 2.2523e-04\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.00023 to 0.00023, saving model to weights.best_mlp.hdf5\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.6889e-04 - mean_squared_error: 3.6889e-04 - val_loss: 2.4152e-04 - val_mean_squared_error: 2.4152e-04\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00023\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.4004e-04 - mean_squared_error: 3.4004e-04 - val_loss: 2.4615e-04 - val_mean_squared_error: 2.4615e-04\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00023\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.2508e-04 - mean_squared_error: 3.2508e-04 - val_loss: 2.7713e-04 - val_mean_squared_error: 2.7713e-04\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00023\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.0103e-04 - mean_squared_error: 3.0103e-04 - val_loss: 2.9870e-04 - val_mean_squared_error: 2.9870e-04\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00023\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.9037e-04 - mean_squared_error: 2.9037e-04 - val_loss: 3.5388e-04 - val_mean_squared_error: 3.5388e-04\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00023\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.7962e-04 - mean_squared_error: 2.7962e-04 - val_loss: 3.9512e-04 - val_mean_squared_error: 3.9512e-04\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00023\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.7708e-04 - mean_squared_error: 2.7708e-04 - val_loss: 4.7319e-04 - val_mean_squared_error: 4.7319e-04\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.00023\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.1456e-04 - mean_squared_error: 3.1456e-04 - val_loss: 4.6931e-04 - val_mean_squared_error: 4.6931e-04\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00023\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.0073e-04 - mean_squared_error: 4.0073e-04 - val_loss: 5.6323e-04 - val_mean_squared_error: 5.6323e-04\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.00023\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.7003e-04 - mean_squared_error: 3.7003e-04 - val_loss: 4.5664e-04 - val_mean_squared_error: 4.5664e-04\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.00023\n",
            "Epoch 00049: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 200\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 3\n",
            "MLP: Train: 0.009965, Validation: 0.000457\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA0DKENwAkDIENwCkDMENAClDcANAyvw/csO4Ct8tIN8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 3s 16ms/step - loss: 5.1473 - mean_squared_error: 5.1473 - val_loss: 0.0063 - val_mean_squared_error: 0.0063\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00627, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00627 to 0.00158, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00158\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00158\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00158\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00158\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00158\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.6228e-04 - mean_squared_error: 9.6228e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00158\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.3563e-04 - mean_squared_error: 9.3563e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00158\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.1773e-04 - mean_squared_error: 9.1773e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00158\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.0003e-04 - mean_squared_error: 9.0003e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00158\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.7722e-04 - mean_squared_error: 8.7722e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00158\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.4845e-04 - mean_squared_error: 8.4845e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00158\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.1726e-04 - mean_squared_error: 8.1726e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00158\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.8888e-04 - mean_squared_error: 7.8888e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00158\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.6668e-04 - mean_squared_error: 7.6668e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00158\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.5270e-04 - mean_squared_error: 7.5270e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00158\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.4422e-04 - mean_squared_error: 7.4422e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00158\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.3862e-04 - mean_squared_error: 7.3862e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00158\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.2958e-04 - mean_squared_error: 7.2958e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00158\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.1586e-04 - mean_squared_error: 7.1586e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00158\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.9401e-04 - mean_squared_error: 6.9401e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00158\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.6713e-04 - mean_squared_error: 6.6713e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00158\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.3668e-04 - mean_squared_error: 6.3668e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.00158 to 0.00143, saving model to weights.best_mlp.hdf5\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.0544e-04 - mean_squared_error: 6.0544e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.00143 to 0.00120, saving model to weights.best_mlp.hdf5\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.7457e-04 - mean_squared_error: 5.7457e-04 - val_loss: 9.9710e-04 - val_mean_squared_error: 9.9710e-04\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.00120 to 0.00100, saving model to weights.best_mlp.hdf5\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.4494e-04 - mean_squared_error: 5.4494e-04 - val_loss: 8.2746e-04 - val_mean_squared_error: 8.2746e-04\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.00100 to 0.00083, saving model to weights.best_mlp.hdf5\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.1846e-04 - mean_squared_error: 5.1846e-04 - val_loss: 6.8100e-04 - val_mean_squared_error: 6.8100e-04\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.00083 to 0.00068, saving model to weights.best_mlp.hdf5\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.9647e-04 - mean_squared_error: 4.9647e-04 - val_loss: 5.6073e-04 - val_mean_squared_error: 5.6073e-04\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.00068 to 0.00056, saving model to weights.best_mlp.hdf5\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.7739e-04 - mean_squared_error: 4.7739e-04 - val_loss: 4.6789e-04 - val_mean_squared_error: 4.6789e-04\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.00056 to 0.00047, saving model to weights.best_mlp.hdf5\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.6521e-04 - mean_squared_error: 4.6521e-04 - val_loss: 4.0064e-04 - val_mean_squared_error: 4.0064e-04\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.00047 to 0.00040, saving model to weights.best_mlp.hdf5\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.5108e-04 - mean_squared_error: 4.5108e-04 - val_loss: 3.4739e-04 - val_mean_squared_error: 3.4739e-04\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.00040 to 0.00035, saving model to weights.best_mlp.hdf5\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3848e-04 - mean_squared_error: 4.3848e-04 - val_loss: 3.0564e-04 - val_mean_squared_error: 3.0564e-04\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00035 to 0.00031, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3160e-04 - mean_squared_error: 4.3160e-04 - val_loss: 2.7064e-04 - val_mean_squared_error: 2.7064e-04\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00031 to 0.00027, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2458e-04 - mean_squared_error: 4.2458e-04 - val_loss: 2.4421e-04 - val_mean_squared_error: 2.4421e-04\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00027 to 0.00024, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.1440e-04 - mean_squared_error: 4.1440e-04 - val_loss: 2.2623e-04 - val_mean_squared_error: 2.2623e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00024 to 0.00023, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.9959e-04 - mean_squared_error: 3.9959e-04 - val_loss: 2.1877e-04 - val_mean_squared_error: 2.1877e-04\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.00023 to 0.00022, saving model to weights.best_mlp.hdf5\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.8694e-04 - mean_squared_error: 3.8694e-04 - val_loss: 2.1979e-04 - val_mean_squared_error: 2.1979e-04\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00022\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.6883e-04 - mean_squared_error: 3.6883e-04 - val_loss: 2.2514e-04 - val_mean_squared_error: 2.2514e-04\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00022\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.4605e-04 - mean_squared_error: 3.4605e-04 - val_loss: 2.3904e-04 - val_mean_squared_error: 2.3904e-04\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00022\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.2518e-04 - mean_squared_error: 3.2518e-04 - val_loss: 2.6114e-04 - val_mean_squared_error: 2.6114e-04\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00022\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.1209e-04 - mean_squared_error: 3.1209e-04 - val_loss: 2.8456e-04 - val_mean_squared_error: 2.8456e-04\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00022\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.0097e-04 - mean_squared_error: 3.0097e-04 - val_loss: 3.1834e-04 - val_mean_squared_error: 3.1834e-04\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00022\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.9242e-04 - mean_squared_error: 2.9242e-04 - val_loss: 3.6533e-04 - val_mean_squared_error: 3.6533e-04\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00022\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.7929e-04 - mean_squared_error: 2.7929e-04 - val_loss: 4.4180e-04 - val_mean_squared_error: 4.4180e-04\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00022\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.7163e-04 - mean_squared_error: 2.7163e-04 - val_loss: 5.1756e-04 - val_mean_squared_error: 5.1756e-04\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.00022\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.6952e-04 - mean_squared_error: 2.6952e-04 - val_loss: 5.9331e-04 - val_mean_squared_error: 5.9331e-04\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00022\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.7283e-04 - mean_squared_error: 2.7283e-04 - val_loss: 6.3756e-04 - val_mean_squared_error: 6.3756e-04\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.00022\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.6771e-04 - mean_squared_error: 2.6771e-04 - val_loss: 6.8524e-04 - val_mean_squared_error: 6.8524e-04\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.00022\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.7103e-04 - mean_squared_error: 2.7103e-04 - val_loss: 7.0711e-04 - val_mean_squared_error: 7.0711e-04\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.00022\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.7132e-04 - mean_squared_error: 2.7132e-04 - val_loss: 7.2280e-04 - val_mean_squared_error: 7.2280e-04\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.00022\n",
            "Epoch 52/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.8050e-04 - mean_squared_error: 2.8050e-04 - val_loss: 7.2741e-04 - val_mean_squared_error: 7.2741e-04\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.00022\n",
            "Epoch 53/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.4427e-04 - mean_squared_error: 3.4427e-04 - val_loss: 7.3583e-04 - val_mean_squared_error: 7.3583e-04\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.00022\n",
            "Epoch 00053: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 200\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 4\n",
            "MLP: Train: 0.009047, Validation: 0.000736\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAFAZJREFUeJzt3W1sXNWdx/Hff+bOjAOEJTiGsDGp\ns9ouz9sgTDar5AWNRBWekVhIK1j1RUXesCKgoirdfVGoqMS+abtIi9jQRkVaoBtBA10ES4EmpBWB\nrgPpEiBLShUUh4c4gUCyTRzb+e+LmbFn7HvHY8d35sz4+5GC5+H6zv8k458P55w7x9xdAIDWkWl2\nAQCAqSG4AaDFENwA0GIIbgBoMQQ3ALQYghsAWgzBDQAthuAGgBZDcANAi4nSOOn8+fO9p6cnjVMD\nQFvavn37AXfvqufYVIK7p6dHfX19aZwaANqSmX1Q77EMlQBAiyG4AaDFENwA0GJSGeMGgKkaGhpS\nf3+/jh071uxSUtXR0aHu7m7lcrlpn4PgBhCE/v5+zZ07Vz09PTKzZpeTCnfXwYMH1d/fr8WLF0/7\nPAyVAAjCsWPH1NnZ2bahLUlmps7OzpP+vwqCG0Aw2jm0y2aijUEF94Mv79Yr7w00uwwACFpQwf3w\nK+/rNwQ3gCY4dOiQHnrooSl/39VXX61Dhw6lUFGyoII7H2V0fOREs8sAMAslBffw8HDN73vuued0\nxhlnpFVWrLpWlZjZHkmHJY1IGnb33jSKKUQZDQ4R3AAab926dXr//fe1ZMkS5XI5dXR0aN68edq1\na5fee+893Xjjjdq7d6+OHTumtWvXas2aNZLGPuLjyJEjuuqqq7RixQq9+uqrWrhwoZ555hnNmTNn\nxmudynLAr7r7gRmvoEIhytLjBqD7/vNtvfPhFzN6zgv//HR977qLEp9/4IEHtHPnTu3YsUNbtmzR\nNddco507d44u29uwYYPOPPNMHT16VJdffrluuukmdXZ2Vp1j9+7deuKJJ/TII4/olltu0VNPPaXb\nbrttRtshBbaOOx9lNDg80uwyAEBLly6tWmv94IMPatOmTZKkvXv3avfu3ROCe/HixVqyZIkk6bLL\nLtOePXtSqa3e4HZJvzIzl/Rv7r4+jWIKUUbHh+lxA7NdrZ5xo5x66qmjt7ds2aKXXnpJ27Zt0ymn\nnKIrrrgidi12oVAYvZ3NZnX06NFUaqs3uFe4+z4zO0vSi2a2y923Vh5gZmskrZGkRYsWTauYYo+b\n4AbQeHPnztXhw4djn/v88881b948nXLKKdq1a5dee+21BldXra7gdvd9pa/7zWyTpKWSto47Zr2k\n9ZLU29vr0ymGyUkAzdLZ2anly5fr4osv1pw5c3T22WePPrdq1So9/PDDuuCCC3Teeedp2bJlTay0\njuA2s1MlZdz9cOn21yR9P41i8lFWnx8dSuPUADCpxx9/PPbxQqGg559/Pva58jj2/PnztXPnztHH\n77nnnhmvr6yeHvfZkjaVLtOMJD3u7v+VRjHFHjeTkwBQy6TB7e5/lPSVBtRSnJxkOSAA1BTclZOM\ncQNAbUEFdyHKsqoEACYRWHBndJwLcACgpuCCmx43ANQWXHAfHzkh92ktAweAhjnttNOa9tpBBXc+\nyshdGhohuAEgSVAfMlWIspKk4yMnlI+C+p0CoM2tW7dO5557ru644w5J0r333qsoirR582Z99tln\nGhoa0v33368bbrihyZUGFtzlsB4cGtFphaBKA9BIz6+TPn5rZs+54BLpqgcSn169erXuuuuu0eDe\nuHGjXnjhBd155506/fTTdeDAAS1btkzXX3990/fGDCodC+XgZoISQINdeuml2r9/vz788EMNDAxo\n3rx5WrBgge6++25t3bpVmUxG+/bt0yeffKIFCxY0tdawgjtXDG4+2hWY5Wr0jNN0880368knn9TH\nH3+s1atX67HHHtPAwIC2b9+uXC6nnp6e2I9zbbSggjufLY5x0+MG0AyrV6/W7bffrgMHDuiVV17R\nxo0bddZZZymXy2nz5s364IMPml2ipMCCuzxUQo8bQDNcdNFFOnz4sBYuXKhzzjlHt956q6677jpd\ncskl6u3t1fnnn9/sEiUFFtyjk5NcPQmgSd56a2xSdP78+dq2bVvscUeOHGlUSRMEteaOyUkAmFxY\nwZ0rreMmuAEgUVDBnc8yVALMZrPh4y5moo1BBXd5OSBDJcDs09HRoYMHD7Z1eLu7Dh48qI6OjpM6\nT1iTk1mCG5ituru71d/fr4GBgWaXkqqOjg51d3ef1DmCCm563MDslcvltHjx4maX0RLCGirJMjkJ\nAJMJK7hzTE4CwGSCCu7yGDc9bgBIFlRwZzKmXNYY4waAGoIKbqm4mQI9bgBIFlxw56MMY9wAUENw\nwV2IMhocoscNAEmCDO7jIwQ3ACQJLrjz9LgBoKbggrsQZelxA0ANwQU3k5MAUFvdwW1mWTN708ye\nTbMgJicBoLap9LjXSno3rULKmJwEgNrqCm4z65Z0jaSfpFsOk5MAMJl6e9w/lvQdSYmJamZrzKzP\nzPpO5vN0mZwEgNomDW4zu1bSfnffXus4d1/v7r3u3tvV1TXtgoo9biYnASBJPT3u5ZKuN7M9kn4u\naaWZ/XtaBRWiDB8yBQA1TBrc7v5dd+929x5JX5f0a3e/La2C+JApAKgt0HXcBDcAJJnSnpPuvkXS\nllQqKSkvB3R3mVmaLwUALSnIHrfEhsEAkCS44C6UgpslgQAQL9jg5iIcAIgXYHBnJbHTOwAkCS+4\nc+z0DgC1BBfc+SyTkwBQS3DBTY8bAGoLLrjz2fIYN8ENAHGCC+5yj5vJSQCIF15wRwyVAEAtwQU3\nV04CQG3BBXd5HTc9bgCIF1xwj/W4GeMGgDjBBXeBoRIAqCnY4GaoBADiBRfcTE4CQG3hBTeXvANA\nTcEFt5mVti9jchIA4gQX3FJp+zJ63AAQK9jgZqgEAOIFGtxZdsABgASBBneGPScBIEGQwZ2PMhoc\nYnISAOIEGdz0uAEgWZDBXexxE9wAECfI4C5EWdZxA0CCQIOboRIASBJkcDNUAgDJggxuetwAkGzS\n4DazDjP7nZn93szeNrP70i6KHjcAJIvqOGZQ0kp3P2JmOUm/NbPn3f21tIoqRFl63ACQYNLgdneX\ndKR0N1f642kWVeACHABIVNcYt5llzWyHpP2SXnT319MsKs+HTAFAorqC291H3H2JpG5JS83s4vHH\nmNkaM+szs76BgYGTKqoQZTV8wjVyItWOPQC0pCmtKnH3Q5I2S1oV89x6d+91996urq6TKirPvpMA\nkKieVSVdZnZG6fYcSVdK2pVmUWwYDADJ6llVco6kR80sq2LQb3T3Z9MsqpAr7zs5ouJcKACgrJ5V\nJf8j6dIG1DKKDYMBIFmYV07mspIIbgCIE2Rwj/W4WcsNAOMFGdzlMW4mJwFgojCDmzFuAEgUZnDn\nCG4ASBJmcEfFyUmGSgBgoiCDu3zlJJOTADBRkMHNlZMAkCzI4B7rcRPcADBekMHNGDcAJAs0uBnj\nBoAkQQb36FAJ+04CwARBBneUMWVM7DsJADGCDG4zY/syAEgQZHBLpZ3eCW4AmCDg4M4wOQkAMYIN\n7nyUYXISAGIEG9yFKKNBJicBYIJggzsfZelxA0CMYIO7EGVYDggAMYIN7uIYN5OTADBesMFdYB03\nAMQKOLhZxw0AcQIObtZxA0CcoIObyUkAmCjY4OYCHACIF2xw0+MGgHjhBneOC3AAIE6wwZ3PMjkJ\nAHEmDW4zO9fMNpvZO2b2tpmtbURhhSijEy4NM1wCAFWiOo4ZlvRtd3/DzOZK2m5mL7r7O2kWVrnT\ne5QN9n8MAKDhJk1Ed//I3d8o3T4s6V1JC9MurLxhMBfhAEC1KXVlzaxH0qWSXk+jmEqFXFaSuOwd\nAMapO7jN7DRJT0m6y92/iHl+jZn1mVnfwMDASReWz5aHSpigBIBKdQW3meVUDO3H3P0Xcce4+3p3\n73X33q6urpMurJBjqAQA4tSzqsQk/VTSu+7+w/RLKhrrcRPcAFCpnh73ckl/L2mlme0o/bk65boY\n4waABJMuB3T330qyBtRSpRAxxg0AcYJdIF25jhsAMCbY4GYdNwDECz646XEDQLWAg7s4OUmPGwCq\nBRvceSYnASBWsMHNGDcAxAs4uFnHDQBxgg3u0aESdsEBgCrBBnc2Y4oypuMjjHEDQKVgg1tip3cA\niBN0cLPTOwBMFHhws9M7AIwXdHDnI3Z6B4Dxgg5uhkoAYKKgg5vJSQCYKOjgpscNABMFHtxMTgLA\neEEHdz7KaJAeNwBUCTq4C1FGg0OsKgGASkEHdz7K8OmAADBO0MFdiLJ8OiAAjBN2cOcyBDcAjBN0\ncOezGR3nykkAqBJ0cNPjBoCJwg7ubDG43b3ZpQBAMMIO7lxx+7KhEYIbAMqCDu58lp3eAWC8oIO7\nkGOndwAYL+zgLm8YTHADwKiggztPcAPABJMGt5ltMLP9ZrazEQVVKkTFyUmGSgBgTD097p9JWpVy\nHbGYnASAiSYNbnffKunTBtQyAZOTADBR0GPc5aESxrgBYMyMBbeZrTGzPjPrGxgYmJFzlicn6XED\nwJgZC253X+/uve7e29XVNSPnHFsOyBg3AJQFPVTCckAAmKie5YBPSNom6Twz6zezb6VfVhEX4ADA\nRNFkB7j7NxpRSBwmJwFgopYYKmFyEgDGBB3cTE4CwERBB/folZND9LgBoCzo4M5krLjv5AjBDQBl\nQQe3VBznpscNAGOCD+5ClNHxEca4AaCsJYKbHjcAjAk+uPNRhnXcAFAh+OAuRFnWcQNAheCDu9jj\nZowbAMqCD+7i5CQ9bgAoCz+4c0xOAkCl4IObC3AAoFrwwV2IsvS4AaBC8MHN5CQAVAs+uAtRhuWA\nAFAh/ODOcQEOAFQKPrjzWS7AAYBKwQc3PW4AqBZ8cJeXA5444c0uBQCCEHxwF3KlfSdZyw0Akloh\nuNnpHQCqBB/c7PQOANWCD252egeAai0U3PS4AUBqoeBmqAQAioIP7jw9bgCoEnxwl1eV0OMGgKIW\nCG4mJwGgUl3BbWarzOx/zewPZrYu7aIqsRwQAKpNGtxmlpX0r5KuknShpG+Y2YVpF1bGBTgAUK2e\nHvdSSX9w9z+6+3FJP5d0QyrV/OlTafCINDI8+lCeoRIAqBLVccxCSXsr7vdL+ptUqvnRxdLQ/xVv\nW1aKCvpStqDXCy5/OqOPn5bMxg63mFPEPVaXaX9j+dvjPwSr+rRjx5h77DFj5/Gq50ye+Nz410+q\npV5ecWZP+IspPz7+a9VjVn5urLLq74l/TY17Pu41VPV6tc8Td66k9tTLbYbOM4Vjk8481deMP/dU\n3zNTec2kn42pvWbSK57s+31aPP41/xT9mf7qn15P/eXrCe66mNkaSWskadGiRdM7yZX3SUNHpeFB\nafiYNHxMNjyoQ/sO6OjQsNyLf18uL932cYFR/E/yP2P9/8BTfit4jR8giw8ZJQTk+POM3jeb2N7E\n75vuD3PSL4C4XzSlCtxjqxl7Lr7auNcZ/0M4/vvjjpm03oQfsrHvm9owXGJQTPI6dZ+nDc3UL8yk\n93VT/iZjfnmP5Oc25KXrCe59ks6tuN9deqyKu6+XtF6Sent7p/f3uPT2CQ+ZpPOmdTIAaE/1jHH/\nt6Qvm9liM8tL+rqkX6ZbFgAgyaQ9bncfNrN/kPSCpKykDe7+duqVAQBi1TXG7e7PSXou5VoAAHUI\n/spJAEA1ghsAWgzBDQAthuAGgBZDcANAizGf4tVedZ3UbEDSB9P89vmSDsxgOaGaLe2UZk9bZ0s7\npdnT1ka280vu3lXPgakE98kwsz537212HWmbLe2UZk9bZ0s7pdnT1lDbyVAJALQYghsAWkyIwb2+\n2QU0yGxppzR72jpb2inNnrYG2c7gxrgBALWF2OMGANQQTHA3c0PitJnZBjPbb2Y7Kx4708xeNLPd\npa/zmlnjTDCzc81ss5m9Y2Zvm9na0uPt2NYOM/udmf2+1Nb7So8vNrPXS+/j/yh9FHLLM7Osmb1p\nZs+W7rdrO/eY2VtmtsPM+kqPBff+DSK4m70hcQP8TNKqcY+tk/Syu39Z0sul+61uWNK33f1CScsk\n3VH6d2zHtg5KWunuX5G0RNIqM1sm6Z8l/cjd/1LSZ5K+1cQaZ9JaSe9W3G/XdkrSV919ScUywODe\nv0EEtxq5IXETuPtWSZ+Oe/gGSY+Wbj8q6caGFpUCd//I3d8o3T6s4g/6QrVnW93dj5Tu5kp/XNJK\nSU+WHm+LtppZt6RrJP2kdN/Uhu2sIbj3byjBHbch8cIm1dIoZ7v7R6XbH0s6u5nFzDQz65F0qaTX\n1aZtLQ0f7JC0X9KLkt6XdMjdh0uHtMv7+MeSviONbs7ZqfZsp1T85fsrM9te2kdXCvD9O2ObBWP6\n3N3NrG2W95jZaZKeknSXu39hlZslt1Fb3X1E0hIzO0PSJknnN7mkGWdm10ra7+7bzeyKZtfTACvc\nfZ+ZnSXpRTPbVflkKO/fUHrcdW1I3GY+MbNzJKn0dX+T65kRZpZTMbQfc/dflB5uy7aWufshSZsl\n/a2kM8ys3CFqh/fxcknXm9keFYcwV0r6F7VfOyVJ7r6v9HW/ir+MlyrA928owT0bNyT+paRvlm5/\nU9IzTaxlRpTGPn8q6V13/2HFU+3Y1q5ST1tmNkfSlSqO6W+W9Helw1q+re7+XXfvdvceFX8uf+3u\nt6rN2ilJZnaqmc0t35b0NUk7FeD7N5gLcMzsahXH0sobEv+gySXNGDN7QtIVKn7S2CeSvifpaUkb\nJS1S8ZMUb3H38ROYLcXMVkj6jaS3NDYe+o8qjnO3W1v/WsWJqqyKHaCN7v59M/sLFXumZ0p6U9Jt\n7j7YvEpnTmmo5B53v7Yd21lq06bS3UjS4+7+AzPrVGDv32CCGwBQn1CGSgAAdSK4AaDFENwA0GII\nbgBoMQQ3ALQYghsAWgzBDQAthuAGgBbz/wWGxisQJ7+TAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 3s 17ms/step - loss: 4.7688 - mean_squared_error: 4.7688 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00266, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00266 to 0.00175, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00175\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00175\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00175\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00175\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.9418e-04 - mean_squared_error: 9.9418e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00175\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.5670e-04 - mean_squared_error: 9.5670e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00175\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.3125e-04 - mean_squared_error: 9.3125e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00175\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.0782e-04 - mean_squared_error: 9.0782e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00175\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.8052e-04 - mean_squared_error: 8.8052e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00175\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.4850e-04 - mean_squared_error: 8.4850e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00175\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.1539e-04 - mean_squared_error: 8.1539e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00175\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.8617e-04 - mean_squared_error: 7.8617e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00175\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.6477e-04 - mean_squared_error: 7.6477e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00175\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.5149e-04 - mean_squared_error: 7.5149e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00175\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.4424e-04 - mean_squared_error: 7.4424e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00175\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.3890e-04 - mean_squared_error: 7.3890e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00175\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.3029e-04 - mean_squared_error: 7.3029e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00175\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.1415e-04 - mean_squared_error: 7.1415e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00175\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.8962e-04 - mean_squared_error: 6.8962e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00175\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.5888e-04 - mean_squared_error: 6.5888e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.00175 to 0.00163, saving model to weights.best_mlp.hdf5\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.2363e-04 - mean_squared_error: 6.2363e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.00163 to 0.00136, saving model to weights.best_mlp.hdf5\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.9023e-04 - mean_squared_error: 5.9023e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.00136 to 0.00112, saving model to weights.best_mlp.hdf5\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.5504e-04 - mean_squared_error: 5.5504e-04 - val_loss: 9.1948e-04 - val_mean_squared_error: 9.1948e-04\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.00112 to 0.00092, saving model to weights.best_mlp.hdf5\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.2828e-04 - mean_squared_error: 5.2828e-04 - val_loss: 7.5053e-04 - val_mean_squared_error: 7.5053e-04\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.00092 to 0.00075, saving model to weights.best_mlp.hdf5\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.0557e-04 - mean_squared_error: 5.0557e-04 - val_loss: 6.0980e-04 - val_mean_squared_error: 6.0980e-04\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.00075 to 0.00061, saving model to weights.best_mlp.hdf5\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.8852e-04 - mean_squared_error: 4.8852e-04 - val_loss: 4.9581e-04 - val_mean_squared_error: 4.9581e-04\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.00061 to 0.00050, saving model to weights.best_mlp.hdf5\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.7704e-04 - mean_squared_error: 4.7704e-04 - val_loss: 4.1165e-04 - val_mean_squared_error: 4.1165e-04\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.00050 to 0.00041, saving model to weights.best_mlp.hdf5\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.6443e-04 - mean_squared_error: 4.6443e-04 - val_loss: 3.5507e-04 - val_mean_squared_error: 3.5507e-04\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.00041 to 0.00036, saving model to weights.best_mlp.hdf5\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.5377e-04 - mean_squared_error: 4.5377e-04 - val_loss: 3.1076e-04 - val_mean_squared_error: 3.1076e-04\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.00036 to 0.00031, saving model to weights.best_mlp.hdf5\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.4502e-04 - mean_squared_error: 4.4502e-04 - val_loss: 2.7112e-04 - val_mean_squared_error: 2.7112e-04\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.00031 to 0.00027, saving model to weights.best_mlp.hdf5\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3873e-04 - mean_squared_error: 4.3873e-04 - val_loss: 2.4332e-04 - val_mean_squared_error: 2.4332e-04\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00027 to 0.00024, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2723e-04 - mean_squared_error: 4.2723e-04 - val_loss: 2.2983e-04 - val_mean_squared_error: 2.2983e-04\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00024 to 0.00023, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.1206e-04 - mean_squared_error: 4.1206e-04 - val_loss: 2.2345e-04 - val_mean_squared_error: 2.2345e-04\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00023 to 0.00022, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.9691e-04 - mean_squared_error: 3.9691e-04 - val_loss: 2.2221e-04 - val_mean_squared_error: 2.2221e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00022 to 0.00022, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.7175e-04 - mean_squared_error: 3.7175e-04 - val_loss: 2.3112e-04 - val_mean_squared_error: 2.3112e-04\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00022\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.5678e-04 - mean_squared_error: 3.5678e-04 - val_loss: 2.4685e-04 - val_mean_squared_error: 2.4685e-04\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00022\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.3908e-04 - mean_squared_error: 3.3908e-04 - val_loss: 2.7028e-04 - val_mean_squared_error: 2.7028e-04\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00022\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.3043e-04 - mean_squared_error: 3.3043e-04 - val_loss: 2.9762e-04 - val_mean_squared_error: 2.9762e-04\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00022\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.0811e-04 - mean_squared_error: 3.0811e-04 - val_loss: 3.5476e-04 - val_mean_squared_error: 3.5476e-04\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00022\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.1144e-04 - mean_squared_error: 3.1144e-04 - val_loss: 3.9770e-04 - val_mean_squared_error: 3.9770e-04\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00022\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.8146e-04 - mean_squared_error: 2.8146e-04 - val_loss: 4.8955e-04 - val_mean_squared_error: 4.8955e-04\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00022\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.9617e-04 - mean_squared_error: 2.9617e-04 - val_loss: 5.3898e-04 - val_mean_squared_error: 5.3898e-04\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00022\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.7604e-04 - mean_squared_error: 2.7604e-04 - val_loss: 6.4514e-04 - val_mean_squared_error: 6.4514e-04\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00022\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.8433e-04 - mean_squared_error: 2.8433e-04 - val_loss: 6.7933e-04 - val_mean_squared_error: 6.7933e-04\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.00022\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.8420e-04 - mean_squared_error: 2.8420e-04 - val_loss: 7.3860e-04 - val_mean_squared_error: 7.3860e-04\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00022\n",
            "Epoch 00047: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 200\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 2\n",
            "MLP: Train: 0.008978, Validation: 0.000739\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBwTRmQXpPPvLAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 3s 17ms/step - loss: 4.9271 - mean_squared_error: 4.9271 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00326, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00326 to 0.00142, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00142\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00142\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00142\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00142\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.9676e-04 - mean_squared_error: 9.9676e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00142\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.5267e-04 - mean_squared_error: 9.5267e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00142\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.2416e-04 - mean_squared_error: 9.2416e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00142\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.0024e-04 - mean_squared_error: 9.0024e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00142\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.7387e-04 - mean_squared_error: 8.7387e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00142\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.4264e-04 - mean_squared_error: 8.4264e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00142\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.0837e-04 - mean_squared_error: 8.0837e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00142\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.7561e-04 - mean_squared_error: 7.7561e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00142\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.4948e-04 - mean_squared_error: 7.4948e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00142\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.3083e-04 - mean_squared_error: 7.3083e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00142\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.2006e-04 - mean_squared_error: 7.2006e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00142\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.1330e-04 - mean_squared_error: 7.1330e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00142\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.0705e-04 - mean_squared_error: 7.0705e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00142\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.9525e-04 - mean_squared_error: 6.9525e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00142\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.7760e-04 - mean_squared_error: 6.7760e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00142\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.5658e-04 - mean_squared_error: 6.5658e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00142\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.3026e-04 - mean_squared_error: 6.3026e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00142\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.9852e-04 - mean_squared_error: 5.9852e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.00142 to 0.00130, saving model to weights.best_mlp.hdf5\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.6804e-04 - mean_squared_error: 5.6804e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.00130 to 0.00109, saving model to weights.best_mlp.hdf5\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.3623e-04 - mean_squared_error: 5.3623e-04 - val_loss: 9.0541e-04 - val_mean_squared_error: 9.0541e-04\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.00109 to 0.00091, saving model to weights.best_mlp.hdf5\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.0838e-04 - mean_squared_error: 5.0838e-04 - val_loss: 7.4898e-04 - val_mean_squared_error: 7.4898e-04\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.00091 to 0.00075, saving model to weights.best_mlp.hdf5\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.8651e-04 - mean_squared_error: 4.8651e-04 - val_loss: 6.1663e-04 - val_mean_squared_error: 6.1663e-04\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.00075 to 0.00062, saving model to weights.best_mlp.hdf5\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.7117e-04 - mean_squared_error: 4.7117e-04 - val_loss: 5.1546e-04 - val_mean_squared_error: 5.1546e-04\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.00062 to 0.00052, saving model to weights.best_mlp.hdf5\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.5937e-04 - mean_squared_error: 4.5937e-04 - val_loss: 4.3870e-04 - val_mean_squared_error: 4.3870e-04\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.00052 to 0.00044, saving model to weights.best_mlp.hdf5\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.4638e-04 - mean_squared_error: 4.4638e-04 - val_loss: 3.7931e-04 - val_mean_squared_error: 3.7931e-04\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.00044 to 0.00038, saving model to weights.best_mlp.hdf5\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3672e-04 - mean_squared_error: 4.3672e-04 - val_loss: 3.3196e-04 - val_mean_squared_error: 3.3196e-04\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.00038 to 0.00033, saving model to weights.best_mlp.hdf5\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2959e-04 - mean_squared_error: 4.2959e-04 - val_loss: 2.8917e-04 - val_mean_squared_error: 2.8917e-04\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00033 to 0.00029, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2098e-04 - mean_squared_error: 4.2098e-04 - val_loss: 2.5737e-04 - val_mean_squared_error: 2.5737e-04\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00029 to 0.00026, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.1380e-04 - mean_squared_error: 4.1380e-04 - val_loss: 2.3581e-04 - val_mean_squared_error: 2.3581e-04\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00026 to 0.00024, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.0137e-04 - mean_squared_error: 4.0137e-04 - val_loss: 2.2524e-04 - val_mean_squared_error: 2.2524e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00024 to 0.00023, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.8415e-04 - mean_squared_error: 3.8415e-04 - val_loss: 2.2409e-04 - val_mean_squared_error: 2.2409e-04\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.00023 to 0.00022, saving model to weights.best_mlp.hdf5\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.6784e-04 - mean_squared_error: 3.6784e-04 - val_loss: 2.2673e-04 - val_mean_squared_error: 2.2673e-04\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00022\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.5386e-04 - mean_squared_error: 3.5386e-04 - val_loss: 2.3775e-04 - val_mean_squared_error: 2.3775e-04\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00022\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.3690e-04 - mean_squared_error: 3.3690e-04 - val_loss: 2.5423e-04 - val_mean_squared_error: 2.5423e-04\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00022\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.2092e-04 - mean_squared_error: 3.2092e-04 - val_loss: 2.8019e-04 - val_mean_squared_error: 2.8019e-04\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00022\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.0452e-04 - mean_squared_error: 3.0452e-04 - val_loss: 3.1757e-04 - val_mean_squared_error: 3.1757e-04\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00022\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.9202e-04 - mean_squared_error: 2.9202e-04 - val_loss: 3.6725e-04 - val_mean_squared_error: 3.6725e-04\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00022\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.8322e-04 - mean_squared_error: 2.8322e-04 - val_loss: 4.2646e-04 - val_mean_squared_error: 4.2646e-04\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00022\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.7580e-04 - mean_squared_error: 2.7580e-04 - val_loss: 5.0299e-04 - val_mean_squared_error: 5.0299e-04\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00022\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.7181e-04 - mean_squared_error: 2.7181e-04 - val_loss: 5.8281e-04 - val_mean_squared_error: 5.8281e-04\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.00022\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.7561e-04 - mean_squared_error: 2.7561e-04 - val_loss: 6.5332e-04 - val_mean_squared_error: 6.5332e-04\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00022\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.6824e-04 - mean_squared_error: 2.6824e-04 - val_loss: 7.0343e-04 - val_mean_squared_error: 7.0343e-04\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.00022\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.7987e-04 - mean_squared_error: 2.7987e-04 - val_loss: 7.4451e-04 - val_mean_squared_error: 7.4451e-04\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.00022\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.7142e-04 - mean_squared_error: 2.7142e-04 - val_loss: 7.7818e-04 - val_mean_squared_error: 7.7818e-04\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.00022\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.0246e-04 - mean_squared_error: 3.0246e-04 - val_loss: 7.5307e-04 - val_mean_squared_error: 7.5307e-04\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.00022\n",
            "Epoch 00051: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 200\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 3\n",
            "MLP: Train: 0.008904, Validation: 0.000753\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbYbgBoA2Q3ADQJshuAGgzfw/llG/u64IiBMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 3s 17ms/step - loss: 5.0915 - mean_squared_error: 5.0915 - val_loss: 0.0081 - val_mean_squared_error: 0.0081\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00807, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00807 to 0.00152, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00152\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00152\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00152\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00152\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00152\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.9202e-04 - mean_squared_error: 9.9202e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00152\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.5645e-04 - mean_squared_error: 9.5645e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00152\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.2933e-04 - mean_squared_error: 9.2933e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00152\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.0263e-04 - mean_squared_error: 9.0263e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00152\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.7148e-04 - mean_squared_error: 8.7148e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00152\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.3607e-04 - mean_squared_error: 8.3607e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00152\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.0007e-04 - mean_squared_error: 8.0007e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00152\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.6840e-04 - mean_squared_error: 7.6840e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00152\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.4471e-04 - mean_squared_error: 7.4471e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00152\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.2979e-04 - mean_squared_error: 7.2979e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00152\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.2146e-04 - mean_squared_error: 7.2146e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00152\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.1516e-04 - mean_squared_error: 7.1516e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00152\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.0573e-04 - mean_squared_error: 7.0573e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00152\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.9389e-04 - mean_squared_error: 6.9389e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00152\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.7290e-04 - mean_squared_error: 6.7290e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00152\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.4777e-04 - mean_squared_error: 6.4777e-04 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00152\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.1784e-04 - mean_squared_error: 6.1784e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.00152 to 0.00144, saving model to weights.best_mlp.hdf5\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.8648e-04 - mean_squared_error: 5.8648e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.00144 to 0.00121, saving model to weights.best_mlp.hdf5\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.5539e-04 - mean_squared_error: 5.5539e-04 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.00121 to 0.00101, saving model to weights.best_mlp.hdf5\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.2427e-04 - mean_squared_error: 5.2427e-04 - val_loss: 8.4189e-04 - val_mean_squared_error: 8.4189e-04\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.00101 to 0.00084, saving model to weights.best_mlp.hdf5\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.9701e-04 - mean_squared_error: 4.9701e-04 - val_loss: 6.9979e-04 - val_mean_squared_error: 6.9979e-04\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.00084 to 0.00070, saving model to weights.best_mlp.hdf5\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.7498e-04 - mean_squared_error: 4.7498e-04 - val_loss: 5.7993e-04 - val_mean_squared_error: 5.7993e-04\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.00070 to 0.00058, saving model to weights.best_mlp.hdf5\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.5894e-04 - mean_squared_error: 4.5894e-04 - val_loss: 4.8456e-04 - val_mean_squared_error: 4.8456e-04\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.00058 to 0.00048, saving model to weights.best_mlp.hdf5\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.4656e-04 - mean_squared_error: 4.4656e-04 - val_loss: 4.1857e-04 - val_mean_squared_error: 4.1857e-04\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.00048 to 0.00042, saving model to weights.best_mlp.hdf5\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3654e-04 - mean_squared_error: 4.3654e-04 - val_loss: 3.6643e-04 - val_mean_squared_error: 3.6643e-04\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.00042 to 0.00037, saving model to weights.best_mlp.hdf5\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3009e-04 - mean_squared_error: 4.3009e-04 - val_loss: 3.2294e-04 - val_mean_squared_error: 3.2294e-04\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00037 to 0.00032, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2814e-04 - mean_squared_error: 4.2814e-04 - val_loss: 2.8618e-04 - val_mean_squared_error: 2.8618e-04\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00032 to 0.00029, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2597e-04 - mean_squared_error: 4.2597e-04 - val_loss: 2.6060e-04 - val_mean_squared_error: 2.6060e-04\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00029 to 0.00026, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2110e-04 - mean_squared_error: 4.2110e-04 - val_loss: 2.4386e-04 - val_mean_squared_error: 2.4386e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00026 to 0.00024, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.1345e-04 - mean_squared_error: 4.1345e-04 - val_loss: 2.3698e-04 - val_mean_squared_error: 2.3698e-04\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.00024 to 0.00024, saving model to weights.best_mlp.hdf5\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.9757e-04 - mean_squared_error: 3.9757e-04 - val_loss: 2.3481e-04 - val_mean_squared_error: 2.3481e-04\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.00024 to 0.00023, saving model to weights.best_mlp.hdf5\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.8384e-04 - mean_squared_error: 3.8384e-04 - val_loss: 2.3891e-04 - val_mean_squared_error: 2.3891e-04\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00023\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.6404e-04 - mean_squared_error: 3.6404e-04 - val_loss: 2.4598e-04 - val_mean_squared_error: 2.4598e-04\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00023\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.4310e-04 - mean_squared_error: 3.4310e-04 - val_loss: 2.6072e-04 - val_mean_squared_error: 2.6072e-04\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00023\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.4180e-04 - mean_squared_error: 3.4180e-04 - val_loss: 2.7687e-04 - val_mean_squared_error: 2.7687e-04\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00023\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.1095e-04 - mean_squared_error: 3.1095e-04 - val_loss: 3.0991e-04 - val_mean_squared_error: 3.0991e-04\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00023\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.2463e-04 - mean_squared_error: 3.2463e-04 - val_loss: 3.3283e-04 - val_mean_squared_error: 3.3283e-04\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00023\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.9306e-04 - mean_squared_error: 2.9306e-04 - val_loss: 3.8691e-04 - val_mean_squared_error: 3.8691e-04\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00023\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.0343e-04 - mean_squared_error: 3.0343e-04 - val_loss: 4.3680e-04 - val_mean_squared_error: 4.3680e-04\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.00023\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.9300e-04 - mean_squared_error: 2.9300e-04 - val_loss: 4.9575e-04 - val_mean_squared_error: 4.9575e-04\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00023\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.1267e-04 - mean_squared_error: 3.1267e-04 - val_loss: 5.4041e-04 - val_mean_squared_error: 5.4041e-04\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.00023\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.2513e-04 - mean_squared_error: 3.2513e-04 - val_loss: 5.4659e-04 - val_mean_squared_error: 5.4659e-04\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.00023\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.8010e-04 - mean_squared_error: 3.8010e-04 - val_loss: 6.4130e-04 - val_mean_squared_error: 6.4130e-04\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.00023\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.7471e-04 - mean_squared_error: 3.7471e-04 - val_loss: 5.1658e-04 - val_mean_squared_error: 5.1658e-04\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.00023\n",
            "Epoch 00051: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 200\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 4\n",
            "MLP: Train: 0.009934, Validation: 0.000517\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAE6ZJREFUeJzt3V2MXOV9x/Hff1521gYjm/WCqRd3\nXTXivbHF4rrCFwQpkXkzSAScCKpcRPiGCoOCIqe9CFREojdpSlVETYISqUBiQVxSBCVAbJwIQ7oG\nUwy4OI6MvAbjtcFgB7/s2v9ezJnd2dlzZmbXe3aeM/5+JMvzcvbs/8Gzv334z3PmMXcXACA7cq0u\nAAAwMQQ3AGQMwQ0AGUNwA0DGENwAkDEENwBkDMENABlDcANAxhDcAJAxhTROOnfuXO/t7U3j1ADQ\nlrZs2bLf3bubOTaV4O7t7VV/f38apwaAtmRmHzR7LK0SAMgYghsAMobgBoCMSaXHDQATNTQ0pIGB\nAR09erTVpaSqs7NTPT09KhaLkz4HwQ0gCAMDA5o1a5Z6e3tlZq0uJxXurgMHDmhgYEALFy6c9Hlo\nlQAIwtGjR9XV1dW2oS1JZqaurq5T/r8KghtAMNo5tCumYoxBBfdDL+/QK+8PtroMAAhaU8FtZrvM\n7G0z22pmqV1Z88grO/VbghtACxw8eFAPP/zwhL/u2muv1cGDB1OoKNlEZtxfcfdF7t6XVjGlQk7H\nhk+mdXoASJQU3MPDw3W/7rnnntPs2bPTKitWUKtKSoW8jhPcAFpgzZo12rlzpxYtWqRisajOzk7N\nmTNH27dv1/vvv6+bbrpJu3fv1tGjR7V69WqtWrVK0uhHfBw+fFjXXHONli1bpldffVXz58/XM888\noxkzZkx5rc0Gt0v6tZm5pH9397VTXomkUjGnY8Mn0jg1gAy5/7/e0bsffj6l57z4z87S92+4JPH5\nBx98UNu2bdPWrVu1ceNGXXfdddq2bdvIsr3HHntMZ599to4cOaIrrrhCN998s7q6usacY8eOHXry\nySf16KOP6tZbb9XTTz+t22+/fUrHITUf3MvcfY+ZnSPpRTPb7u6bqg8ws1WSVknSggULJlUMrRIA\noViyZMmYtdYPPfSQ1q9fL0navXu3duzYMS64Fy5cqEWLFkmSLr/8cu3atSuV2poKbnffE/29z8zW\nS1oiaVPNMWslrZWkvr4+n0wxpUKe4AZQd2Y8Xc4444yR2xs3btRLL72kzZs3a+bMmbrqqqti12KX\nSqWR2/l8XkeOHEmltoZvTprZGWY2q3Jb0tckbUujmPKMm1YJgOk3a9YsHTp0KPa5zz77THPmzNHM\nmTO1fft2vfbaa9Nc3VjNzLjPlbQ+WjRekPSEu/93GsV0FHI6NsSMG8D06+rq0pVXXqlLL71UM2bM\n0Lnnnjvy3PLly/XII4/ooosu0gUXXKClS5e2sNImgtvd/yjpy9NQi0qFnA4drb/0BgDS8sQTT8Q+\nXiqV9Pzzz8c+V+ljz507V9u2jTYj7r333imvryKoKyfLPW5aJQBQT1jBXWRVCQA0ElZwF3JcgAMA\nDQQW3CwHBIBGAgvunI4N0eMGgHrCCm563ADQUFjBXchr+KRr+AThDSBsZ555Zsu+d1DB3VEol3Oc\n4AaARIF9rGs5uI8NndTMjhYXA+C0smbNGp1//vm68847JUn33XefCoWCNmzYoE8//VRDQ0N64IEH\ndOONN7a40uCCOy9J9LmB093za6S9b0/tOeddJl3zYOLTK1eu1N133z0S3OvWrdMLL7ygu+66S2ed\ndZb279+vpUuXasWKFS3fGzOw4I5m3Fw9CWCaLV68WPv27dOHH36owcFBzZkzR/PmzdM999yjTZs2\nKZfLac+ePfr44481b968ltYaVnAXox43M27g9FZnZpymW265RU899ZT27t2rlStX6vHHH9fg4KC2\nbNmiYrGo3t7e2I9znW5hBTetEgAttHLlSt1xxx3av3+/XnnlFa1bt07nnHOOisWiNmzYoA8++KDV\nJUoKLrhplQBonUsuuUSHDh3S/Pnzdd555+m2227TDTfcoMsuu0x9fX268MILW12ipFCDm8/kBtAi\nb789+qbo3LlztXnz5tjjDh8+PF0ljRPkOm5aJQCQLKjgHu1x0yoBgCRhBXeRGTdwOnOf1D7jmTIV\nYwwruOlxA6etzs5OHThwoK3D29114MABdXZ2ntJ5AntzklYJcLrq6enRwMCABgcHW11Kqjo7O9XT\n03NK5wgruGmVAKetYrGohQsXtrqMTAizVUJwA0CioIK7I09wA0AjQQW3mZW3L6PHDQCJggpuqXwR\nDqtKACBZcMHNTu8AUF+AwU2rBADqCS+42ekdAOoKL7gLeXrcAFBHgMGdY5d3AKij6eA2s7yZvWlm\nz6ZZUKmQ07EhetwAkGQiM+7Vkt5Lq5CKUpFVJQBQT1PBbWY9kq6T9ON0y6msKiG4ASBJszPuH0n6\nrqTUE7WD5YAAUFfD4Daz6yXtc/ctDY5bZWb9ZtZ/Kh/LWOLKSQCoq5kZ95WSVpjZLkk/l3S1mf1H\n7UHuvtbd+9y9r7u7e9IFceUkANTXMLjd/Xvu3uPuvZK+Iek37n57WgVx5SQA1BfeOm6unASAuia0\nA467b5S0MZVKIqVCXseHT8rdZWZpfisAyKTwZtzRLjhcPQkA8YINbtolABAvvOAuRju9syQQAGKF\nF9wj+06ysgQA4oQX3EVaJQBQT3jBXelx0yoBgFgBBnfU46ZVAgCxAgxuWiUAUE94wU2PGwDqCi+4\no1bJcYIbAGIFGNwsBwSAegIMbi7AAYB6ggvuDt6cBIC6ggtuWiUAUF94wc2qEgCoK7jg7shz5SQA\n1BNccBfyORVyRqsEABIEF9xSZd9JZtwAECfM4C7muQAHABKEGdzs9A4AiYIM7g5aJQCQKMjgLhVy\nrCoBgASBBneeVgkAJAg0uGmVAECSMIO7SHADQJIwg5tWCQAkCjS4eXMSAJIEG9zHTxDcABAn0ODO\nM+MGgARBBncHV04CQKKGwW1mnWb2ezN7y8zeMbP70y6K5YAAkKzQxDHHJF3t7ofNrCjpd2b2vLu/\nllZRLAcEgGQNZ9xedji6W4z+eJpFlQp5nTjpGuYNSgAYp6ket5nlzWyrpH2SXnT312OOWWVm/WbW\nPzg4eEpFldgwGAASNRXc7n7C3RdJ6pG0xMwujTlmrbv3uXtfd3f3KRVFcANAsgmtKnH3g5I2SFqe\nTjllpWJeEju9A0CcZlaVdJvZ7Oj2DElflbQ9zaIqM252wQGA8ZpZVXKepJ+ZWV7loF/n7s+mWVSp\nUJlxE9wAUKthcLv7/0paPA21jOio9Li5ehIAxgnyysnRNyfpcQNArcCDmxk3ANQKM7hZVQIAicIM\nbnrcAJAo7OCmVQIA44QZ3LRKACBRmMHNBTgAkCjo4KZVAgDjBRncHQQ3ACQKM7jzlVUl9LgBoFaQ\nwW1mbF8GAAmCDG6JfScBIEm4wV3MsxwQAGKEG9yFHFdOAkCMsIObVgkAjBNwcOcJbgCIEW5wF3P0\nuAEgRrDB3ZGnVQIAcYIN7vKqEoIbAGqFG9yFHFdOAkCMoIObTwcEgPECDm5aJQAQJ9zgZlUJAMQK\nN7i5chIAYgUc3HkdO0FwA0CtYIO7I3pz0t1bXQoABCXY4Gb7MgCIR3ADQMaEG9zFvCSxsgQAaoQb\n3JUZNytLAGCMhsFtZueb2QYze9fM3jGz1dNRGK0SAIhXaOKYYUnfcfc3zGyWpC1m9qK7v5tmYaUC\nrRIAiNNwxu3uH7n7G9HtQ5LekzQ/7cJKRWbcABBnQj1uM+uVtFjS6zHPrTKzfjPrHxwcPOXCKq0S\nPmgKAMZqOrjN7ExJT0u6290/r33e3de6e5+793V3d59yYfS4ASBeU8FtZkWVQ/txd/9luiWVjfS4\n+UxuABijmVUlJuknkt5z9x+mX1IZM24AiNfMjPtKSX8r6Woz2xr9uTbluqpWlRDcAFCt4XJAd/+d\nJJuGWsYYXVVCqwQAqnHlJABkTMDBTasEAOIEG9wdBVolABAn2ODO50zFvDHjBoAawQa3JHXkc1w5\nCQA1gg7uUjFPqwQAaoQd3Oz0DgDjhB/ctEoAYIzAg5tWCQDUCju4i8y4AaBW2MFNjxsAxgk8uGmV\nAECtwIObVgkA1Ao6uDsKXIADALWCDm5m3AAwXuDBTY8bAGqFHdwsBwSAccIObpYDAsA4gQd3uVXi\n7q0uBQCCEXhw53TSpeGTBDcAVIQd3CMbBtMuAYCKsIO7su/kECtLAKAi6OCu7Dt5/AQzbgCoCDq4\nS5UNg1lZAgAjAg/uqFVCjxsARgQe3JU3J+lxA0BF2MHNqhIAGCfs4B5ZVUJwA0BF4MFNqwQAajUM\nbjN7zMz2mdm26SioGq0SABivmRn3TyUtT7mOWKOrSphxA0BFw+B2902SPpmGWsYZuQCHGTcAjMhI\nj5vgBoCKKQtuM1tlZv1m1j84ODgl5+TKSQAYb8qC293Xunufu/d1d3dPyTnpcQPAeEG3Sop5kxmt\nEgCo1sxywCclbZZ0gZkNmNm30y9r5Huz0zsA1Cg0OsDdvzkdhSQpFfJ8HjcAVAm6VSKJGTcA1Ag+\nuDsIbgAYI/jgLhVyXIADAFUyENx5lgMCQJXwg7tIqwQAqoUf3IUcV04CQJUMBDetEgColoHgplUC\nANXCD+5inuAGgCrhB3chx5WTAFAl+ODmAhwAGCv44OYCHAAYKwPBTY8bAKplILhzOn7ipE6e9FaX\nAgBBCD+4i9GGwSeYdQOAlIXgrmxfxtWTACApE8Fd2emdJYEAIGUquJlxA4CUheAustM7AFQLPrg7\n8uUSj9LjBgBJGQhuVpUAwFjhB3elx82MGwAkZSK46XEDQLUMBDerSgCgWvDB3VkkuAGgWvDBPXrl\nJK0SAJAyEdzMuAGgWgaCu/LmJMENAFIGgruDzyoBgDEyE9zsggMAZU0Ft5ktN7P/M7M/mNmatIuq\nls+ZinmjVQIAkYbBbWZ5Sf8m6RpJF0v6ppldnHZh1UqFPFdOAkCkmRn3Ekl/cPc/uvtxST+XdGMq\n1RzYKX02IH3xiTR8TPLydmWlQo4eNwBECk0cM1/S7qr7A5L+OpVqHlkmDX0xet/yUnGmXjhR0NBW\n0963TKbRvSct4TTlxz3msQYSDvLmvjo6RfzemObV568ag8ePZ/Q8Pua5seOPv91sTZPhNVWOPj72\n+dr/ZiOPW+2/jtV8Xf3vVft9xtYwvp56Ndc/rvHjSSpjPNXzpG2i9UzudTQVY57Y951onYkV+sTH\n+6fCbF34D5sn/HUT1UxwN8XMVklaJUkLFiyY3ElW/Kt0/LA0dEQ6/qdyiA8d0Rd7B3Xg0JHyP4dL\nlaaJu5d/SKr++3rVjbgfoLH/Fs38w0zNi9VdksUHS3L41A+/8V+bXMPUhEbCLwqvfczH3K+tcPSX\nVfzzteev/UGs/fq4Y+IfTzrP+OcanTdJ4vETDIGp/GXbDib8y3PCr/eE4yd4mhPFWRP8vpPTTHDv\nkXR+1f2e6LEx3H2tpLWS1NfXN7lX3WVfj314QfQHANBcj/t/JH3JzBaaWYekb0j6VbplAQCSNJxx\nu/uwmf2dpBck5SU95u7vpF4ZACBWUz1ud39O0nMp1wIAaELwV04CAMYiuAEgYwhuAMgYghsAMobg\nBoCMMZ/EZZ0NT2o2KOmDSX75XEn7p7CcLGDM7e90G6/EmCfqz929u5kDUwnuU2Fm/e7e1+o6phNj\nbn+n23glxpwmWiUAkDEENwBkTIjBvbbVBbQAY25/p9t4JcacmuB63ACA+kKccQMA6ggmuFu5IfF0\nMbPHzGyfmW2reuxsM3vRzHZEf89pZY1TzczON7MNZvaumb1jZqujx9t23GbWaWa/N7O3ojHfHz2+\n0Mxej17jv4g+JrltmFnezN40s2ej+209Xkkys11m9raZbTWz/uix1F/bQQR3CBsST5OfSlpe89ga\nSS+7+5ckvRzdbyfDkr7j7hdLWirpzujftp3HfUzS1e7+ZUmLJC03s6WS/knSP7v7X0r6VNK3W1hj\nGlZLeq/qfruPt+Ir7r6oahlg6q/tIIJb07khcQu5+yZJn9Q8fKOkn0W3fybppmktKmXu/pG7vxHd\nPqTyD/Z8tfG4vexwdLcY/XFJV0t6Knq8rcZsZj2SrpP04+i+qY3H20Dqr+1QgjtuQ+L5Laplup3r\n7h9Ft/dKOreVxaTJzHolLZb0utp83FHbYKukfZJelLRT0kF3H44OabfX+I8kfVejW8J2qb3HW+GS\nfm1mW6J9d6VpeG1P2WbBOHXu7mbWlst8zOxMSU9LutvdP7fqjZPbcNzufkLSIjObLWm9pAtbXFJq\nzOx6SfvcfYuZXdXqeqbZMnffY2bnSHrRzLZXP5nWazuUGXdTGxK3qY/N7DxJiv7e1+J6ppyZFVUO\n7cfd/ZfRw20/bkly94OSNkj6G0mzzawyWWqn1/iVklaY2S6V25xXS/oXte94R7j7nujvfSr/gl6i\naXhthxLcp/OGxL+S9K3o9rckPdPCWqZc1Ov8iaT33P2HVU+17bjNrDuaacvMZkj6qsq9/Q2Svh4d\n1jZjdvfvuXuPu/eq/LP7G3e/TW063gozO8PMZlVuS/qapG2ahtd2MBfgmNm1KvfJKhsS/6DFJU05\nM3tS0lUqf4LYx5K+L+k/Ja2TtEDlT1S81d1r38DMLDNbJum3kt7WaP/z71Xuc7fluM3sr1R+Uyqv\n8uRonbv/o5n9hcoz0rMlvSnpdnc/1rpKp17UKrnX3a9v9/FG41sf3S1IesLdf2BmXUr5tR1McAMA\nmhNKqwQA0CSCGwAyhuAGgIwhuAEgYwhuAMgYghsAMobgBoCMIbgBIGP+H5FexgDOrtN9AAAAAElF\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 3s 17ms/step - loss: 4.7700 - mean_squared_error: 4.7700 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00257, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0040 - mean_squared_error: 0.0040 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00257 to 0.00222, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00222 to 0.00207, saving model to weights.best_mlp.hdf5\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00207 to 0.00201, saving model to weights.best_mlp.hdf5\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00201 to 0.00194, saving model to weights.best_mlp.hdf5\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00194 to 0.00188, saving model to weights.best_mlp.hdf5\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.8567e-04 - mean_squared_error: 9.8567e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00188 to 0.00184, saving model to weights.best_mlp.hdf5\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.4038e-04 - mean_squared_error: 9.4038e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00184\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.0900e-04 - mean_squared_error: 9.0900e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00184\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.8099e-04 - mean_squared_error: 8.8099e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00184\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.5058e-04 - mean_squared_error: 8.5058e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00184\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.1771e-04 - mean_squared_error: 8.1771e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00184\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.8588e-04 - mean_squared_error: 7.8588e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00184\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.6016e-04 - mean_squared_error: 7.6016e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00184\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.4289e-04 - mean_squared_error: 7.4289e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00184\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.3366e-04 - mean_squared_error: 7.3366e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00184\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.2903e-04 - mean_squared_error: 7.2903e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00184\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.2558e-04 - mean_squared_error: 7.2558e-04 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00184\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.1639e-04 - mean_squared_error: 7.1639e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00184\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.0231e-04 - mean_squared_error: 7.0231e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00184\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.7954e-04 - mean_squared_error: 6.7954e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.00184 to 0.00179, saving model to weights.best_mlp.hdf5\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.5128e-04 - mean_squared_error: 6.5128e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.00179 to 0.00154, saving model to weights.best_mlp.hdf5\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.1965e-04 - mean_squared_error: 6.1965e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.00154 to 0.00130, saving model to weights.best_mlp.hdf5\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.8632e-04 - mean_squared_error: 5.8632e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.00130 to 0.00109, saving model to weights.best_mlp.hdf5\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.5915e-04 - mean_squared_error: 5.5915e-04 - val_loss: 9.0651e-04 - val_mean_squared_error: 9.0651e-04\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.00109 to 0.00091, saving model to weights.best_mlp.hdf5\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.2913e-04 - mean_squared_error: 5.2913e-04 - val_loss: 7.5225e-04 - val_mean_squared_error: 7.5225e-04\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.00091 to 0.00075, saving model to weights.best_mlp.hdf5\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.0768e-04 - mean_squared_error: 5.0768e-04 - val_loss: 6.2226e-04 - val_mean_squared_error: 6.2226e-04\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.00075 to 0.00062, saving model to weights.best_mlp.hdf5\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.9973e-04 - mean_squared_error: 4.9973e-04 - val_loss: 5.1947e-04 - val_mean_squared_error: 5.1947e-04\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.00062 to 0.00052, saving model to weights.best_mlp.hdf5\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.9073e-04 - mean_squared_error: 4.9073e-04 - val_loss: 4.4336e-04 - val_mean_squared_error: 4.4336e-04\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.00052 to 0.00044, saving model to weights.best_mlp.hdf5\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.7897e-04 - mean_squared_error: 4.7897e-04 - val_loss: 3.8918e-04 - val_mean_squared_error: 3.8918e-04\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.00044 to 0.00039, saving model to weights.best_mlp.hdf5\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.7252e-04 - mean_squared_error: 4.7252e-04 - val_loss: 3.4060e-04 - val_mean_squared_error: 3.4060e-04\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.00039 to 0.00034, saving model to weights.best_mlp.hdf5\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.6728e-04 - mean_squared_error: 4.6728e-04 - val_loss: 3.0393e-04 - val_mean_squared_error: 3.0393e-04\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.00034 to 0.00030, saving model to weights.best_mlp.hdf5\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.5859e-04 - mean_squared_error: 4.5859e-04 - val_loss: 2.6844e-04 - val_mean_squared_error: 2.6844e-04\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00030 to 0.00027, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3847e-04 - mean_squared_error: 4.3847e-04 - val_loss: 2.5328e-04 - val_mean_squared_error: 2.5328e-04\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00027 to 0.00025, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2399e-04 - mean_squared_error: 4.2399e-04 - val_loss: 2.4221e-04 - val_mean_squared_error: 2.4221e-04\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00025 to 0.00024, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.9460e-04 - mean_squared_error: 3.9460e-04 - val_loss: 2.4617e-04 - val_mean_squared_error: 2.4617e-04\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00024\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.7656e-04 - mean_squared_error: 3.7656e-04 - val_loss: 2.5429e-04 - val_mean_squared_error: 2.5429e-04\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00024\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.4552e-04 - mean_squared_error: 3.4552e-04 - val_loss: 2.7905e-04 - val_mean_squared_error: 2.7905e-04\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00024\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.3361e-04 - mean_squared_error: 3.3361e-04 - val_loss: 2.9578e-04 - val_mean_squared_error: 2.9578e-04\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00024\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.2116e-04 - mean_squared_error: 3.2116e-04 - val_loss: 3.1672e-04 - val_mean_squared_error: 3.1672e-04\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00024\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.1034e-04 - mean_squared_error: 3.1034e-04 - val_loss: 3.7272e-04 - val_mean_squared_error: 3.7272e-04\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00024\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.1169e-04 - mean_squared_error: 3.1169e-04 - val_loss: 3.9676e-04 - val_mean_squared_error: 3.9676e-04\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00024\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.1174e-04 - mean_squared_error: 3.1174e-04 - val_loss: 5.0540e-04 - val_mean_squared_error: 5.0540e-04\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00024\n",
            "Epoch 00043: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 200\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 2\n",
            "MLP: Train: 0.009965, Validation: 0.000505\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAE05JREFUeJzt3V+MXOV5x/HfM7OzO2MwtVkvtuu1\nu64a8b+xxeK6MhcUKa35j0rBiaDKRYRvqDAoKHLaC6AiEr1oSpGKqAlWIhVILQgljaAIiI0bYUjX\nxCkGXBxSkNf88drYYBfvev88vZgzOzO7Z2bOrnd23sN+P5LlmTlnzzw+3v359fOeOa+5uwAA6ZFp\ndQEAgKkhuAEgZQhuAEgZghsAUobgBoCUIbgBIGXakuxkZu9LOi5pVNKIu/c2sygAQG2JgjvyJ+5+\nuGmVAAASoVUCACljST45aWb/K+moJJf0z+6+pd7+ixYt8p6enhkpEADmgt27dx92964k+yZtlVzm\n7gfN7BxJL5rZPnffWbmDmW2UtFGSVqxYob6+vikVDQBzmZl9kHTfRK0Sdz8Y/X5I0jOS1sTss8Xd\ne929t6sr0T8aAIBpaBjcZnaGmc0vPZb0p5L2NrswAEC8JK2SxZKeMbPS/k+4+380tSoAQE0Ng9vd\nfyvpq7NQC4A5bHh4WP39/RocHGx1KU2Vz+fV3d2tXC437WNM5TpuAGia/v5+zZ8/Xz09PYr+h/+l\n4+46cuSI+vv7tXLlymkfh+u4AQRhcHBQnZ2dX9rQliQzU2dn52n/r4LgBhCML3Nol8zEnzGo4H7o\n5f165d2BVpcBAEELKrgfeeU97SS4AbTAsWPH9PDDD0/566666iodO3asCRXVFlRwF3JZDQ6PtroM\nAHNQreAeGRmp+3XPPfecFixY0KyyYgV1VUk+l9Xg8FirywAwB23evFnvvfeeVq1apVwup3w+r4UL\nF2rfvn169913dcMNN+jAgQMaHBzUpk2btHHjRklST0+P+vr6dOLECV155ZW67LLL9Oqrr2rZsmV6\n9tlnVSgUZrzWwII7w4gbgO7797f09oefz+gxL/jds3TPtRfW3P7AAw9o79692rNnj3bs2KGrr75a\ne/fuHb9sb+vWrTr77LN18uRJXXrppbrxxhvV2dlZdYz9+/frySef1KOPPqqbb75ZTz/9tG699dYZ\n/XNIwQU3rRIAYVizZk3VtdYPPfSQnnnmGUnSgQMHtH///knBvXLlSq1atUqSdMkll+j9999vSm3h\nBfcIwQ3MdfVGxrPljDPOGH+8Y8cOvfTSS9q1a5fmzZunyy+/PPZa7I6OjvHH2WxWJ0+ebEptwU1O\nnjxFcAOYffPnz9fx48djt3322WdauHCh5s2bp3379um1116b5eqqBTbizujT/2NyEsDs6+zs1Lp1\n63TRRRepUCho8eLF49vWr1+vRx55ROeff77OPfdcrV27toWVBhbcHfS4AbTQE088Eft6R0eHnn/+\n+dhtpT72okWLtHdv+Y7Xd99994zXVxJcq4TgBoD6ggrufC6jwRFaJQBQT1DBzeQkADQWVHCXLgdM\nsvI8AMxVwQW3uzREuwQAagouuCVpiPuVAEBNgQV3sRw+PQkgdGeeeWbL3juo4C5EI24mKAGgtqA+\ngFNqlTDiBjDbNm/erOXLl+v222+XJN17771qa2vT9u3bdfToUQ0PD+v+++/X9ddf3+JKgwvu4n8A\nGHEDc9zzm6WP35zZYy65WLrygZqbN2zYoDvvvHM8uLdt26YXXnhBd9xxh8466ywdPnxYa9eu1XXX\nXdfytTEDC+5oxM3kJIBZtnr1ah06dEgffvihBgYGtHDhQi1ZskR33XWXdu7cqUwmo4MHD+qTTz7R\nkiVLWlprmMFNqwSY2+qMjJvppptu0lNPPaWPP/5YGzZs0OOPP66BgQHt3r1buVxOPT09sbdznW1B\nBXdpcnKQVgmAFtiwYYNuu+02HT58WK+88oq2bdumc845R7lcTtu3b9cHH3zQ6hIlBRbcjLgBtNKF\nF16o48ePa9myZVq6dKluueUWXXvttbr44ovV29ur8847r9UlSgouuKPruOlxA2iRN98sT4ouWrRI\nu3btit3vxIkTs1XSJFzHDQApE1Rw0yoBgMaCCu6OtqhVwogbmJPmwp1BZ+LPGFRwmxmLKQBzVD6f\n15EjR77U4e3uOnLkiPL5/GkdJ6jJSSm6JzfLlwFzTnd3t/r7+zUwMNDqUpoqn8+ru7v7tI6ROLjN\nLCupT9JBd7/mtN61DlbBAeamXC6nlStXtrqMVJhKq2STpHeaVUhJcRUcWiUAUEui4DazbklXS/pB\nc8spBjcjbgCoLemI+0FJ35FUcyhsZhvNrM/M+k6nR5XPZTTE5YAAUFPD4DazayQdcvfd9fZz9y3u\n3uvuvV1dXdMuKN/G5CQA1JNkxL1O0nVm9r6kH0u6wsz+pVkFFdqzOklwA0BNDYPb3b/r7t3u3iPp\n65J+7u63NqugfC7DvUoAoI6gPoAjFVslTE4CQG1T+gCOu++QtKMplUTy7VkmJwGgjiBH3LRKAKC2\n4IK70J5hchIA6gguuPNtWY2OuYZHGXUDQJzwgru0mAKjbgCIFV5wt0eLKRDcABArvOCOFlMYYoIS\nAGIFF9yFdlolAFBPcMGdb6NVAgD1hBfcpQWDaZUAQKzggrvQXiyJVgkAxAsuuDtolQBAXcEFd4HL\nAQGgruCCu9zjJrgBIE54wR1dx83kJADECy64uY4bAOoLLri5jhsA6gsuuDMZU3sbt3YFgFqCC26p\n2OfmXiUAEC/M4M5laZUAQA1BBnehPUurBABqCDK4i+tOEtwAECfM4M5ldJIeNwDECjS4GXEDQC3B\nBvcQwQ0AsYIM7kKOyUkAqCXI4M7nMtyrBABqCDS46XEDQC3BBjetEgCIF2xw85F3AIgXZHAXclmd\nGh3T6Ji3uhQACE6QwZ3PlRZToF0CABMFGtzckxsAamkY3GaWN7NfmtmvzewtM7uv2UUVcqyCAwC1\ntCXYZ0jSFe5+wsxykn5hZs+7+2vNKqojx7qTAFBLw+B2d5d0Inqai341ddawQKsEAGpK1OM2s6yZ\n7ZF0SNKL7v56M4uixw0AtSUKbncfdfdVkrolrTGziybuY2YbzazPzPoGBgZOq6hycNMqAYCJpnRV\nibsfk7Rd0vqYbVvcvdfde7u6uk6rKCYnAaC2JFeVdJnZguhxQdLXJO1rZlFcxw0AtSW5qmSppB+Z\nWVbFoN/m7j9rZlF5RtwAUFOSq0r+W9LqWahlXCm4WUwBACYL9JOTXMcNALUEGty0SgCgliCDO5fN\nqC1jTE4CQIwgg1sqrYJDqwQAJgo6uGmVAMBkAQd3hqtKACBGsMFdYMQNALGCDW5WegeAeAEHd4bJ\nSQCIEXBw0yoBgDhBBzetEgCYLNjgLhDcABAr2OCmxw0A8QIO7qwGRxhxA8BEwQZ3IZfVyVMENwBM\nFGxwd+SyGhoZ09hYUxeUB4DUCTa4S+tODo3Q5waASsEGN+tOAkC8gIO7OOJmghIAqgUb3KVWCROU\nAFAt2OBm3UkAiBdwcLPuJADECT64WUwBAKoFH9xMTgJAtWCDuzw5SY8bACoFG9xcxw0A8QIOblol\nABAn+ODmOm4AqBZwcBdL414lAFAt2OBuz2aUMUbcADBRsMFtZqw7CQAxgg1uiVVwACBO0MFdXAWH\nHjcAVGoY3Ga23My2m9nbZvaWmW2ajcIkqSOXYcQNABO0JdhnRNK33f0NM5svabeZvejubze5NhVy\nWQ0yOQkAVRqOuN39I3d/I3p8XNI7kpY1uzCJHjcAxJlSj9vMeiStlvR6zLaNZtZnZn0DAwMzUlw+\nl+F+3AAwQeLgNrMzJT0t6U53/3zidnff4u697t7b1dU1I8UVJycZcQNApUTBbWY5FUP7cXf/SXNL\nKuugVQIAkyS5qsQkPSbpHXf/fvNLKmNyEgAmSzLiXifpLyVdYWZ7ol9XNbkuSVGPm3uVAECVhpcD\nuvsvJNks1DJJvo2PvAPARGF/crI9q5PDo3L3VpcCAMEIOrjzuazcpVOjtEsAoCT44JbEtdwAUCHw\n4GbdSQCYKOzgbiuNuAluACgJOrgL7dG6kwQ3AIwLOrjLrRJ63ABQEnZw0yoBgEnCDm5aJQAwSdjB\nHY24hwhuABgXdHAzOQkAkwUd3ExOAsBkYQc3k5MAMEnQwU2rBAAmCzq4O9polQDAREEHt5lFCwYz\n4gaAkqCDWyreIZDgBoCy8IObVXAAoErwwV1cBYceNwCUBB/cHW30uAGgUvDBXWinVQIAlYIPbnrc\nAFAt/ODOZbiOGwAqBB/cxclJRtwAUBJ8cNMqAYBq4Qd3e5ZWCQBUCD+4GXEDQJXwg5t7lQBAleCD\nu5DLamTMNTxKuwQApBQEdz7HYgoAUCkFwc09uQGgUgqCmxE3AFRqGNxmttXMDpnZ3tkoaCKCGwCq\nJRlx/1DS+ibXUVMhx7qTAFCpYXC7+05Jn85CLbHKI2563AAgzWCP28w2mlmfmfUNDAzM1GErJicZ\ncQOANIPB7e5b3L3X3Xu7urpm6rDjI25aJQBQxFUlAJAywQd3oZ3gBoBKSS4HfFLSLknnmlm/mX2r\n+WWV5dv4AA4AVGprtIO7f2M2CqmFVgkAVAu+VcLkJABUCz64sxlTe5Z1JwGgJPjglrgnNwBUSklw\nswoOAJQQ3ACQMqkI7kIuy+QkAERSEdzFHjeTkwAgpSa4aZUAQAnBDQApk5LgplUCACWpCG4mJwGg\nLBXBTasEAMoIbgBImRQFNz1uAJBSE9wZnRod0+iYt7oUAGi5VAR3gXtyA8C4VAQ3iykAQFlKgjta\nvmyEPjcApCS4o1VwTjHiBoBUBTetEgBISXCXJieHRghuAEhFcJdbJfS4ASAlwR1NTtIqAYB0BHep\nVcKNpgAgJcHN5CQAlKUruLmOGwDSEtxRj5vruAEgLcFNqwQASlIR3LlsRm0ZY3ISAJSS4Ja4JzcA\nlKQruPnkJACkKbgzTE4CgBIGt5mtN7P/MbPfmNnmZhcVhxE3ABQ1DG4zy0r6J0lXSrpA0jfM7IJm\nFzZRIZfltq4AoGQj7jWSfuPuv3X3U5J+LOn65pY1WT6XYXISACS1JdhnmaQDFc/7Jf1RU6r5+/Ol\n4S+Kj82iF4u/PzY0quHRMR25x6q+xCqeuspPqveq3jZRvW1xx0qm9sLGVmdb5fuV9/M62ybvV1mv\njb+W7D2no96RK8/txPMcty3u76JqPyvtVxJ3jLivnbwt/v0m1+EN9o07br33aPT9VrWvTfe7r/7X\nNWPZ7UaV1qqp0ffm6alV1fTes1GtX2R/R+f9za5pHXsqkgR3Ima2UdJGSVqxYsX0DnLRn0ujwxo/\nqV7+kRk6PqSDR7/QWPT6mEsul7vk0X7F38ondvyR1znZ9bY15A1+QOK3ubzmtvI+CYLMkoViZXzX\nfc9pZITVPX3J/nEZf8198mt194uP74nvWeuHzSb93dfef/JrFe9T93uoettUQmragdbge7q5QZke\nU/kHNOnXjbbPn245U5IkuA9KWl7xvDt6rYq7b5G0RZJ6e3un953xZ9+ruakr+gUAc12SHvd/SfqK\nma00s3ZJX5f00+aWBQCopeGI291HzOyvJL0gKStpq7u/1fTKAACxEvW43f05Sc81uRYAQAKp+eQk\nAKCI4AaAlCG4ASBlCG4ASBmCGwBSxvy0PjlY46BmA5I+mOaXL5J0eAbL+bLh/DTGOaqP89NYK87R\n77l7os8ZNiW4T4eZ9bl7b6vrCBXnpzHOUX2cn8ZCP0e0SgAgZQhuAEiZEIN7S6sLCBznpzHOUX2c\nn8aCPkfB9bgBAPWFOOIGANQRTHCHsCBxaMxsq5kdMrO9Fa+dbWYvmtn+6PeFrayxlcxsuZltN7O3\nzewtM9sUvc45iphZ3sx+aWa/js7RfdHrK83s9ejn7V+jWzbPWWaWNbNfmdnPoudBn58ggjuUBYkD\n9ENJ6ye8tlnSy+7+FUkvR8/nqhFJ33b3CyStlXR79H3DOSobknSFu39V0ipJ681sraS/k/QP7v4H\nko5K+lYLawzBJknvVDwP+vwEEdwKZEHi0Lj7TkmfTnj5ekk/ih7/SNINs1pUQNz9I3d/I3p8XMUf\nvGXiHI3zohPR01z0yyVdIemp6PU5fY7MrFvS1ZJ+ED03BX5+QgnuuAWJl7WoltAtdvePoscfS1rc\nymJCYWY9klZLel2coypRG2CPpEOSXpT0nqRj7j4S7TLXf94elPQdqbikraROBX5+QgluTIMXLwma\n85cFmdmZkp6WdKe7f165jXMkufuou69Scb3YNZLOa3FJwTCzayQdcvfdra5lKmZslffTlGhBYkiS\nPjGzpe7+kZktVXEUNWeZWU7F0H7c3X8Svcw5iuHux8xsu6Q/lrTAzNqiUeVc/nlbJ+k6M7tKUl7S\nWZL+UYGfn1BG3CxInNxPJX0zevxNSc+2sJaWinqRj0l6x92/X7GJcxQxsy4zWxA9Lkj6mopzAdsl\n/UW025w9R+7+XXfvdvceFXPn5+5+iwI/P8F8ACf6F+9BlRck/l6LS2o5M3tS0uUq3qnsE0n3SPo3\nSdskrVDxDow3u/vECcw5wcwuk/Sfkt5UuT/51yr2uTlHkszsD1WcXMuqOFDb5u5/a2a/r+JFAGdL\n+pWkW919qHWVtp6ZXS7pbne/JvTzE0xwAwCSCaVVAgBIiOAGgJQhuAEgZQhuAEgZghsAUobgBoCU\nIbgBIGUIbgBImf8HmT6aUUmObJoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 3s 18ms/step - loss: 4.8398 - mean_squared_error: 4.8398 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00249, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00249 to 0.00173, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00173\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00173\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00173\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00173\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.9363e-04 - mean_squared_error: 9.9363e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00173\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.5107e-04 - mean_squared_error: 9.5107e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00173\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.2512e-04 - mean_squared_error: 9.2512e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00173\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.0548e-04 - mean_squared_error: 9.0548e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00173\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.8372e-04 - mean_squared_error: 8.8372e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00173\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.5650e-04 - mean_squared_error: 8.5650e-04 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00173\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.2531e-04 - mean_squared_error: 8.2531e-04 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00173\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.9528e-04 - mean_squared_error: 7.9528e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00173\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.7085e-04 - mean_squared_error: 7.7085e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00173\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.5351e-04 - mean_squared_error: 7.5351e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00173\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.4384e-04 - mean_squared_error: 7.4384e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00173\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.3812e-04 - mean_squared_error: 7.3812e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00173\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.3207e-04 - mean_squared_error: 7.3207e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00173\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.2023e-04 - mean_squared_error: 7.2023e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00173\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.0227e-04 - mean_squared_error: 7.0227e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00173\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.7931e-04 - mean_squared_error: 6.7931e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00173\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.4722e-04 - mean_squared_error: 6.4722e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.00173 to 0.00159, saving model to weights.best_mlp.hdf5\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.1606e-04 - mean_squared_error: 6.1606e-04 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.00159 to 0.00135, saving model to weights.best_mlp.hdf5\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.7999e-04 - mean_squared_error: 5.7999e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.00135 to 0.00113, saving model to weights.best_mlp.hdf5\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.4559e-04 - mean_squared_error: 5.4559e-04 - val_loss: 9.3077e-04 - val_mean_squared_error: 9.3077e-04\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.00113 to 0.00093, saving model to weights.best_mlp.hdf5\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.1751e-04 - mean_squared_error: 5.1751e-04 - val_loss: 7.6187e-04 - val_mean_squared_error: 7.6187e-04\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.00093 to 0.00076, saving model to weights.best_mlp.hdf5\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.9048e-04 - mean_squared_error: 4.9048e-04 - val_loss: 6.2111e-04 - val_mean_squared_error: 6.2111e-04\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.00076 to 0.00062, saving model to weights.best_mlp.hdf5\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.7464e-04 - mean_squared_error: 4.7464e-04 - val_loss: 5.0643e-04 - val_mean_squared_error: 5.0643e-04\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.00062 to 0.00051, saving model to weights.best_mlp.hdf5\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.6082e-04 - mean_squared_error: 4.6082e-04 - val_loss: 4.2489e-04 - val_mean_squared_error: 4.2489e-04\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.00051 to 0.00042, saving model to weights.best_mlp.hdf5\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.5064e-04 - mean_squared_error: 4.5064e-04 - val_loss: 3.6487e-04 - val_mean_squared_error: 3.6487e-04\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.00042 to 0.00036, saving model to weights.best_mlp.hdf5\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.4418e-04 - mean_squared_error: 4.4418e-04 - val_loss: 3.1656e-04 - val_mean_squared_error: 3.1656e-04\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.00036 to 0.00032, saving model to weights.best_mlp.hdf5\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.4077e-04 - mean_squared_error: 4.4077e-04 - val_loss: 2.7371e-04 - val_mean_squared_error: 2.7371e-04\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00032 to 0.00027, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3702e-04 - mean_squared_error: 4.3702e-04 - val_loss: 2.4234e-04 - val_mean_squared_error: 2.4234e-04\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00027 to 0.00024, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2820e-04 - mean_squared_error: 4.2820e-04 - val_loss: 2.2388e-04 - val_mean_squared_error: 2.2388e-04\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00024 to 0.00022, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.1710e-04 - mean_squared_error: 4.1710e-04 - val_loss: 2.1852e-04 - val_mean_squared_error: 2.1852e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00022 to 0.00022, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.9797e-04 - mean_squared_error: 3.9797e-04 - val_loss: 2.1683e-04 - val_mean_squared_error: 2.1683e-04\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.00022 to 0.00022, saving model to weights.best_mlp.hdf5\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.7538e-04 - mean_squared_error: 3.7538e-04 - val_loss: 2.2303e-04 - val_mean_squared_error: 2.2303e-04\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00022\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.5252e-04 - mean_squared_error: 3.5252e-04 - val_loss: 2.3381e-04 - val_mean_squared_error: 2.3381e-04\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00022\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.3295e-04 - mean_squared_error: 3.3295e-04 - val_loss: 2.5587e-04 - val_mean_squared_error: 2.5587e-04\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00022\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.1653e-04 - mean_squared_error: 3.1653e-04 - val_loss: 2.8789e-04 - val_mean_squared_error: 2.8789e-04\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00022\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.9701e-04 - mean_squared_error: 2.9701e-04 - val_loss: 3.3438e-04 - val_mean_squared_error: 3.3438e-04\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00022\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.8767e-04 - mean_squared_error: 2.8767e-04 - val_loss: 3.9108e-04 - val_mean_squared_error: 3.9108e-04\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00022\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.7418e-04 - mean_squared_error: 2.7418e-04 - val_loss: 4.7609e-04 - val_mean_squared_error: 4.7609e-04\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00022\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.6759e-04 - mean_squared_error: 2.6759e-04 - val_loss: 5.5419e-04 - val_mean_squared_error: 5.5419e-04\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00022\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.6680e-04 - mean_squared_error: 2.6680e-04 - val_loss: 6.2135e-04 - val_mean_squared_error: 6.2135e-04\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.00022\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.6762e-04 - mean_squared_error: 2.6762e-04 - val_loss: 6.6935e-04 - val_mean_squared_error: 6.6935e-04\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00022\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.6344e-04 - mean_squared_error: 2.6344e-04 - val_loss: 6.9145e-04 - val_mean_squared_error: 6.9145e-04\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.00022\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.6324e-04 - mean_squared_error: 2.6324e-04 - val_loss: 7.0473e-04 - val_mean_squared_error: 7.0473e-04\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.00022\n",
            "Epoch 50/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.8210e-04 - mean_squared_error: 2.8210e-04 - val_loss: 7.4843e-04 - val_mean_squared_error: 7.4843e-04\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.00022\n",
            "Epoch 51/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.2646e-04 - mean_squared_error: 3.2646e-04 - val_loss: 7.4087e-04 - val_mean_squared_error: 7.4087e-04\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.00022\n",
            "Epoch 52/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.8313e-04 - mean_squared_error: 2.8313e-04 - val_loss: 7.2999e-04 - val_mean_squared_error: 7.2999e-04\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.00022\n",
            "Epoch 00052: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 200\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 3\n",
            "MLP: Train: 0.008934, Validation: 0.000730\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbgBoMwQ3ALQZghsA2gzBDQBt5v8BjCLDBqavRZQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "Train on 172 samples, validate on 22 samples\n",
            "Epoch 1/200\n",
            "172/172 [==============================] - 3s 18ms/step - loss: 4.9815 - mean_squared_error: 4.9815 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00288, saving model to weights.best_mlp.hdf5\n",
            "Epoch 2/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00288 to 0.00162, saving model to weights.best_mlp.hdf5\n",
            "Epoch 3/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00162\n",
            "Epoch 4/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00162\n",
            "Epoch 5/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00162\n",
            "Epoch 6/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00162\n",
            "Epoch 7/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00162\n",
            "Epoch 8/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.6946e-04 - mean_squared_error: 9.6946e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00162\n",
            "Epoch 9/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.3928e-04 - mean_squared_error: 9.3928e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00162\n",
            "Epoch 10/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 9.1661e-04 - mean_squared_error: 9.1661e-04 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00162\n",
            "Epoch 11/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.9301e-04 - mean_squared_error: 8.9301e-04 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00162\n",
            "Epoch 12/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.6489e-04 - mean_squared_error: 8.6489e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00162\n",
            "Epoch 13/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 8.3246e-04 - mean_squared_error: 8.3246e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00162\n",
            "Epoch 14/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.9997e-04 - mean_squared_error: 7.9997e-04 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00162\n",
            "Epoch 15/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.7183e-04 - mean_squared_error: 7.7183e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00162\n",
            "Epoch 16/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.5148e-04 - mean_squared_error: 7.5148e-04 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00162\n",
            "Epoch 17/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.3758e-04 - mean_squared_error: 7.3758e-04 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00162\n",
            "Epoch 18/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.2868e-04 - mean_squared_error: 7.2868e-04 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00162\n",
            "Epoch 19/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.2065e-04 - mean_squared_error: 7.2065e-04 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00162\n",
            "Epoch 20/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 7.1038e-04 - mean_squared_error: 7.1038e-04 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00162\n",
            "Epoch 21/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.9568e-04 - mean_squared_error: 6.9568e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00162\n",
            "Epoch 22/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.7292e-04 - mean_squared_error: 6.7292e-04 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00162\n",
            "Epoch 23/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.4666e-04 - mean_squared_error: 6.4666e-04 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00162\n",
            "Epoch 24/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 6.1739e-04 - mean_squared_error: 6.1739e-04 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.00162 to 0.00139, saving model to weights.best_mlp.hdf5\n",
            "Epoch 25/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.8625e-04 - mean_squared_error: 5.8625e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.00139 to 0.00117, saving model to weights.best_mlp.hdf5\n",
            "Epoch 26/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.5289e-04 - mean_squared_error: 5.5289e-04 - val_loss: 9.8140e-04 - val_mean_squared_error: 9.8140e-04\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.00117 to 0.00098, saving model to weights.best_mlp.hdf5\n",
            "Epoch 27/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 5.2277e-04 - mean_squared_error: 5.2277e-04 - val_loss: 8.2318e-04 - val_mean_squared_error: 8.2318e-04\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.00098 to 0.00082, saving model to weights.best_mlp.hdf5\n",
            "Epoch 28/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.9701e-04 - mean_squared_error: 4.9701e-04 - val_loss: 6.8507e-04 - val_mean_squared_error: 6.8507e-04\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.00082 to 0.00069, saving model to weights.best_mlp.hdf5\n",
            "Epoch 29/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.7980e-04 - mean_squared_error: 4.7980e-04 - val_loss: 5.7070e-04 - val_mean_squared_error: 5.7070e-04\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.00069 to 0.00057, saving model to weights.best_mlp.hdf5\n",
            "Epoch 30/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.6381e-04 - mean_squared_error: 4.6381e-04 - val_loss: 4.7987e-04 - val_mean_squared_error: 4.7987e-04\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.00057 to 0.00048, saving model to weights.best_mlp.hdf5\n",
            "Epoch 31/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.5162e-04 - mean_squared_error: 4.5162e-04 - val_loss: 4.1541e-04 - val_mean_squared_error: 4.1541e-04\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.00048 to 0.00042, saving model to weights.best_mlp.hdf5\n",
            "Epoch 32/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.4394e-04 - mean_squared_error: 4.4394e-04 - val_loss: 3.6254e-04 - val_mean_squared_error: 3.6254e-04\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.00042 to 0.00036, saving model to weights.best_mlp.hdf5\n",
            "Epoch 33/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.3568e-04 - mean_squared_error: 4.3568e-04 - val_loss: 3.1696e-04 - val_mean_squared_error: 3.1696e-04\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00036 to 0.00032, saving model to weights.best_mlp.hdf5\n",
            "Epoch 34/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2947e-04 - mean_squared_error: 4.2947e-04 - val_loss: 2.7768e-04 - val_mean_squared_error: 2.7768e-04\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00032 to 0.00028, saving model to weights.best_mlp.hdf5\n",
            "Epoch 35/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.2331e-04 - mean_squared_error: 4.2331e-04 - val_loss: 2.4941e-04 - val_mean_squared_error: 2.4941e-04\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00028 to 0.00025, saving model to weights.best_mlp.hdf5\n",
            "Epoch 36/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.1501e-04 - mean_squared_error: 4.1501e-04 - val_loss: 2.3139e-04 - val_mean_squared_error: 2.3139e-04\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00025 to 0.00023, saving model to weights.best_mlp.hdf5\n",
            "Epoch 37/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 4.0050e-04 - mean_squared_error: 4.0050e-04 - val_loss: 2.2820e-04 - val_mean_squared_error: 2.2820e-04\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.00023 to 0.00023, saving model to weights.best_mlp.hdf5\n",
            "Epoch 38/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.8319e-04 - mean_squared_error: 3.8319e-04 - val_loss: 2.2444e-04 - val_mean_squared_error: 2.2444e-04\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.00023 to 0.00022, saving model to weights.best_mlp.hdf5\n",
            "Epoch 39/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.6297e-04 - mean_squared_error: 3.6297e-04 - val_loss: 2.3347e-04 - val_mean_squared_error: 2.3347e-04\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00022\n",
            "Epoch 40/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.5045e-04 - mean_squared_error: 3.5045e-04 - val_loss: 2.3565e-04 - val_mean_squared_error: 2.3565e-04\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00022\n",
            "Epoch 41/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.2851e-04 - mean_squared_error: 3.2851e-04 - val_loss: 2.6281e-04 - val_mean_squared_error: 2.6281e-04\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00022\n",
            "Epoch 42/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.2541e-04 - mean_squared_error: 3.2541e-04 - val_loss: 2.7833e-04 - val_mean_squared_error: 2.7833e-04\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00022\n",
            "Epoch 43/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.9394e-04 - mean_squared_error: 2.9394e-04 - val_loss: 3.3379e-04 - val_mean_squared_error: 3.3379e-04\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00022\n",
            "Epoch 44/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 3.0699e-04 - mean_squared_error: 3.0699e-04 - val_loss: 3.6649e-04 - val_mean_squared_error: 3.6649e-04\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00022\n",
            "Epoch 45/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.8448e-04 - mean_squared_error: 2.8448e-04 - val_loss: 4.7033e-04 - val_mean_squared_error: 4.7033e-04\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00022\n",
            "Epoch 46/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.9812e-04 - mean_squared_error: 2.9812e-04 - val_loss: 4.8914e-04 - val_mean_squared_error: 4.8914e-04\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.00022\n",
            "Epoch 47/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.9019e-04 - mean_squared_error: 2.9019e-04 - val_loss: 6.1141e-04 - val_mean_squared_error: 6.1141e-04\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00022\n",
            "Epoch 48/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.9492e-04 - mean_squared_error: 2.9492e-04 - val_loss: 6.1314e-04 - val_mean_squared_error: 6.1314e-04\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.00022\n",
            "Epoch 49/200\n",
            "172/172 [==============================] - 0s 2ms/step - loss: 2.9982e-04 - mean_squared_error: 2.9982e-04 - val_loss: 6.9524e-04 - val_mean_squared_error: 6.9524e-04\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.00022\n",
            "Epoch 00049: early stopping\n",
            "MLP: Number of nodes in the hidden layer: 200\n",
            "MLP: Number of epoches: 200\n",
            "MLP: Early stopping criteria: loss\n",
            "MLP: Patience: Epochs before early stop: 4\n",
            "MLP: Train: 0.009205, Validation: 0.000695\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAFoMwQ0ALeb/AUUPtAqywLesAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm2SUM5SE3f-",
        "colab_type": "text"
      },
      "source": [
        "### Step 2. Best Model Prediction and Performance on Test + Export Predictions to CSV\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3sfqRfPE33T",
        "colab_type": "code",
        "outputId": "d9dd4119-ebd4-4f71-bd6a-f02e7c6a619b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# tuned hyper parameters for best model\n",
        "'''\n",
        "Epoch 00048: val_loss did not improve from 0.00024\n",
        "Epoch 00048: early stopping\n",
        "Number of nodes in the hidden layer: 200\n",
        "Number of epoches: 200\n",
        "Early stopping criteria: loss\n",
        "Patience: Epochs before early stop: 4\n",
        "Train: 0.009736, Validation: 0.000552\n",
        "'''\n",
        "\n",
        "# Use weights from best model to predict test\n",
        "def best_model_def_loadwts_pred(X_test_flat,y_test,n_nodes,wts_fpath):  # ,n_nodes=100,n_epochs=200,estop_criteria='loss',n_patience=2):\n",
        "  \"Tuned MLP: Use Weights from Best Model\"\n",
        "  # model define\n",
        "  model = Sequential()\n",
        "  model.add(Dense(n_nodes, activation='relu', input_dim=n_input))\n",
        "  model.add(Dense(nsteps_output))\n",
        "  # load weights\n",
        "  model.load_weights(wts_fpath)  # \"weights.best.hdf5\")\n",
        "  # model compile\n",
        "  model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "  print(\"Defined model and loaded weights from best model\")\n",
        "  # model predict\n",
        "  yhat = model.predict(X_test_flat, verbose=0)\n",
        "  # predicted: select last step of predicted multistep output\n",
        "  y_pred = np.asarray([x[-1] for x in yhat])\n",
        "  print('N size: y_pred ', len(y_pred))\n",
        "  # actual: select last step of actual multistep output\n",
        "  y_actual = np.asarray([x[-1] for x in y_test])\n",
        "  print('N size: y_actual ', len(y_actual))\n",
        "  # plot actual & predicted y on test\n",
        "  plt.plot(y_actual)\n",
        "  plt.plot(y_pred)\n",
        "  plt.ylabel('LOGPRICE: Actual vs. Predicted')\n",
        "  plt.show()\n",
        "  return y_actual, y_pred\n",
        "  \n",
        "\n",
        "# model performance metrics\n",
        "def y_pred_metrics(y_actual, y_pred):\n",
        "  \"Use saved weights from best model checkpoint to predict test sample.\"\n",
        "  # import error metrics from sklearn\n",
        "  from sklearn.metrics import explained_variance_score,r2_score,mean_squared_error,mean_absolute_error,median_absolute_error\n",
        "  # compute root mean squared error of predictions\n",
        "  rmse = np.sqrt(np.mean(np.square((y_actual - y_pred)),axis=0))\n",
        "  print(f'Root Mean Squared Error: {rmse:.3f}')\n",
        "  # compute mean squared error\n",
        "  mae = mean_squared_error(y_actual, y_pred)\n",
        "  print(f'Mean Squared Error: {mae:.3f}')\n",
        "  # compute mean absolute error\n",
        "  mae = mean_absolute_error(y_actual, y_pred)\n",
        "  print(f'Mean Absolute Error: {mae:.3f}')\n",
        "  # compute median absolute error\n",
        "  mdae = median_absolute_error(y_actual, y_pred)\n",
        "  print(f'Median Absolute Error: {mdae:.3f}')\n",
        "  # compute explained_variance_score\n",
        "  evar = explained_variance_score(y_actual, y_pred)\n",
        "  print(f'Explained Variance: {evar:.3f}')\n",
        "  # compute R2\n",
        "  r2 = r2_score(y_actual, y_pred)\n",
        "  print(f'R^2: {r2:.3f}')\n",
        "  return\n",
        "\n",
        "\n",
        "# export model predictions to csv\n",
        "def export2csv(y_actual, y_pred, test_df, ticker, model_type):\n",
        "  \"Export actual and predicted y, model type, ticker and date to csv file.\"\n",
        "  # convert arrays to dfs\n",
        "  df_actual = pd.DataFrame(y_actual)\n",
        "  df_pred = pd.DataFrame(y_pred)\n",
        "  # create date, ticker, model labels\n",
        "  df_date = pd.DataFrame(test_df.index [-22:])\n",
        "  df_ticker = pd.DataFrame([ticker]*22) \n",
        "  df_model_type = pd.DataFrame([model_type]*22) \n",
        "  # concatenate y actual pred arrays\n",
        "  df_actual_pred = pd.concat((df_model_type, df_ticker, df_date, df_actual, df_pred), axis=1)\n",
        "  df_actual_pred.columns = ['MODEL', 'TICKER', 'DATE', 'LOGPRICE', 'Pred_LOGPRICE']\n",
        "  print(df_actual_pred)\n",
        "  # create file name for csv\n",
        "  fname = ticker + '_' + model_type + '.csv'\n",
        "  # write df to csv\n",
        "  df_actual_pred.to_csv(fname)\n",
        "  return df_actual_pred\n",
        "\n",
        "\n",
        "# set parameters and run model\n",
        "if __name__ == '__main__':\n",
        "  # use weights from best model to predict test\n",
        "  wts_fpath = \"weights.best_mlp.hdf5\"\n",
        "  # n_nodes one best model\n",
        "  n_nodes = 200\n",
        "  # call best model funcs\n",
        "  y_actual, y_pred = best_model_def_loadwts_pred(X_test_flat,y_test,n_nodes,wts_fpath)\n",
        "  best_mlp_metrics = y_pred_metrics(y_actual, y_pred)\n",
        "  print(best_mlp_metrics)\n",
        "  # export actual and predicted to csv\n",
        "  df_actual_pred_test = export2csv(y_actual, y_pred, test_df, 'EBAY', 'MLP')\n",
        "  print(df_actual_pred_test)\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Defined model and loaded weights from best model\n",
            "N size: y_pred  22\n",
            "N size: y_actual  22\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAD8CAYAAACGsIhGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8rmv2FYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Root Mean Squared Error: 0.032\n",
            "Mean Squared Error: 0.001\n",
            "Mean Absolute Error: 0.026\n",
            "Median Absolute Error: 0.021\n",
            "Explained Variance: 0.324\n",
            "R^2: -0.295\n",
            "None\n",
            "   MODEL TICKER       DATE  LOGPRICE  Pred_LOGPRICE\n",
            "0    MLP   EBAY 2017-06-27      3.58           3.58\n",
            "1    MLP   EBAY 2017-06-28      3.59           3.57\n",
            "2    MLP   EBAY 2017-06-29      3.57           3.59\n",
            "3    MLP   EBAY 2017-06-30      3.58           3.56\n",
            "4    MLP   EBAY 2017-07-03      3.57           3.57\n",
            "5    MLP   EBAY 2017-07-05      3.58           3.58\n",
            "6    MLP   EBAY 2017-07-06      3.56           3.57\n",
            "7    MLP   EBAY 2017-07-07      3.56           3.57\n",
            "8    MLP   EBAY 2017-07-10      3.58           3.56\n",
            "9    MLP   EBAY 2017-07-11      3.59           3.59\n",
            "10   MLP   EBAY 2017-07-12      3.61           3.59\n",
            "11   MLP   EBAY 2017-07-13      3.62           3.60\n",
            "12   MLP   EBAY 2017-07-14      3.64           3.62\n",
            "13   MLP   EBAY 2017-07-17      3.64           3.62\n",
            "14   MLP   EBAY 2017-07-18      3.63           3.61\n",
            "15   MLP   EBAY 2017-07-19      3.64           3.59\n",
            "16   MLP   EBAY 2017-07-20      3.64           3.59\n",
            "17   MLP   EBAY 2017-07-21      3.63           3.59\n",
            "18   MLP   EBAY 2017-07-24      3.62           3.57\n",
            "19   MLP   EBAY 2017-07-25      3.62           3.56\n",
            "20   MLP   EBAY 2017-07-26      3.64           3.57\n",
            "21   MLP   EBAY 2017-07-27      3.62           3.58\n",
            "   MODEL TICKER       DATE  LOGPRICE  Pred_LOGPRICE\n",
            "0    MLP   EBAY 2017-06-27      3.58           3.58\n",
            "1    MLP   EBAY 2017-06-28      3.59           3.57\n",
            "2    MLP   EBAY 2017-06-29      3.57           3.59\n",
            "3    MLP   EBAY 2017-06-30      3.58           3.56\n",
            "4    MLP   EBAY 2017-07-03      3.57           3.57\n",
            "5    MLP   EBAY 2017-07-05      3.58           3.58\n",
            "6    MLP   EBAY 2017-07-06      3.56           3.57\n",
            "7    MLP   EBAY 2017-07-07      3.56           3.57\n",
            "8    MLP   EBAY 2017-07-10      3.58           3.56\n",
            "9    MLP   EBAY 2017-07-11      3.59           3.59\n",
            "10   MLP   EBAY 2017-07-12      3.61           3.59\n",
            "11   MLP   EBAY 2017-07-13      3.62           3.60\n",
            "12   MLP   EBAY 2017-07-14      3.64           3.62\n",
            "13   MLP   EBAY 2017-07-17      3.64           3.62\n",
            "14   MLP   EBAY 2017-07-18      3.63           3.61\n",
            "15   MLP   EBAY 2017-07-19      3.64           3.59\n",
            "16   MLP   EBAY 2017-07-20      3.64           3.59\n",
            "17   MLP   EBAY 2017-07-21      3.63           3.59\n",
            "18   MLP   EBAY 2017-07-24      3.62           3.57\n",
            "19   MLP   EBAY 2017-07-25      3.62           3.56\n",
            "20   MLP   EBAY 2017-07-26      3.64           3.57\n",
            "21   MLP   EBAY 2017-07-27      3.62           3.58\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cNG8xfTfKYZ",
        "colab_type": "text"
      },
      "source": [
        "## LSTM Test Harness: Early Stopping, Model Checkpoint for Hyper Parameter Optimization "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJn4MWGeiz3w",
        "colab_type": "text"
      },
      "source": [
        "### Step 1. Define, Fit and Evaluate Performance of Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPhOoXxWW_Qk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# multivariate multi-step lstm\n",
        "\n",
        "# import libs\n",
        "from pandas import DataFrame\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "# from numpy import hstack\n",
        "from sklearn.metrics import explained_variance_score,r2_score,mean_squared_error,mean_absolute_error,median_absolute_error \n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.optimizers import Adam\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# model definition, fit, eval\n",
        "def model_def_fit_eval(X_train,y_train,X_val,y_val,wts_fpath,n_nodes=100,n_epochs=200,n_patience=2):\n",
        "  \"Define and Fit Model + Tune Hyper Parameters: n_nodes, n_epochs, n_patience\"\n",
        "  # model define\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(n_nodes, activation='relu', return_sequences=True, input_shape=(nsteps_input, nfeatures)))\n",
        "  model.add(LSTM(n_nodes, activation='relu'))\n",
        "  model.add(Dense(nsteps_output))\n",
        "  # model compile\n",
        "  model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "  # early stopping\n",
        "  early_stop = EarlyStopping(monitor='val_loss', patience=n_patience, verbose=1)\n",
        "  # checkpoint\n",
        "  # filepath=\"weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
        "  filepath= wts_fpath\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "  callbacks_list = [early_stop, checkpoint]\n",
        "  # model fit\n",
        "  history = model.fit(X_train,y_train,validation_data=(X_val,y_val),epochs=n_epochs,batch_size=1,verbose=1,callbacks=callbacks_list,shuffle=False)\n",
        "  # hyper parameter values\n",
        "  print(f'Number of nodes in the hidden layer: {n_nodes}')\n",
        "  print(f'Number of epoches: {n_epochs}')\n",
        "  print(f'Patience: Epochs before early stop: {n_patience}')\n",
        "  # model evaluate\n",
        "  _, train_mse = model.evaluate(X_train, y_train, verbose=0)\n",
        "  _, val_mse = model.evaluate(X_val, y_val, verbose=0)\n",
        "  print(f'Train: {train_mse:.6f}, Validation: {val_mse:.6f}')\n",
        "  # plot accuracy of model learning\n",
        "  pyplot.plot(history.history['mean_squared_error'], label='train')\n",
        "  pyplot.plot(history.history['val_mean_squared_error'], label='val')\n",
        "  pyplot.legend()\n",
        "  pyplot.show()\n",
        "  return\n",
        "\n",
        "\n",
        "# set parameters and run model\n",
        "if __name__ == '__main__':\n",
        "  # file path for weights\n",
        "  wts_fpath = \"weights.best_lstm.hdf5\"\n",
        "  # hyper parameter combos\n",
        "  n_nodes = [50,100,200]\n",
        "  n_epochs = [100,1000,2000]\n",
        "  n_patience = [2,3,4]\n",
        "  # run model\n",
        "  for n in n_nodes:\n",
        "    for e in n_epochs:\n",
        "      for p in n_patience:\n",
        "        model_output = model_def_fit_eval(X_train,y_train,X_val,y_val,wts_fpath,n_nodes=n,n_patience=p)\n",
        "        print(model_output)\n",
        "          \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4ItZscti-Yv",
        "colab_type": "text"
      },
      "source": [
        "### Step 2. Best Model Prediction and Performance on Test + Export Predictions to CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tge3jdEEXxyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tuned hyper parameters for best model\n",
        "'''\n",
        "Epoch 00048: val_loss did not improve from 0.00024\n",
        "Epoch 00048: early stopping\n",
        "Number of nodes in the hidden layer: 200\n",
        "Number of epoches: 200\n",
        "Early stopping criteria: loss\n",
        "Patience: Epochs before early stop: 4\n",
        "Train: 0.009736, Validation: 0.000552\n",
        "'''\n",
        "\n",
        "# use weights from best model to predict test\n",
        "def best_model_def_loadwts_pred(X_test,y_test,n_nodes,wts_fpath):  # ,n_nodes=100,n_epochs=200,estop_criteria='loss',n_patience=2):\n",
        "  \"Use Weights from Best Model\"\n",
        "  # model define\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(n_nodes, activation='relu', return_sequences=True, input_shape=(nsteps_input, nfeatures)))\n",
        "  model.add(LSTM(n_nodes, activation='relu'))\n",
        "  model.add(Dense(nsteps_output))\n",
        "  # load weights\n",
        "  model.load_weights(wts_fpath)  # \"weights.best.hdf5\")\n",
        "  # model compile\n",
        "  model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "  print(\"Defined model and loaded weights from best model\")\n",
        "  # model predict\n",
        "  yhat = model.predict(X_test, verbose=0)\n",
        "  # predicted: select last step of predicted multistep output\n",
        "  y_pred = np.asarray([x[-1] for x in yhat])\n",
        "  print('N size: y_pred ', len(y_pred))\n",
        "  # actual: select last step of actual multistep output\n",
        "  y_actual = np.asarray([x[-1] for x in y_test])\n",
        "  print('N size: y_actual ', len(y_actual))\n",
        "  # plot actual & predicted y on test\n",
        "  plt.plot(y_actual)\n",
        "  plt.plot(y_pred)\n",
        "  plt.ylabel('LOGPRICE: Actual vs. Predicted')\n",
        "  plt.show()\n",
        "  return y_actual, y_pred\n",
        "  \n",
        "\n",
        "# model performance metrics\n",
        "def y_pred_metrics(y_actual, y_pred):\n",
        "  \"Use weights from best model to predict test sample.\"\n",
        "  # import error metrics from sklearn\n",
        "  from sklearn.metrics import explained_variance_score,r2_score,mean_squared_error,mean_absolute_error,median_absolute_error\n",
        "  # compute root mean squared error of predictions\n",
        "  rmse = np.sqrt(np.mean(np.square((y_actual - y_pred)),axis=0))\n",
        "  print(f'Root Mean Squared Error: {rmse:.3f}')\n",
        "  # compute mean squared error\n",
        "  mae = mean_squared_error(y_actual, y_pred)\n",
        "  print(f'Mean Squared Error: {mae:.3f}')\n",
        "  # compute mean absolute error\n",
        "  mae = mean_absolute_error(y_actual, y_pred)\n",
        "  print(f'Mean Absolute Error: {mae:.3f}')\n",
        "  # compute median absolute error\n",
        "  mdae = median_absolute_error(y_actual, y_pred)\n",
        "  print(f'Median Absolute Error: {mdae:.3f}')\n",
        "  # compute explained_variance_score\n",
        "  evar = explained_variance_score(y_actual, y_pred)\n",
        "  print(f'Explained Variance: {evar:.3f}')\n",
        "  # compute R2\n",
        "  r2 = r2_score(y_actual, y_pred)\n",
        "  print(f'R^2: {r2:.3f}')\n",
        "  return\n",
        "\n",
        "\n",
        "# export model predictions to csv\n",
        "def export2csv(y_actual, y_pred, test_df, ticker, model_type):\n",
        "  \"Export actual and predicted y, model type, ticker and date to csv file.\"\n",
        "  # convert arrays to dfs\n",
        "  df_actual = pd.DataFrame(y_actual)\n",
        "  df_pred = pd.DataFrame(y_pred)\n",
        "  # create date, ticker, model labels\n",
        "  df_date = pd.DataFrame(test_df.index [-22:])\n",
        "  df_ticker = pd.DataFrame([ticker]*22) \n",
        "  df_model_type = pd.DataFrame([model_type]*22) \n",
        "  # concatenate y actual pred arrays\n",
        "  df_actual_pred = pd.concat((df_model_type, df_ticker, df_date, df_actual, df_pred), axis=1)\n",
        "  df_actual_pred.columns = ['MODEL', 'TICKER', 'DATE', 'LOGPRICE', 'Pred_LOGPRICE']\n",
        "  print(df_actual_pred)\n",
        "  # create file name for csv\n",
        "  fname = ticker + '_' + model_type + '.csv'\n",
        "  # write df to csv\n",
        "  df_actual_pred.to_csv(fname)\n",
        "  return df_actual_pred\n",
        "\n",
        "\n",
        "# set parameters and run model\n",
        "if __name__ == '__main__':\n",
        "  # use weights from best model to predict test\n",
        "  wts_fpath = \"weights.best_lstm.hdf5\"\n",
        "  # n_nodes one best model\n",
        "  n_nodes = 200\n",
        "  # call best model funcs\n",
        "  y_actual, y_pred = best_model_def_loadwts_pred(X_test,y_test,n_nodes,wts_fpath)\n",
        "  best_model_metrics = y_pred_metrics(y_actual, y_pred)\n",
        "  print(best_model_metrics)\n",
        "  # export actual and predicted to csv\n",
        "  df_actual_pred_test = export2csv(y_actual, y_pred, test_df, 'EBAY', 'MLP')\n",
        "  print(df_actual_pred_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAQ-nHZbXyLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewh4a-oFnUYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_SosSvifSFJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# multivariate multi-step lstm\n",
        "\n",
        "# 0. import libs\n",
        "from pandas import DataFrame\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "# from numpy import hstack\n",
        "from sklearn.metrics import explained_variance_score,r2_score,mean_squared_error,mean_absolute_error,median_absolute_error \n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.optimizers import Adam\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# model definition, fit, eval\n",
        "\n",
        "def model_def_fit_eval(X_train,y_train,X_val,y_val,wts_fpath,n_nodes=100,n_epochs=200,estop_criteria='loss',n_patience=2):\n",
        "  \"MLP: Define and Fit - hyper params: n_nodes, n_epochs, estop_criteria, n_patience\"\n",
        "  # model define\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(n_nodes, activation='relu', return_sequences=True, input_shape=(nsteps_input, nfeatures)))\n",
        "  model.add(LSTM(n_nodes, activation='relu'))\n",
        "  model.add(Dense(nsteps_output))\n",
        "  # model compile\n",
        "  model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "  # early stopping\n",
        "  early_stop = EarlyStopping(monitor=estop_criteria, patience=n_patience, verbose=1)\n",
        "  # checkpoint\n",
        "  # filepath=\"weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
        "  filepath= wts_fpath\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "  callbacks_list = [early_stop, checkpoint]\n",
        "  # model fit\n",
        "  history = model.fit(X_train,y_train,validation_data=(X_val,y_val),epochs=n_epochs,batch_size=1,verbose=1,callbacks=callbacks_list,shuffle=False)\n",
        "  # hyper parameter values\n",
        "  print(f'LSTM: Number of nodes in the hidden layer: {n_nodes}')\n",
        "  print(f'LSTM: Number of epoches: {n_epochs}')\n",
        "  print(f'LSTM: Early stopping criteria: {estop_criteria}')\n",
        "  print(f'LSTM: Patience: Epochs before early stop: {n_patience}')\n",
        "  # model evaluate\n",
        "  _, train_mse = model.evaluate(X_train, y_train, verbose=0)\n",
        "  _, val_mse = model.evaluate(X_val, y_val, verbose=0)\n",
        "  print(f'Train: {train_mse:.6f}, Validation: {val_mse:.6f}')\n",
        "  # plot accuracy of model learning\n",
        "  pyplot.plot(history.history['mean_squared_error'], label='train')\n",
        "  pyplot.plot(history.history['val_mean_squared_error'], label='val')\n",
        "  pyplot.legend()\n",
        "  pyplot.show()\n",
        "  return\n",
        "\n",
        "\n",
        "def best_model_def_fit_eval(X_test,y_test,wts_fpath):  # ,n_nodes=100,n_epochs=200,estop_criteria='loss',n_patience=2):\n",
        "  \"Tuned MLP: Use Weights from Best Model\"\n",
        "  # model define\n",
        "  model = Sequential()\n",
        "  model.add(Dense(n_nodes, activation='relu', input_dim=n_input))\n",
        "  model.add(Dense(nsteps_output))\n",
        "  # load weights\n",
        "  model.load_weights(wts_fpath)  # \"weights.best.hdf5\")\n",
        "  # model compile\n",
        "  model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "  print(\"Defined model and loaded weights from best model\")\n",
        "  # model evaluate\n",
        "  _, test_mse = model.evaluate(X_test, y_test, verbose=0)\n",
        "  print(f'Test MSE from Best Model Checkpoint: {test_mse:.6f}')\n",
        "  \n",
        "  # model predict on test\n",
        "  \n",
        "  # import error metrics from sklearn\n",
        "  from sklearn.metrics import explained_variance_score,r2_score,mean_squared_error,mean_absolute_error,median_absolute_error\n",
        "\n",
        "  # def pred_y(X_data, y_data):\n",
        "  # model predict\n",
        "  yhat = model.predict(X_data, verbose=0)\n",
        "  #print('Multivariate MLP: Multistep Prediction Error Metrics\\n')\n",
        "\n",
        "  # predicted: select last step of predicted multistep output\n",
        "  y_pred = np.asarray([x[-1] for x in yhat])\n",
        "  print('N size: y_pred ', len(y_pred))\n",
        "\n",
        "  # actual: select last step of actual multistep output\n",
        "  y_actual = np.asarray([x[-1] for x in y_data])\n",
        "  print('N size: y_actual ', len(y_actual))\n",
        "\n",
        "  # plot actual & predicted y on test\n",
        "  plt.plot(y_actual)\n",
        "  plt.plot(y_pred)\n",
        "  plt.ylabel('LOGPRICE: Actual vs. Predicted')\n",
        "  plt.show()\n",
        "\n",
        "  # compute root mean squared error of predictions\n",
        "  rmse = np.sqrt(np.mean(np.square((y_actual - y_pred)),axis=0))\n",
        "  print(f'Root Mean Squared Error: {rmse:.3f}')\n",
        "\n",
        "  # sklearn regression metrics\n",
        "  # explained_variance_score,r2_score,mean_squared_error,mean_absolute_error,median_absolute_error\n",
        "\n",
        "  # compute mean squared error\n",
        "  mae = mean_squared_error(y_actual, y_pred)\n",
        "  print(f'Mean Squared Error: {mae:.3f}')\n",
        "\n",
        "  # compute mean absolute error\n",
        "  mae = mean_absolute_error(y_actual, y_pred)\n",
        "  print(f'Mean Absolute Error: {mae:.3f}')\n",
        "\n",
        "  # compute median absolute error\n",
        "  mdae = median_absolute_error(y_actual, y_pred)\n",
        "  print(f'Median Absolute Error: {mdae:.3f}')\n",
        "\n",
        "  # compute explained_variance_score\n",
        "  evar = explained_variance_score(y_actual, y_pred)\n",
        "  print(f'Explained Variance: {evar:.3f}')\n",
        "\n",
        "  # compute R2\n",
        "  r2 = r2_score(y_actual, y_pred)\n",
        "  print(f'R^2: {r2:.3f}')\n",
        "\n",
        "  return y_actual, y_pred \n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # initialize flattened X train, val, test samples\n",
        "  n_input, X_train_flat, X_val_flat, X_test_flat = flatten(nsteps_input,nfeatures,X_train,X_val,X_test)\n",
        "  # hyper parameter combos\n",
        "  n_nodes = [50,100,200]\n",
        "  n_epochs = [100,1000,2000]\n",
        "  estop_criteria = ['loss','val_loss']\n",
        "  n_patience = [2,3,4]\n",
        "  # run model\n",
        "  for n in n_nodes:\n",
        "    for e in n_epochs:\n",
        "      for s in estop_criteria:\n",
        "        for p in n_patience:\n",
        "          mlp = model_def_fit_eval(X_train_flat,y_train,X_val_flat,y_val,n_nodes=n,n_patience=p)\n",
        "          print(mlp)\n",
        "          \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMF5VS73JshU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# multivariate multi-step mlp\n",
        "\n",
        "# 0. import libs\n",
        "from pandas import DataFrame\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "# from numpy import hstack\n",
        "from sklearn.metrics import explained_variance_score,r2_score,mean_squared_error,mean_absolute_error,median_absolute_error \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.optimizers import Adam\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# flatten input data for MLP (TO SPEED UP CALCULATIONS IN TF)\n",
        "\n",
        "def flatten(nsteps_input,nfeatures,X_train,X_val,X_test):\n",
        "  \"\"\"MLP: vectorize input samples\"\"\"\n",
        "  # calc new n_input dim\n",
        "  n_input = nsteps_input*nfeatures  # - FLATTENED NUMBER OF COLUMNS IN INPUT SAMPLE\n",
        "  print('n_input: ', n_input)\n",
        "  # flatten train, val, test samples\n",
        "  X_train_flat = X_train.reshape((X_train.shape[0],n_input))\n",
        "  print('X_train shape after reshape vectorization: ', X_train_flat.shape)\n",
        "  X_val_flat = X_val.reshape((X_val.shape[0], n_input)) # CREATES FLATTENED INPUT SAMPLE\n",
        "  print('X_val_flat shape after reshape vectorization: ', X_val_flat.shape)\n",
        "  X_test_flat = X_test.reshape((X_test.shape[0], n_input)) # CREATES FLATTENED INPUT SAMPLE\n",
        "  print('X_test_flat shape after reshape vectorization: ', X_test_flat.shape)\n",
        "  \n",
        "  return n_input,X_train_flat,X_val_flat,X_test_flat \n",
        "\n",
        "\n",
        "# set parameters\n",
        "nsteps_input = 3\n",
        "nfeatures = 4\n",
        "\n",
        "# create flattened X train, val, test samples\n",
        "n_input, X_train_flat, X_val_flat, X_test_flat = flatten(nsteps_input,nfeatures,X_train,X_val,X_test)\n",
        "\n",
        "# model definition, fit, eval\n",
        "\n",
        "def model_def_fit_eval(X_train_flat,y_train,X_val_flat,y_val,n_nodes=100,n_epochs=200,estop_criteria='loss',n_patience=2):\n",
        "  \"MLP: Define and Fit - hyper params: n_nodes, n_epochs, estop_criteria, n_patience\"\n",
        "  # model define\n",
        "  model = Sequential()\n",
        "  model.add(Dense(n_nodes, activation='relu', input_dim=n_input))\n",
        "  model.add(Dense(nsteps_output))\n",
        "  # model compile\n",
        "  model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "  # early stopping\n",
        "  early_stop = EarlyStopping(monitor=estop_criteria, patience=n_patience, verbose=1)\n",
        "  # checkpoint\n",
        "  # filepath=\"weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
        "  filepath=\"weights.best.hdf5\"\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "  callbacks_list = [early_stop, checkpoint]\n",
        "  # model fit\n",
        "  history = model.fit(X_train_flat,y_train,validation_data=(X_val_flat,y_val),epochs=n_epochs,batch_size=1,verbose=1,callbacks=callbacks_list,shuffle=False)\n",
        "  \n",
        "  # hyper parameter values\n",
        "  print(f'Number of nodes in the hidden layer: {n_nodes}')\n",
        "  print(f'Number of epoches: {n_epochs}')\n",
        "  print(f'Early stopping criteria: {estop_criteria}')\n",
        "  print(f'Patience: Epochs before early stop: {n_patience}')\n",
        "  # model evaluate\n",
        "  _, train_mse = model.evaluate(X_train_flat, y_train, verbose=0)\n",
        "  _, val_mse = model.evaluate(X_val_flat, y_val, verbose=0)\n",
        "  print('Train: %.6f, Validation: %.6f' % (train_mse,val_mse))\n",
        "  # plot accuracy of model learning\n",
        "  pyplot.plot(history.history['mean_squared_error'], label='train')\n",
        "  pyplot.plot(history.history['val_mean_squared_error'], label='val')\n",
        "  pyplot.legend()\n",
        "  pyplot.show()\n",
        "  return\n",
        "\n",
        "\n",
        "def best_model_def_fit_eval(X_test_flat,y_test):  # ,n_nodes=100,n_epochs=200,estop_criteria='loss',n_patience=2):\n",
        "  \"Tuned MLP: Use Weights from Best Model\"\n",
        "  # model define\n",
        "  model = Sequential()\n",
        "  model.add(Dense(n_nodes, activation='relu', input_dim=n_input))\n",
        "  model.add(Dense(nsteps_output))\n",
        "  # load weights\n",
        "  model.load_weights(\"weights.best.hdf5\")\n",
        "  # model compile\n",
        "  model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "  print(\"Defined model and loaded weights from best model\")\n",
        "  # model evaluate\n",
        "  _, test_mse = model.evaluate(X_test_flat, y_test, verbose=0)\n",
        "  print(f'Test MSE from Best Model Checkpoint: {test_mse:.6f}')\n",
        "  \n",
        "  # model predict on test\n",
        "  \n",
        "  # import error metrics from sklearn\n",
        "  from sklearn.metrics import explained_variance_score,r2_score,mean_squared_error,mean_absolute_error,median_absolute_error\n",
        "\n",
        "  # def pred_y(X_data, y_data):\n",
        "  # model predict\n",
        "  yhat = model.predict(X_data, verbose=0)\n",
        "  #print('Multivariate MLP: Multistep Prediction Error Metrics\\n')\n",
        "\n",
        "  # predicted: select last step of predicted multistep output\n",
        "  y_pred = np.asarray([x[-1] for x in yhat])\n",
        "  print('N size: y_pred ', len(y_pred))\n",
        "\n",
        "  # actual: select last step of actual multistep output\n",
        "  y_actual = np.asarray([x[-1] for x in y_data])\n",
        "  print('N size: y_actual ', len(y_actual))\n",
        "\n",
        "  # plot actual & predicted y on test\n",
        "  plt.plot(y_actual)\n",
        "  plt.plot(y_pred)\n",
        "  plt.ylabel('LOGPRICE: Actual vs. Predicted')\n",
        "  plt.show()\n",
        "\n",
        "  # compute root mean squared error of predictions\n",
        "  rmse = np.sqrt(np.mean(np.square((y_actual - y_pred)),axis=0))\n",
        "  print(f'Root Mean Squared Error: {rmse:.3f}')\n",
        "\n",
        "  # sklearn regression metrics\n",
        "  # explained_variance_score,r2_score,mean_squared_error,mean_absolute_error,median_absolute_error\n",
        "\n",
        "  # compute mean squared error\n",
        "  mae = mean_squared_error(y_actual, y_pred)\n",
        "  print(f'Mean Squared Error: {mae:.3f}')\n",
        "\n",
        "  # compute mean absolute error\n",
        "  mae = mean_absolute_error(y_actual, y_pred)\n",
        "  print(f'Mean Absolute Error: {mae:.3f}')\n",
        "\n",
        "  # compute median absolute error\n",
        "  mdae = median_absolute_error(y_actual, y_pred)\n",
        "  print(f'Median Absolute Error: {mdae:.3f}')\n",
        "\n",
        "  # compute explained_variance_score\n",
        "  evar = explained_variance_score(y_actual, y_pred)\n",
        "  print(f'Explained Variance: {evar:.3f}')\n",
        "\n",
        "  # compute R2\n",
        "  r2 = r2_score(y_actual, y_pred)\n",
        "  print(f'R^2: {r2:.3f}')\n",
        "\n",
        "  return y_actual, y_pred \n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # initialize flattened X train, val, test samples\n",
        "  n_input, X_train_flat, X_val_flat, X_test_flat = flatten(nsteps_input,nfeatures,X_train,X_val,X_test)\n",
        "  # hyper parameter combos\n",
        "  n_nodes = [50,100,200]\n",
        "  n_epochs = [100,1000,2000]\n",
        "  estop_criteria = ['loss','val_loss']\n",
        "  n_patience = [2,3,4]\n",
        "  # run model\n",
        "  for n in n_nodes:\n",
        "    for e in n_epochs:\n",
        "      for s in estop_criteria:\n",
        "        for p in n_patience:\n",
        "          mlp = model_def_fit_eval(X_train_flat,y_train,X_val_flat,y_val,n_nodes=n,n_patience=p)\n",
        "          print(mlp)\n",
        "          \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id5tnomPJ17W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTBEA1qCEfwu",
        "colab_type": "text"
      },
      "source": [
        "### Multivariate LSTM: Define - Fit - Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja35W3Jcb0gN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# multivariate multi-step stacked lstm example\n",
        "\n",
        "# 0. import libs\n",
        "from pandas import DataFrame\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "from numpy import hstack\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# 1. train: model definition\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(nsteps_input, nfeatures)))\n",
        "model.add(LSTM(100, activation='relu'))\n",
        "model.add(Dense(nsteps_output))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error',metrics=['mean_squared_error'])\n",
        "\n",
        "# 2. train: model fit\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=2000, verbose=0)\n",
        "\n",
        "# 3. model evaluate\n",
        "_, train_mse = model.evaluate(X_train, y_train, verbose=0)\n",
        "_, test_mse = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_mse,test_mse))\n",
        "\n",
        "# 4. plot accuracy of model learning\n",
        "pyplot.plot(history.history['mean_squared_error'], label='train')\n",
        "pyplot.plot(history.history['val_mean_squared_error'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()\n",
        "\n",
        "# 5. test: predict seq data\n",
        "yhat = model.predict(X_test, verbose=0)\n",
        "print('Multivariate LSTM: Multistep prediction')\n",
        "print(yhat)\n",
        "y_pred = [x[-1] for x in yhat]\n",
        "print(y3_test,y_pred)\n",
        "# rmse = np.sqrt(((predictions - targets) ** 2).mean())\n",
        "# rmspe = np.sqrt(np.mean(np.square(((y_true - y_pred) / y_true)), axis=0))\n",
        "rmspe = np.sqrt(np.mean(np.square(((y3_test - y_pred)/y3_test)),axis=0))\n",
        "print(f'rmspe is {rmspe:.3f}')\n",
        "\n",
        "# export csv of predicted values\n",
        "y_pred_lstm = DataFrame(y_pred)\n",
        "\n",
        "from google.colab import files\n",
        "y_pred_lstm.to_csv('y_pred_lstm.csv') \n",
        "files.download('y_pred_lstm.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAIllbw8l6XD",
        "colab_type": "text"
      },
      "source": [
        "### **4. Multivariate CNN: Define - Fit - Predict**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VghQyoJmJYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# multivariate multi-step 1d cnn example\n",
        "\n",
        "# 0. import libs\n",
        "from pandas import DataFrame\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "from numpy import hstack\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# 1. train: model definition\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(nsteps_input, nfeatures)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(nsteps_output))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error',metrics=['mean_squared_error'])\n",
        "\n",
        "# 2. train: model fit\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=2000, verbose=0)\n",
        "\n",
        "# 3. model evaluate\n",
        "_, train_mse = model.evaluate(X_train, y_train, verbose=0)\n",
        "_, test_mse = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_mse,test_mse))\n",
        "\n",
        "# 4. plot accuracy of model learning\n",
        "pyplot.plot(history.history['mean_squared_error'], label='train')\n",
        "pyplot.plot(history.history['val_mean_squared_error'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()\n",
        "\n",
        "# 5. test: predict seq data\n",
        "yhat = model.predict(X_test, verbose=0)\n",
        "print('Multivariate CNN: Multistep prediction')\n",
        "print(yhat)\n",
        "y_pred = [x[-1] for x in yhat]\n",
        "print(y3_test,y_pred)\n",
        "# rmse = np.sqrt(((predictions - targets) ** 2).mean())\n",
        "# rmspe = np.sqrt(np.mean(np.square(((y_true - y_pred) / y_true)), axis=0))\n",
        "rmspe = np.sqrt(np.mean(np.square(((y3_test - y_pred)/y3_test)),axis=0))\n",
        "print(f'rmspe is {rmspe:.3f}')\n",
        "\n",
        "# export csv of predicted values\n",
        "y_pred_cnn = DataFrame(y_pred)\n",
        "\n",
        "from google.colab import files\n",
        "y_pred_cnn.to_csv('y_pred_cnn.csv') \n",
        "files.download('y_pred_cnn.csv')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX9Bj_mvw6gK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pandas import DataFrame\n",
        "y_pred_cnn = DataFrame(y_pred)\n",
        "\n",
        "from google.colab import files\n",
        "y_pred_cnn.to_csv('y_pred_cnn.csv') \n",
        "files.download('y_pred_cnn.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SChPtgFszCXO",
        "colab_type": "text"
      },
      "source": [
        "## CNN Test Harness: Early Stopping, Model Checkpoint for Hyper Parameter Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ngp_4ORzFpi",
        "colab_type": "text"
      },
      "source": [
        "### Step 1. Define, Fit and Evaluate Performance of Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixq7WdJJzLD6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# multivariate multi-step cnn\n",
        "\n",
        "# import libs\n",
        "from pandas import DataFrame\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "# from numpy import hstack\n",
        "from sklearn.metrics import explained_variance_score,r2_score,mean_squared_error,mean_absolute_error,median_absolute_error\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.optimizers import Adam\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# model definition, fit, eval\n",
        "def model_def_fit_eval(X_train,y_train,X_val,y_val,wts_fpath,n_filters=64,n_kernel=2,n_epochs=200,n_patience=2):\n",
        "  \"Define and Fit Model + Tune Hyper Parameters: n_nodes, n_epochs, n_patience\"\n",
        "  # model define\n",
        "  model = Sequential()\n",
        "  model.add(Conv1D(filters=n_filters, kernel_size=n_kernel, activation='relu', input_shape=(nsteps_input, nfeatures)))\n",
        "  model.add(MaxPooling1D(pool_size=2))  # NEED REFERENCE\n",
        "  model.add(Flatten())  # NEED REFERENCE\n",
        "  model.add(Dense(50, activation='relu'))  # NEED REFERENCE - WHY 50?\n",
        "  model.add(Dense(nsteps_output))\n",
        "  # model compile\n",
        "  model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "  # early stopping\n",
        "  early_stop = EarlyStopping(monitor='val_loss', patience=n_patience, verbose=1)\n",
        "  # checkpoint\n",
        "  # filepath=\"weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
        "  filepath= wts_fpath\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "  callbacks_list = [early_stop, checkpoint]\n",
        "  # model fit\n",
        "  history = model.fit(X_train,y_train,validation_data=(X_val,y_val),epochs=n_epochs,batch_size=1,verbose=1,callbacks=callbacks_list,shuffle=False)\n",
        "  # hyper parameter values\n",
        "  print(f'Number of filters: {n_filters}')\n",
        "  print(f'Kernel Size: {n_kernel}')\n",
        "  print(f'Number of epoches: {n_epochs}')\n",
        "  print(f'Patience: Epochs before early stop: {n_patience}')\n",
        "  # model evaluate\n",
        "  _, train_mse = model.evaluate(X_train, y_train, verbose=0)\n",
        "  _, val_mse = model.evaluate(X_val, y_val, verbose=0)\n",
        "  print(f'Train: {train_mse:.6f}, Validation: {val_mse:.6f}')\n",
        "  # plot accuracy of model learning\n",
        "  pyplot.plot(history.history['mean_squared_error'], label='train')\n",
        "  pyplot.plot(history.history['val_mean_squared_error'], label='val')\n",
        "  pyplot.legend()\n",
        "  pyplot.show()\n",
        "  return\n",
        "\n",
        "\n",
        "# set parameters and run model\n",
        "if __name__ == '__main__':\n",
        "  # file path for weights\n",
        "  wts_fpath = \"weights.best_cnn.hdf5\"\n",
        "  # hyper parameter combos\n",
        "  n_filters = [64]\n",
        "  n_kernel = [2,3,5]\n",
        "  n_epochs = [100,1000,2000]\n",
        "  n_patience = [2,3,5]\n",
        "  # run model\n",
        "  for f in n_filters:\n",
        "    for k in n_kernel:\n",
        "      for e in n_epochs:\n",
        "        for p in n_patience:\n",
        "          model_output = model_def_fit_eval(X_train,y_train,X_val,y_val,wts_fpath,n_filters=f,n_kernel=k,n_epochs=e,n_patience=p)\n",
        "          print(model_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JdVgOYlzSpR",
        "colab_type": "text"
      },
      "source": [
        "### Step 2. Best Model Prediction and Performance on Test + Export Predictions to CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6U3bocLKzYLp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Epoch 00031: val_loss did not improve from 0.00026\n",
        "Epoch 00031: early stopping\n",
        "Number of filters: 64\n",
        "Kernel Size: 2\n",
        "Number of epoches: 2000\n",
        "Patience: Epochs before early stop: 5\n",
        "Train: 0.008301, Validation: 0.000790\n",
        "'''\n",
        "\n",
        "# use weights from best model to predict test\n",
        "def best_model_def_loadwts_pred(X_test,y_test,n_filters,n_kernel,wts_fpath):\n",
        "  \"Use Weights from Best Model\"\n",
        "  # model define\n",
        "  model = Sequential()\n",
        "  model.add(Conv1D(filters=n_filters, kernel_size=n_kernel, activation='relu', input_shape=(nsteps_input, nfeatures)))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(50, activation='relu'))\n",
        "  model.add(Dense(nsteps_output))\n",
        "  # load weights\n",
        "  model.load_weights(wts_fpath)\n",
        "  # model compile\n",
        "  model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "  print(\"Defined model and loaded weights from best model\")\n",
        "  # model predict\n",
        "  yhat = model.predict(X_test, verbose=0)\n",
        "  # predicted: select last step of predicted multistep output\n",
        "  y_pred = np.asarray([x[-1] for x in yhat])\n",
        "  print('N size: y_pred ', len(y_pred))\n",
        "  # actual: select last step of actual multistep output\n",
        "  y_actual = np.asarray([x[-1] for x in y_test])\n",
        "  print('N size: y_actual ', len(y_actual))\n",
        "  # plot actual & predicted y on test\n",
        "  plt.plot(y_actual)\n",
        "  plt.plot(y_pred)\n",
        "  plt.ylabel('LOGPRICE: Actual vs. Predicted')\n",
        "  plt.show()\n",
        "  return y_actual, y_pred\n",
        "  \n",
        "\n",
        "# model performance metrics\n",
        "def y_pred_metrics(y_actual, y_pred):\n",
        "  \"Use weights from best model to predict test sample.\"\n",
        "  # import error metrics from sklearn\n",
        "  from sklearn.metrics import explained_variance_score,r2_score,mean_squared_error,mean_absolute_error,median_absolute_error\n",
        "  # compute root mean squared error of predictions\n",
        "  rmse = np.sqrt(np.mean(np.square((y_actual - y_pred)),axis=0))\n",
        "  print(f'Root Mean Squared Error: {rmse:.3f}')\n",
        "  # compute mean squared error\n",
        "  mae = mean_squared_error(y_actual, y_pred)\n",
        "  print(f'Mean Squared Error: {mae:.3f}')\n",
        "  # compute mean absolute error\n",
        "  mae = mean_absolute_error(y_actual, y_pred)\n",
        "  print(f'Mean Absolute Error: {mae:.3f}')\n",
        "  # compute median absolute error\n",
        "  mdae = median_absolute_error(y_actual, y_pred)\n",
        "  print(f'Median Absolute Error: {mdae:.3f}')\n",
        "  # compute explained_variance_score\n",
        "  evar = explained_variance_score(y_actual, y_pred)\n",
        "  print(f'Explained Variance: {evar:.3f}')\n",
        "  # compute R2\n",
        "  r2 = r2_score(y_actual, y_pred)\n",
        "  print(f'R^2: {r2:.3f}')\n",
        "  return\n",
        "\n",
        "\n",
        "# export model predictions to csv\n",
        "def export2csv(y_actual, y_pred, test_df, ticker, model_type):\n",
        "  \"Export actual and predicted y, model type, ticker and date to csv file.\"\n",
        "  # convert arrays to dfs\n",
        "  df_actual = pd.DataFrame(y_actual)\n",
        "  df_pred = pd.DataFrame(y_pred)\n",
        "  # create date, ticker, model labels\n",
        "  df_date = pd.DataFrame(test_df.index [-22:])\n",
        "  df_ticker = pd.DataFrame([ticker]*22) \n",
        "  df_model_type = pd.DataFrame([model_type]*22) \n",
        "  # concatenate y actual pred arrays\n",
        "  df_actual_pred = pd.concat((df_model_type, df_ticker, df_date, df_actual, df_pred), axis=1)\n",
        "  df_actual_pred.columns = ['MODEL', 'TICKER', 'DATE', 'LOGPRICE', 'Pred_LOGPRICE']\n",
        "  print(df_actual_pred)\n",
        "  # create file name for csv\n",
        "  fname = ticker + '_' + model_type + '.csv'\n",
        "  # write df to csv\n",
        "  df_actual_pred.to_csv(fname)\n",
        "  return df_actual_pred\n",
        "\n",
        "\n",
        "# set parameters and run model\n",
        "if __name__ == '__main__':\n",
        "  # use weights from best model to predict test\n",
        "  wts_fpath = \"weights.best_cnn.hdf5\"\n",
        "  # number of filters in best model\n",
        "  n_filters = 64\n",
        "  # kernel size in best model\n",
        "  n_kernel = 2\n",
        "  # call best model funcs\n",
        "  y_actual, y_pred = best_model_def_loadwts_pred(X_test,y_test,n_filters,n_kernel,wts_fpath)\n",
        "  best_model_metrics = y_pred_metrics(y_actual, y_pred)\n",
        "  print(best_model_metrics)\n",
        "  # export actual and predicted to csv\n",
        "  df_actual_pred_test = export2csv(y_actual, y_pred, test_df, 'EBAY', 'CNN')\n",
        "  print(df_actual_pred_test)\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}